{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "MO_GPKaibAji",
    "outputId": "e1dc802a-5a47-48cc-9295-e4605bb86bbd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-286dcffc6698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ls /gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!ls /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaR1PUrubPeA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  This is a hack to allow sibling import\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "from matrix_completion.mat_fact import MatrixFactorization, MF\n",
    "from matrix_completion.utilis import test_train_split, is_split_good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "B3U1bCq9bmKC",
    "outputId": "d06df611-c878-4036-f39f-e1151dea13d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INBRED</th>\n",
       "      <th>TESTER</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inbred_1071</td>\n",
       "      <td>Tester_1345</td>\n",
       "      <td>0.986544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inbred_1071</td>\n",
       "      <td>Tester_4373</td>\n",
       "      <td>1.057704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inbred_1071</td>\n",
       "      <td>Tester_4473</td>\n",
       "      <td>1.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inbred_1071</td>\n",
       "      <td>Tester_4541</td>\n",
       "      <td>1.014735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inbred_1071</td>\n",
       "      <td>Tester_5305</td>\n",
       "      <td>1.062727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        INBRED       TESTER      mean\n",
       "0  Inbred_1071  Tester_1345  0.986544\n",
       "1  Inbred_1071  Tester_4373  1.057704\n",
       "2  Inbred_1071  Tester_4473  1.023704\n",
       "3  Inbred_1071  Tester_4541  1.014735\n",
       "4  Inbred_1071  Tester_5305  1.062727"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df = pd.read_csv(\"../data/input/grouped_data.csv\")\n",
    "grp_df.drop(labels=grp_df.columns[0], axis=1, inplace=True)\n",
    "grp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeBAQe5ObmKI"
   },
   "outputs": [],
   "source": [
    "train_df = grp_df.pivot(index='INBRED', columns='TESTER', values='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZJVQ9XcbmKa"
   },
   "outputs": [],
   "source": [
    "test_, train_ = test_train_split(train_df=train_df, seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixFactorization(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmax = 30\n",
    "\n",
    "parameters = {\n",
    "    'alphas': [10**i for i in range(-5,0)],\n",
    "    'betas': [10**i for i in range(-5,0)],\n",
    "    'lambdas':[10**i for i in range(-5,0)],\n",
    "    'ks': list(range(20, kmax+1, 2))\n",
    "}\n",
    "\n",
    "bests = model.grid_search(parameters=parameters, kfold=5, iter_max=300, n_cores=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOydd5hdVb2w37XL6dNnMhnSKySE0MFYkUsJem1YEIUPFMWCeEXxWpF7BRW4elFQQa6ACAqCFQUDQURaKCEhEBISkkmfyfQ5fZ+zy/r+2OecnJlMS5nMJFnv88xzcnZde2dm/davCyklCoVCoVCMFG2sB6BQKBSKgwslOBQKhUKxRyjBoVAoFIo9QgkOhUKhUOwRSnAoFAqFYo8wxnoAB4L6+no5ffr0sR6GQqFQjH+yWdi8GTIZXoJOKWVD/0MOC8Exffp0li9fPtbDUCgUivFLLgfXXgvXXQc1NXDnnYjzztsy0KHKVKVQKBSHO8uWwfHH+4Lj/PNh7Vr4yEcGPVwJDoVCoThcSafhS1+Ct7wFUil4+GH49a+hrm7I0w4LU5VCoVAo+vHYY/DpT/v+jM9/Hn7wA6isHNGpSuNQKBSKw4meHrjkEjjzTDBNePJJ+NnPRiw0QAkOhUKhOHz4059g/ny46y74+tdh1Sp429v2+DLKVKVQKBSHOm1tcPnl8MADcOyx8Le/wYkn7vXllMahUCgUhypS+s7uefPgL3+B730PXnxxn4QGKI1DoVAoDk22boXPfAaWLIFFi+D2230Bsh9QGodCoVAcSnie7+w++mh46im46Sb/cz8JDVAah0KhUBw6rFsHn/oUPP20HzV1220wCuWWlMahUCgUBzu27ZcKOfZYWL0a7rwTHnlkVIQGKI1DoVAoDm5WrvTzMlauhHPP9c1UEyeO6i2VxqFQKBQHI5YF3/oWnHwytLTA738Pf/jDqAsNUBqHQqFQHHw884yvZaxbBxddBP/7v1Bbe8BurzQOhUKhOFhIpeCLX/SzvS3LD7X91a8OqNAAJTgUCoXi4OCRR2DBAvjpT+ELX/Cd4GefPSZDUYJDoVAoxjPd3XDxxbB4MYRCu3IzYrExG5ISHAqFQjFe+cMf/KKE99wD3/wmvPyy3ztjjFHOcYVCoRhvtLb65qg//tHvzLdkCRx33FiPqoTSOBQKhWK8IKXv7J4/Hx56yE/qe/75cSU0QGkcCoVCMT7YvBkuvRSWLoW3vhV++Us48sghT1nbGmfJ6jZ29GaZVB1m8YJG5jVVjfpQlcahUCgUY4nrws03+xFTy5b5md//+teIhMZtT24inrVpqgoRz9rc9uQm1rbGR33Io6ZxCCHuAP4daJdSLhhgvwB+ArwLyAAXSylXCCHeCdxYduhRwEellH8WQvwKeAdQfDMXSylfHq1nUCgUipGyV6v/tWv9ooTPPutHTd16K0ybNqL7LVndRlXYpCpsApQ+l6xuG3WtYzQ1jl8Bi4fYfw4wp/BzKXALgJTyn1LK46SUxwGn4wuVR8vO+2pxvxIaCoViPLDHq3/b9psqHXccvP6632zp4YdHLDQAdvRmqQj1XftXhAx29Gb35VFGxKgJDinlk0D3EIe8D/i19HkOqBZCNPU75kPA36WUmdEap0KhUOwr5at/TYjSv5esbtv94BUr/PpS3/42vO99sGYNXHghCLFH95xUHSZpOX22JS2HSdXhfXmUETGWPo5JwLay79sL28r5KHBvv23fE0K8IoS4UQgRHOziQohLhRDLhRDLOzo69s+IFQqFYgBGtPrPZuHrX4dTTvF7gP/xj3D//dDYuFf3XLygkXjWJp618aQs/Xvxgr273p4wloJjIPEqSzt97eMY4JGy/d/A93mcDNQCXxvs4lLK26SUJ0kpT2poaNg/I1YoFIoBGHb1/9RTvlnq+uv9LPA1a+ADH9ine85rquLSt8+gKmzSGreoCptc+vYZBySqaizDcbcDU8q+TwZayr5/BPiTlNIubpBSthb+mRNC3AlcOeqjVCgUimFYvKCR257cBPiaRtJyiGdtzj+qEi67DH7+c7+p0tKlcMYZ++2+85qqDoig6M9YCo4HgS8IIe4DTgXiZYIB4Hx8DaOEEKJJStlaiMh6P7D6gI1WoVAcVuxJlFRx9V9+/CeTa5l6xnth+3b40pfg2mshGj3ATzE6jGY47r3AaUC9EGI7cDVgAkgpbwUexg/F3YAfOfWJsnOn42sj/+p32d8IIRrwzVwvA58drfErFIrDl2KUVFXY7BMlNZQpqLT67+qCK66Au++GefP83hmLFh3gJxhdRk1wSCnPH2a/BC4bZN9mdneUI6U8fb8MTqFQKIZgr3IkpIQHHvBrTPX0wFVX+R36goPG8By0qJIjCoVC0Y8dvVmaqkJ9tg2ZI9HS4vsy/vxnOPFEeOwxWLjwAIx0bFAlRxQKhaIfI86RkBJuv90vSrhkCdxwAzz33CEtNEAJDoVCodiNEeVINDfDmWf6JUOOPRZeeQW++lUwDn1DjhIcCoVC0Y8hcyRcF378YzjmGHjhBbjlFvjnP2HOnLEe9gHj0BeNCoVCsRcMmCPx2mtwySV+j4x3vcsvSjhlysAXOIRRgkOhUIw5Y9VXYsTk837W9zXXQGWl38r1Yx/b4/pShwpKcCgUijFlb3ImRnMsuwmw7et9LePVV+GjH4Wf/AQmTDig4xpvKB+HQqEYU/aosuwo0r80eqY3Qetn/gP5pjf5SX1/+Qvce+9hLzRAaRwKhWKM2eOciVGiXIBNXvU8Z9x4FTUtW3j1nI9wzL23QdXYmc7GmylPCQ6FQjGmTKoOE8/apexs2JUzcSAnzB29WaYZNu/4yTUsfOh39DZN4f7rf8ULM47jh2MsNMaLKa+IEhwKhWJMGayy7MnTq/d5wtwTwfPWdc9z+o+/Q6yng5c++Ameveg/6JIGk8oE2lgwli1iB0P5OBQKxZgyWM7E+rb0Pvk+RtzOtaMDPv5x3n/VZ0iHY9x+3T08cenX6JLGAWuMNBRj2SJ2MJTgUCgUY8pgWsG+TpjDOt2lhPvu88uFPPAA/Nd/0fv0MlLHncja1gRrWhOkcjZLVrcN3jv8ADCWLWIHQwkOhUIxZgylFezrhDmk4Nmxw+/3ff75MHOm3wf86quZN62BxQsaqQiZzG+q5KiJlYNrKgeIsWwROxhKcCgUijFjKK1gbybMta1xbly6nisfWMXW7gxbOtN99qcyeRY/86CvZTz2GPzoR/Dss7BgwYjGNBaMZYvYwVDOcYVCMSasbY3z6JqdIKEibDK7IUpDRaikFQzUVe+8kycPOmH2jz6yHZcVW3sBmFYfxdzUzMdu+g5z1iyHd74T/u//YNas3a4zXsKDyxmrFrGDoQSHQqE44BQn+YCuIaUkZ/uT/AlTqwkYeskctScTZv/oo+n1MQB29mY4c8lvWHz/z9ECAbjtNr+i7SDlQoYKDy6OfTzlVIwFSnAoFIcoozHB7a9rFif5BZMqeWlLL0FDENQFr7UkmNkQ47yTJ+/xfQfSFI5P7uD0H32DqRtfg/e8x69kO2m35qJ9GCw8+LyTJ4/LnIqxQPgdXEfhwkLcAfw70C6lXDDAfgH8BL/veAa4WEq5orDPBV4tHLpVSvnewvYZwH1ALbACuFBKmR9uLCeddJJcvnz5vj+UQnGQUD7BlU9++zLB7c9rXvnAKpqqQmhC0Jmy2NCeJp7NowmNH31kYel6a1vj3L1sC09v6KImYjL/iAqChjHgfW9cur6kKWh2nlPu/QWn3HcruWglkV/8HD7ykREXJewvqOY2RlnflmbpmjZMXXD0EZU0VPhCKp61sR2X+orQIaeFCCFeklKe1H/7aDrHfwUsHmL/OcCcws+lwC1l+7JSyuMKP+8t2349cKOUcg7QA1yyf4esUBwajIaDd2+uWe6svnHp+lJkUnnEVH0sxJtm1vGmmfWcOb+xj9C47clNvNaSoCbsG0dWbo1ju+6A9y0602OrVvDxz5/Lont+yso3ncX2p1+E887bo0q285qquOLMufzww8eyeEEjj63tKDjpPZCSFVt76UhaAOQch6c2dA2fL3IIMWqCQ0r5JNA9xCHvA34tfZ4DqoUQTYMdXNBQTgd+X9h0F/D+/TVeheJQYn8kjfWf9Ne0xvfomkOF2o4kYqooqGxXEjR1QqZO0NDY0J4e8L7zKg2+/c9fcsnXPo6RSvDna35B9P7fMnfBzBE/80CUC8yqcACE8MfR4UdsrWlJUhMZP1FYB4KxDMedBGwr+769sA0gJIRYLoR4TghRFA51QK+U0hng+N0QQlxauMbyjo6O/T12hWJcs685EANN+tu6smzt6hveOtQ1h9JQRhJiWhR+sZBBzvEACBoaCcve/b6PPw4LF1L3i5+hXXop1c3ref+3L90v5qJyITx7QpSc4yGlJFkQdj0Zm3lNFX3OGesorNFmLJ3jA+mNRYfLVCllixBiJvC4EOJVIDHE8bvvkPI24DbwfRz7OliFYrwykON4KAfvSBioPtLcxhjr2lLURIMjuuZwYa39I6aKGs6O3ixBXfDylm7+kfE1EhBMrAwSNDQCurbrvr29fp/vX/4SZs+GJ56Ad7xjr97ZYEKmGGVluy4b2tPkHJfutEM0aFAVNnnr7DoCht7nnLHO7B5txlLj2A6U91ycDLQASCmLn83AE8DxQCe+Ocvof7xCcbiytjXODUvW8cS6dl7bEeeJde3csGQdwD4ljQ1k6ppWH2VyTXjE19wTradcwzF1+Nf6Drb2ZEhZDpbtkrZsNraleKM9jedJzpjXwLwX/wVHHw133OELj1WrRiw0RlTDqsDiBY1s686wbGM32bxDZdCgImgyvS7K4gWNXLho2rjL7B5txlLjeBD4ghDiPuBUIC6lbBVC1AAZKWVOCFEPvAW4QUophRD/BD6EH1l1EfCXsRq8QjEeuHvZFrZ2ZYiFDCoKJp2tXRnuXraF75+7cK9NNYPlMhx9hO80Hgl7ovWUazjPNSfI5BwEAk0Dz5M4BZtBzBAcY+ao//TF8MJjtEydwxM33c/x557BvEhkROPa02qz85qqaKwM0pnKYbuSWMjg6ElVBAydJavbuOLMuXuUqHgoMGqCQwhxL3AaUC+E2A5cDZgAUspbgYfxQ3E34IfjfqJw6jzgF0IID18juk5Kuaaw72vAfUKIa4GVwO2jNX6F4mBg5bZeYkHfcQz4n1KyclvvPl13X01dsKtUxt3LtvDY2i4EguOnDDyZlpu12pMWiZyD57s1EAJ0AdKTnLHyMb629DbCuSz3vPtTdHzui8RdjRf3IJdibzLD867k7XMb0MoiszwpBzW7HeqMmuCQUp4/zH4JXDbA9meBYwY5pxk4Zb8MUKE4BBCI3Rx9srB9X9iTch/D+QuytsepM+pKAmighLlyP0Ii61CeXuZKmJzs4JpHfsY7Ny7n1SnzuPYDXyYxcy5nBYIUrzLS/hTDZYbvr3MOZVTmuEJxEHP8lCqWNXcjCiGiOccjlXNZNLN2n689klX0YJnUZ8xrYH1bmkfX7CSgayyYVIkmzEHNQkUNp7kjRV3UJJm1saVEx+NjK5fwtSfuRJceP3rX5/jdqe8lLwVHhnZN4nsSxbQ32tTcxig3P74R2/Woiwb85EVNK2WTH24lSFR1XIXiIOaCRdOYUR8FIGHZAMyoj3LBomkH5P4Dhdx6nsfN/9hIPGuDBCklL23ppTPlJ8wNmINR0HDyroeuaUytCzM/tZPf/PabXPvoz1nVNJcPfvZWHj/7o7hCBwR1UZPnmrt4dM1OnlzfQVD3tazBkg7732ukTv61rXEeW9vB3Akx6qIButM263amOGNeA8AeOdoPFZTGoVAcxMxrquLKs+eO2Yp3IH9Ba9zC8aRfmiRskrNdgoZgQ3ua+lhoUBPPvKYqzpo/kWQqyzsfvoc3/fpmcrrJVe/5Eg+duJim6jCW7VER1ImFDF7fmSIW1AlogpTl0BK3eOiVHTy2tmPYWlJ7WzxxRoNfODGetVnflu7TpRDGR1vXA4ESHArFQU5xgioKj2LG8oGYuAay/Xel89RFAwDMboiyYmsvQV0Qz+ZLoaqDmYXeJ9oxr7yUKc1reePNZ/Dgp77JdqKcUxUi58qSYLx72RZea0nsFuV017KtzG+qHHQiH4lZqf8xz23sRAKpnEssZDC7IUpdLFjSmsZbCfYDgRIcCsU4YF/s5GNZsXUgf4Gpa0ys9CfThooQM+oirNjai+V4rGlNcNGiqbuPK5eDa69l5nXX4VTX8Ldv/5i/zF5E3HKpDAnqK0J93slgUU5tCYtZDRGea06QsGwqQyYzGyLs6HVG9J76H7NqWw8rt/ViaILqiInreazYajN3QqyP9nG4Oc2Vj0OhGGP2NCGtP2PZsW4gf8Hlp89CL2R3tyWyrG9LEQ0avOuYRuY3VfLY2o6+z7ZsGRx/PFx7LXzsYxivr2XW5y8mFgowv6mSeU1927eubY2ztTvDw6+28lxzV8l3krT85Lznm3uwbJeKoIFluzzf3ENQFyN6T+XHdKdzvLojQdDQkEDOkXSmcuQcl/VtqVKG/uGW/AdK41Aoxpw9TUjrz4HqWDeYVjSQv2BmQ4wlq9t4flMXsZDBgkmVSAlrWhN0p/Jc/eAavvtvUznqp/8DN90EkyfDww/DOecAsGTp+j7vJO+4NHek+Pw9KwiaOhMrAiQ0Xzgt39zDURMr0DSNyTVh1rel+oxF4Icoj+Q9lR+zoT2NJyWxoEHW9giaGtm8i2W7zJ4QKz3z4Zb8B0pwKBRjzr5O/Acix6C/CWdTR4or7tvJlLow85uqdjOtFYVJ8dm6Ujnf12Fo1EQMpq94hoarPgQdLXDZZfCDH0DFrkKBO3qzGJovaDoSFgnLoTZq0p3JY2oaLb1ZGmIBIkGDbF7S3JHmmMlVrNiaJBbQ8aQs+SQmVgZZua0XgeCNtmSfXhr931P5u0xYNmFTJ+94mLqfGaNrgmzeoyEW3O1ZDyeUqUqhGGP2tZLtgTCXlGtFXakc69tTICCesYc0rRWfbUNHmqChUZdPc+mvf8ANt30VaQa4/0e/gZ/+tI/QAAjoghc39ZCzXXKOhwB2xi0yeRdNQNjUSOVcbFcyZ0KEuOUQMHQaK4K4EmxXcvzUKmY3RNnQniagaxw7pZKk5fB8czdtieyA76n8XVYEDYKGRtZ2yNoutuvheX5mYlsid8iH3A6FEhwKxRizrxP/nuYl7A3lRQ+LQqAyZJDMOUP6VIrP1p3K89ZXn+SH//0xTn9hCf8695Pc84u/8MKUowe8X9G8BJB3PVzPI5N3cT1JOu/gSXClJGhovLS1l5ChsaY1QWcqR0cyh5V3eKMtxWstCSSwYFIlEyrCLJpVSyxksGp7fMD3VP4uNQEJywEEnpRk8y5CCN48q5YptZFDut/GcChTlUIxxgxU3uPk6dUsWd3G7U9vHlGU1WibS8pNOCnLIRbUyTkelYXs7cFMa/OaqvjcvChdN3yFRSseZ9OUOdx71c/JH3s88azNpGpzt3PWtsZZuS2OlB4dqRw52y0kBgqQEscrRjIZICUJyyFi6uRsl/pYEEPT6Ernsboz1EaDnDqzhvqYb5qqj4V4+9wgrXFr0GKNxfe4tTtDY2WIpzd04noSXdM4dUYNcxor+9SpOhxRgkOhGAeUT/xrW+P88JH1pQieN9qSrN4R58qz546acBguHHjxgkZuWLKO7nSeHT0Z8q6HJgQz66N0pixMXd/dtCYl3H03c7/0Jbx0mofO+wIvfeQSorFwqQlS/3yOoi/F1AVmyAThJ/cFdA2nYCYKGxpp28N2ASH8Io8Bo1TosSYaQNMElWGTRTPr/Az2MkZiBiw3ze1M5LBsF4DOtM2cEV7jUEYJDoVinHHPsi1s6vTbo1aGTHKOx6bONPcs28L3zl243+/30Cs7uPkfG0nnHVxPognBI6t3cvm/zeLdC3c12dSEIJN3sF0XzwNDF+Qcl2Ubu5lRH+W8s8tW8Fu20P6xi5jw7L94ddrR3P4f32LeaSdRkZd9oo+AUvOmgC54ZXscy/YImRopy6YqEkATIIRAEwJXSkxDZ1YhIXBmQwxPSlp6s7QlLMKmTjSoF8Jtjb2u8lsesDB7QpSXtvQS0EVJ4O1ppeBDDSU4FIpxxspt8d1KpUspWbnNd8buz6J6a1vj3Pz4RnKOSzrnoAm/2q6hwc2Pb2Rmgx92umR1G1NqIyRzDpGAgev5k/W2XougrhEOFDrgeR7ccgvOf36NmOPys3P/g3+d/kEStuSl57bx9XOOLJmIyiO1Ulae5zb1kMraVEdM6mJBEALHkxi6hutK3r1wIj3pPKt2xNnWY1ERNjhqYpS1rQkqC6VNkpZNTyZPRUinKhKguSNF2NR4ftOusu4j8f+Um+bqYyFOnFbN6h0JEH4plcMh5HYolOBQKMYZErlbWXRR2D7SLPGRCpclq9uwXY+c42HqWsEJ7JDK2YDgst+s4D3HTuK1ljjzmipJWQ4CSXsyRzrvIoAjaoPkbI8/3fdPpt13A5EXlvHKUSdz9TmX01o7kUAyT20kQDRocNeyrSUt5ubH3uClrb2k836Xv7ChY+iC3qxNMucQCxrEggZvmVXHup0pUpbDps4MlSGTiqDBURMr+NOKViZWBdnSlSFn+88hhJ+sFzYE3/3rGqojATQhMA1BS9wa0f9Bf03F1HVmNsQOSDb+wYASHArFOOP4KdU839wN/UqlnzqzdkTJgsOVOi8XJjt6s9RFA7yRTmFoglTOwfM8bA90TbK9O8sfV2wnk3exHRchYEevRd5xEUh0TdDek+YLK/7CBY/8CicUpuXHP+fCzulEgzqBgrbQGrdorAzQlvALEd746Ho2dmbQBOiar6gk827pHegCMnmHLV0ZGiqCXP5vs7hr2VZsz6M+FmT2hCj1sRCvtSTpSOaQgOX4fhdNgOtKtvVkydoeZPJMq4vu1h1xKPakH8nhiBIcCsU448JF02iNW3Sn8yQth4ChMbUuwoWLpnH705uHTRa8Z9kWmjtS5F0/6mn2hGip1PmbZtX1ESYRU6OpKsS6nUl6LBspwZN+KKyBQNcFrudRGTZYuTXu12tyPCzbb893XFsz1/7tx8zfuZGnFr6dWz90BdOnzCSWasf1JEL4vhCA9kSesKnxjT+uJpVzkPj+8zJ5AfhCw5US14XKkM4RVSHevXASf1rZAlKStBzeaE8jJYRMwRvtKSIBA09KKkIGmgAQtMYtKkIGrvR9JHvaHfFwTOwbKUpwKBTjjHlNVfzn4iMHNDUNliUe1AXf+uMrLGvuYltPllhAZ1JNGMt2eWlLL4agVOoc6FPKI5Vz0TWB4/UbiJTEgiZ5VzK5JojjeiQsG1dKQm6ey5+5j08v+z09kUq+/OFvs/FtZ4EQPLWhi9kNEV5tSQJg6gLLdknlHTrTfvJY8Vb9bwl+8p+H74x/51EN5FzfRLe9xxeOhgZbOlOsaYnjuh6OK/1EQdsjZ+eJBnWm1kbozdo4rtzlf2H/dEdUjGICoBDiDiFEuxBi9SD7hRDiJiHEBiHEK0KIEwrbjxNCLBNCvFbYfl7ZOb8SQmwSQrxc+DlutMavUIxHBkoW3Nad4Y32FMuau0lZDoYmSOddtnVn8QpJcjviFrXRvjkTFSGDvCsJ6gJP+hO2oVGaVouRTEVzWVNVGEPXWBxv5tG7vsTnnr2fvy78N979mVt55Kg3k3MlRx9RSU3EJJX3eOvsOoKmRjLnYDkewULZjoGERRHfl+PftzZiEjINJlWHWbK6jSMbY+Rsl+09FhJwPA/Hk3jSN1MhfMGQzftZ3tGAXy4kFjSQUvrCK+cO2vdcMXJGU+P4FfBT4NeD7D8HmFP4ORW4pfCZAf6flPINIcQRwEtCiEeklEX98qtSyt+P4rgVijFluLpQ/X0VG/IOq1viOK7Ek5KQoeEJyNoOmzpTmLpOOu8QNvuuE4u5CEvXJJhaG8aTftJb3nGxHEnO9SfbiZGQLzhEng/89aec98Jf2Vk9gf/81PX8pXEBUkJY1zhhajUNFSHmNUme3djN8VNrOPeECp5c30HKctiZyCJ365C+C0OAWchIrwqbhANGKez19qc3M7UuysaOtJ9BLn17mqSQYS59X4kAHAktcYu3za6nK50n53i0Jyyytoem7eoSqMxQe8+oCQ4p5ZNCiOlDHPI+4NdSSgk8J4SoFkI0SSnXl12jRQjRDjQAIzNMKhQHOeUO8I6ktVtdqMfWdpSiex56ZQf/91QzjutHRVm2JGN7BDSwPcDxiJgGTZUhmjszVIUDTKuP9slneHTNzpL5RghRiOryJ+R03sF2XN7V+ioX3Pl9JsQ7uPeU93L92y4gZYYxpKQ+GmRyXbRUODBkGrx1dh1VYZMdvVkSlo2Q0ndUD8GkmjBHTayguTPNzkSOiqBDOudw2W9WkLAcaiImjieZVhdBCL9gYXfGRsfXYgTgAkFD0BALcsuFJ7G2Nc7dy7bw+Lp2JL72tbolwQ8fWT+qCZWHOmPp45gEbCv7vr2wrbW4QQhxChAANpYd9z0hxHeAfwBfl1LmBrq4EOJS4FKAqVOn7t+RKxSDsD9yLPqU9i7UhQoaWqkuVE86x9UPrmFqbYRnN3ZiagKBBvhmJcv2yLlg6qAJv/R43vWoCps0d6YJmHqfKKHptRGWNXdj2S6eJynO7wEdJjpZ/uM3P+F9q5bSMnEanz/vf/lH7Rx0TRAAXA+6MjmaqoIs29hJd9qmJ2PheZD3JKYmyNkO3gg9Cy9vjxM2dU6YUsXm7ixtiRy6gMqwQWvcAgmGEESCBl5ReRFgaKKQXe4RCRiECn6NeU1VCCBiGlSEjJLZbTQTKg8HxlJwDPR7VNJjhRBNwN3ARVLK4lLlG8BOfGFyG/A14LsDXVxKeVvhGE466aTB9WOFYj+xvzrxDVUXqjNl8frOJK4Hp86opSedx/MkrifxAFPzBUbOBdsFU/f7SZi6IJ6xyTkel7x1ep/Q3bwriQZ10jmbcqXgrLXPcPWjt1CTSXDrW87jjtM+TlozCGsCKX1nu6ZJaiMBNndbVEdMcnmHnoyDLiCgCXpz/p+eQKIPIznakzkiAY12y2ZHT9a/vgAhIOu4RMke4bkAACAASURBVAuJh+3JHNODBpGARjrna1bSlXiaR8TU/cq4U6pL1x0uoVKx54xlddztwJSy75OBFgAhRCXwEPBtKeVzxQOklK3SJwfcCZxyAMerUAzJ/urEV+4AjwV1EpZDzvGYPcEvES6EoDYWoDvtK9sS3zcQMAS252sBhgZVIYOaSIBwQMcoOACSls2X71/FjUvXl7SjKbURTjuygaCpI4CGVDe3/un7/PRPP6AtVsd7L/ox1731QjocjWzew7JdDE0ghL/66826hE2Nf194BF0ZP6TX9iDt7FqvlXwRg2AIyNkeXWmHTN53ekvAlf7zOK5ESl9AxkIGScumM5UnqGtEAxpBU8P1/FDiupjJhYumld1b7rZKLSZUKvaOsdQ4HgS+IIS4D98pHpdStgohAsCf8P0fD5SfUPCBtAohBPB+YMCILYViLBiqIdOemLDKk88qwyYJy+HIxhi10SCdqW5MTaM+avLPdR1oQmC5HhJoiAXI5D0s2/ZNVJaDnvc1Fl3TyORdgoYGEjZ3prjidzuJZ22m1ISZ0xijIRbg3S89yDcf+yUhJ89177iY/zvlA7iav1IvTv6OC71ZP78kGvDrP3Wl89z/4pZh/Rjl5dKL6IVoqMHOlIAmIGu7eBJs12NSdRgpoSudpyZiYrt+kqSha3z17CP7vNuhEioVe8eoCQ4hxL3AaUC9EGI7cDVgAkgpbwUeBt4FbMCPpPpE4dSPAG8H6oQQFxe2XSylfBn4jRCiAf/372Xgs6M1foVipBSFwmstcd5oS7JgUmWpjHcxx2JPTVj9q+UWhU5dLEjEEDR3ZsjmXarCBhq+KSeetTE0gesJhF6Y5D2IZ10EfgMkV0ps12NdWwor75C0bF7f6ZBZv5H/eegmTlq/nBcmH83XF19Oc13fIn7FSb84wQd1geN6SCkxNI1krl8mXz90Ddx+hwh8raI//QWMpgnyriRiasSCfh+Qhoogpq6RyjtUR4JMrtGpDJt9CjPC0AmVir1D+EFNhzYnnXSSXL58+VgPQ3EIUu7XsGyHFzf1IIFTZ9YQNPxw0rCpETD0Pkl7RR/GYD0hhrrfFb9bBUAimyfv+AaX2qhJTTTIhrYkqUKdp6TV12ehCdCFwNQFdQXtxMnnuWDFQ1y29E48IbjhtIu557hz8IQ2oHZQTkAXRAL+c/Vk8qRz7oBCoPx4T0pM3V/1F53b5fcxhL9BQGnsAggaGp70mF4XpSbqt221bF+DSuVczpzfOOQ73Z+FIQ8nhBAvSSlP6r9dZY4rFPtAuV+jKmxy6kzBay0JVm1LcOb8xlIOQl2s75/anvQUL2deUxWTa8IksjZ5xyWdz6MD3ak8nak8CcumMqijawLfortrJvckGJok70o6kzmOTrTynQdvZOGW13hy1ol86+zL2Fk1AU36pUCGW1KGTR0JxEIG3an8sCcYmu/3ydnFQoQeUvrhs0L42ovt+o5XSbH0CESDGrrQ0DWddM7l5BlRAF7a0lsIGjCGLXWuyofsX5TgUCj2gf5+jYaK3TvMDVYmZG8bAR19RBXxrI3tujy7oYtM3iWRzWN7ErdgmgoYHqbum3eKaMIXHrrjcMlzf+CLz96HFQzz3Q9/jQcXnI7leoSAcECnK5UfUnsoPoOuweaONI6UaJrvyB4My/aoi2nYHsR0jZqI7yNJ5R2Qko5knrqojmV7ZGxfI9ELY64M6sRCJgnLJpl1mFYf5cjGGOvaUlQWhPb+KkKotJPhGVJwCCG+PNR+KeX/7t/hKBQHFyMRCnvbTGgg1rbG6UhaPL2hC8t2qA6bhEyN7jREAwbpnIMjwXIkAa1MaAAhQ2PO9vVc99CPmdexmYfnvY3rz/kcLaEq9LyDUxA8w/kqwDcfiUKoUsb2qAjq2ELieN5uikfxWCmhM5UnbOo4EhqrAoRNHV33czAqQibVkQBtiSxJy8X1PAKGRlNVmJzjceK0alKWQ2siR8DUmV4f47Onzdqvk/r+Cqk+1BlO46gofB4JnIwfCQXwHuDJ0RqUQnGwMJxQKK5eUzmbHb1ZKkMGRx9R1Wd1PNIVbrFTn+NJKoI6Pek8lp3Ddj0iAZ1YyCBgaHSn83gS8mWr/0psLnv8t3xi2R/ojlTx+Q9+m8eOXFQKcRWa70jfk/J/nvRNSeX5GQP5RYpmJyn9kFtPSsKm4ZusgICu0ZOxefOsWhorwyxr7qIq7LKjN4vryVL+xYb2NKfMqMU0dH744WP3YKQjZyRl6xXDCA4p5X8DCCEeBU6QUiYL3/8LeGCIUxWKw4Kh+jaUr16PmlhZEirlgqH8GEODv7/ayq+e2URtLMCimXVcuGha6Vo3P76RnOuSzOZJ5Tzyrocu/AkfKenJeHie70g2dUE671EZ1Dm6eRXX/O0nTO9u4YFjz+J/z/wUdlUVTQGDrnQeK++WSpuPNFTGrzLr/xiaIJ1zCZraoCG1jucnJ4LvG2mImbQnc4RMjX87agI96TxrW5Os2h6nI5ljYmUQvZgogv9MCcse9V7fQ4VUK3YxUh/HVCBf9j0PTN/vo1EoDkIGc7yOZPVaPCbvuPxrXUfBtyBJ5hy2dWdYsrqVxQuaEEA65zuAc45EIP1Eu8JMn3X8LAtNgKkJ8q5HnWvxjUfu5v3L/sK26kYu+tj3eWbqQoKmhsy5JLIOhi6IhQx6s84eP3eh7QWuJ3HxTVZDHi8Ehi4wdQ3D0Gms0DANjbZkjg1tSRwP6qImmoBNneniSWzuTO1W9HC02N/+qEOVkQqOu4EXhBB/wl9sfIDBq94qFApGtnotHvP46910pfM4hRjV4hScsBye2dBJb9Ymm3f8yChd4HqFznmFxktFPAk5V3LaxuV8/9GfMTHRyV2nvI+b33kR+VAEkXfJ5j2/30XhnGx+z4UGgKb55idnADXF1HwzVnnIbTSgA4LGyhDHTqli+eYeLNsjkbUJBQysvFMomyLIO5JIQGdSdYi2ZJ7tvRanza0fdV/D/vRHHcqMSHBIKb8nhPg78LbCpk9IKVeO3rAUioOLgfwUI1m9Fo9pS1g4rt8xrxjNVOzGtzNhEQuaeC44+DWiim4FXdsVOqsBVZk4Vz3+S8597Z+sr5vKeRf+Dy8ecRQAwnL6mKI86BN1tacUS5iXY4hdZUKEgEBBgAgJOdcjoGvsTFiwjULpFL8eV2WhAGHI1CEKmbxLxnZpT+U5ojrEjLooMxpio+5nUC1jR8aehONGgISU8k4hRIMQYoaUctNoDUyhGG8M5sQeqsf3Y2s7gMEd551Ji6c2dJGy/NU2ZSv0YokPz5NUhXS607vGUpyvA7og60l0JO9Z/wzffuQWqqwUP3nzR/nZovPIG+Zu5+wvvH4XDBkaFSGD3kwe2/OFhQQ0CULzq9rqml/Hd3NnmvqKICdMrWZDR5pcIZmvPWnRlcr7SYrRIA0VQXKORzSkHzA/g8r5GJ4RCQ4hxNXASfjRVXfilw65B3jL6A1NoRg/DBWmOZgvY31beljHuef5oaw7+8/CZdgebO7ODjjx5xzJhGQX1yy9hbPeeI5XJs7hgvOu5fUJM0bjNZQoheMWBmVqfh8PU/cr5wKETc0vVOh5IPET/DyJXjjW9bxSD48VW/1kPst2/fLoUlIbDZQiqta0JDntyAmj+kxFVB7H8IxU4/gAcDywAkoNliqGPkWhOHTo31xpQ0ea7lSeqx9cQ1XY4KiJlX2OL/oyhnKce55fMypoaMxpjLF2Z7K0iu8vJIrbDa2sZ7cn+eDLS/nWP28n4Np877RPcsfJ7ysVJRxNdCAW9Mucp3IuEokmNNJ5vxBhY4XJ9PoK0jmH5s40edvFciVzJkSpCpv0Zny/xaaOFNPqozRWBFjdkiRp2eiFLn0dyVyhDpagJ+uweEHjqD+XyuMYGSMVHHkppRRCSAAhRHQUx6RQjDuKTuyOpMWKrb0EDY2aiEFXKkciYxMpJKQVKRY3vHHp+gFXrjt6s7TGrV12fVOnNmLSm7VLPouBlBDH84sLnmYkufTu6zhl08usnHEsV551GRurjzgg70IDPAHxrEPYEFQEdXKFYoeTayL0BHI0xPx6UtGgQU3EpDvtt7WtjgQACAcMplSH2ZnMkbFd2pJ5jjmigldbkn5vEelhux7bey0mVAR46+y6AzJxqzyOkTFSwXG/EOIXQLUQ4tPAJ4Ffjt6wFIrxxaTqMJs7Uyzf0kMm7/oJd0GD+liQxoog69pS1ESDJV/Gtu6MX9DP0AdcuU6qDrNyaw91UX8iTef8iCkpQdcEIVMjXcjg1jVKeRaa5/LxF//GV5/6Na7Q+K9zLuP+E87xk/2Gjobdb5Q78DOOpMYUnD57Ao1V4ZJG9nxzN5bjkSzkXliOR0XAb6CUczxyjscJ06uxXf/dTq6JsKY1QX0sQGcqj45O0NCoDJsgOWCVbFUex8gYaVTVD4UQZwIJfD/Hd6SUS0d1ZArFOGJuY5Q/rthOKucQMf32rEnLYlZDlGn1UTK2W+qxPak6TL4y2Kcibt5xae5I8ZX7X+HM+Y3MbYxi6hoJy++Wt6PQFjWgC4QmChnWEDDA1HU8z2VG+xZu+PtNHN+6jsdnncw3z7qMnZX1GO7AIbHDVbfdW/pfM2t7rN2ZpCpikso5XPLW6WxoT7G+LYUQEAno2I6H5bisaU1QGTY5bnIVQcNgQoVZmqxTlt8aN2BodKXypPMOk2vCVEXMA7baV3kcI2OkzvHrpZRfA5YOsE2hOCQpd5Ju7c4wqz7C+o402bxLOKBTFw3QlbaZYDkcfURVn3LeVz6wCk86rGlN0JGw6ErnMQqJeXcv24zluERNjbznaxkhU6M6HKAtKQuOZD901nZBty0+/+zv+cKzvyMVjPDF91zJg/PeUSoWNZDQgNERGrC7CS3neGzvyfDoazaLF0xkXlMVsyfEyDkediHEWBOCdN4lZGrURwO8vjNFzpGcd/Zclqxu87sdhgxytkskYKBVCKaYEeY1VfaZxGF0ndcqj2NkjLR17JkDbDtnfw5EoRhPFJ2k8axNU1WI7lSe1kSO4yZX0VgZoiEWxNBhfVuSx19vpyNpsbZ1Vw/rgC54cVMPOdsladlk8y49GZtUzqU3Y2M7Ho4HFUEDT/rJbgFDI6BrZGwXTQgMDRa0rOcPd17Bl5/+DX8/8i2c8albeHD+absqDI4Dig2jejI2yzZ28dArO8i7krfPbeDM+Y1EgwYTKkNMqQnjSf/YipBBY2WQeU1VpVa5EyuC5GyPeNbGsl0mVgZLJVqK9P9/KZoAy9/9vlDM46gKm7TGLarCpnKMD8Bw1XE/B3wemCWEeKVsVwXw7GgOTKEYK9a2xrn6wTV0p/LUxgLMbohSGwuQyNp0pW1OmFrNK9vjbOu2iAR03jyrlnTO4YrfrWJyTZijj6iiN50vrfjj2b4NlSiUCs/YLlProtiupCrsh556UqILgW5l+MrTv+WTL/yZ9mgNl3zwKv4x+9QD/i4Efjl2KYd3oQQMQdDUufnxjZwwtZpkwfSUsvz2tQCzGmK8aWYdnpS0xi2gb9JdxnZJWA5VYYPp9bHdtImBnNc96RxXP7iGqbWR/aKBqDyO4RnOVPVb4O/AD4Cvl21PSim7R21UCsUYUVzRdqVy1EZMcrbLiq29zKiLkMjYdKZynDKjFl0TNFQEWTTL71u9ri0FQCJrs6kjxdMbu3xTR2+e/iWcSqYeVxI0NMKmXx1WyhyJrM0pm1/h2iU3M6Onld8et5gfnPYJksGxCWTUNV87SOdcPCn9goqDoOFXwbUcv+ptPGsDEAvqJCy/rMmCSX7Ycn+/wUgn6/7O686Uxes7k7genDqjVoXPHiCGNFVJKeNSys3AT4BuKeUWKeUWwBZCHPjlj0IxyhRXtPWxIHnXL+kdNDS6MjZzG2PUxfwmTbYrOXVmDfWxEBva034EUMigM5VjfXuKgK6RyOTpydiD3suT0JPJUxXxw01D2RT//fef8pt7v4mQcP5Hv883z/7CmAkN8EuHJDIOhiYIGRq68DWQgSYOy5Gs3ZmkM2nRnsqVTD6VYRMr7yClZMWWXp5Y186WrvRe5WVMqg6TtHbV1trQni6ULgmgCVHSRpasbtuHp1YMx0h9HLcAqbLv6cK2IRFC3CGEaBdCrB5kvxBC3CSE2CCEeEUIcULZvouEEG8Ufi4q236iEOLVwjk3CTGOjL2Kg4K1rXFuXLqeKx9YxY1L1/exj+/ozVIRMpg9IVqWySzoTuXRdY3/fu98fvjhYzlzfiNBw1fYE5ZN0PD7aFuOV8jNEKTyfpnzwZDA9u4MiaxN09OP8ZsbP8V5qx7ltpM/wOJP3syyaQtH+U0MTakeli4wdJhcG6U6EmBSdYi6mLnb5KEVwrgyeY832pIAXHHmXL5y1lxmTaggZOrIggFP28s/26I/JJ618aSkM5UDCbMbdglXFT47+oxUcAgpZSmWQkrpMbKIrF8Bi4fYfw4wp/BzKQVhJISoBa4GTgVOAa4WQtQUzrmlcGzxvKGur1D0YTjnanFFWx8LceK0akKmTnfGpjYW6GP+WLygkS1daZ5Y105HMsf6tiSbOtPEMzY7ejK0xnPA8H6BylScH/75ei6/8SvEQzE+eMH/8P3TL8EyQ8OcObqEDFEQgBpCQCrnoWtwyvQaokGTI6oj1EYDFHPUNXzBYeoCIcDUtdKqf8nqNqbURjjtyAmcfXQTpx05gSm1kb3SCvo7r+tiQeY2xkqlS0CFzx4IRpoA2CyE+CK7tIzPA83DnSSlfFIIMX2IQ94H/LoglJ4TQlQLIZqA04ClRT+KEGIpsFgI8QRQKaVcVtj+a+D9+H4YhWJYlqxuw3U91rQmfKdtyGBiRbCUGVwMx+xO5diZsOhO25iaxkWLpu5mMy+umgWSeNZBE75PwLLdUoJcsc93OQIwheTda57kqsd+QWUuwy/eeSF3vO082nMH4CUMQtEJXsxa96TfWTBne0gJb7SlyNkecybE6MrYuJ7EMARhTYAQuJ7fDyQSMDA0rbTq39GbxdThueYECcumMmQysyHCjt69K+de7g8pXwio8NkDx0gFx2eBm4Bv4/9e/QN/1b+vTAK2lX3fXtg21PbtA2zfDSHEpcUxTp06dT8MVXEwMFyM/2stcbZ3ZwmaGrGgTs52Wd+WImP7qdnzmqo4Y14DNz++Edv1CBsaeVdy/ZL1PLOhi7fMrmN9W5qla9owdcGk6hDbe7JUhQ3yrkc27/WJlNUHEBwTE51c8+jPOGPji7zcNJfrP3Qla2qnomvgF04fGzy5yzylCQgZOlnbw3b973lX0tyRJpN3OGlaLdGAzubONEnLF5qi0I2wOuIn8RVX/UFdsKy5m4qQQUXQwLJdnm/uYdHM2n0esyqDPjaMNHO8HfjoKNx/IEOn3Ivtu2+U8jbgNoCTTjpptHKhFGNMuaAI6oKWuMW0uuigBeoSlgOCUtXVkKmTc7xS1A/4VW3fNLMO23V5aUsv4YBO2NR4cXM3T73RyayGKG2JLHnbY3VLHCSEAzoRU8P1IGJq9BQ66rnergxuIT0+uupRvvHPOzA9l2tO/xR3nvgeKiJB8q6Ha4/9r2nIgFkTKtjRa5HNO7iuLPQFF2hCYDkeHckczZ1p/ufDC2nuSPHDR9bTkcoTCWjURgw8D2qjgZLze6A/3P2Z1a7CZw88w+Vx/KeU8gYhxM0M8P8spfziPt5/OzCl7PtkoKWw/bR+258obJ88wPGKw5D+lUyfXN9BynJoqgqhCXPAAnVVYYN4Jo9V6P+Qc/zifFXhXX8KxZDPFzYlSkUIpZTsiFtUhAxe3ZFASsh7kkLFcLJ5l0ze70GBqREytMIq3S8dMrW7heuW3Myira/y7NSFfGPx5WypaQIgk3f8vhr70FRpf2G70JXKI6X0NaVCMyZT1/CkXz7E0ASe3DVhz2yIcc+yLazcFkciOX5KdalXOviayskzamjuzJTMg/OaKvapiZRibBlO41hb+Fw+Svd/EPiCEOI+fEd4XErZKoR4BPh+mUP8LOAbUspuIURSCPEm4Hng/wE3j9LYFOOc/slgedcjFtTZ0JEuOUv7R9jMb6oiYursTORK9vbpdZE+lW0nVYdZta2HV7b34nqSgKGVihFaeT+fwdCK/g1fcLiykMegCZI5h4CuYeiCsNC44IU/89l/3IWt6Xxt8eX8buFZfTK/bQ/sIfpxHEgc6UeJTawKQcqPXkKIgs9DEjZ0DF2UoqOK1FeEmH+EHNA8WKz/tGhmXWlb/3pQioOLIQWHlPKvhc+79ubiQoh78TWHeiHEdvxIKbNwzVuBh4F3ARuADPCJwr5uIcQ1wIuFS323LOHwc/jRWmF8p7hyjB+m9E8GqwyZZPMOqTKzU/8IG9/5nWFeU2UfZ2p5TkEkIHhhc7efxa2B7Xq09Fp+CQzLIRb0/RmVIYOuVL50noevheiaL0CmtzRz7UM/YWHLev4x+xS+edbnaauoH92Xso9oQDbvsaUrQyxoIPGbRWF4REzdL7yo6xw/pRoYWf8KVf/p0GM4U9VfGcIUKaV871DnSynPH2a/BC4bZN8dwB0DbF8OLBjquopDn7WtcbZ2Z1i5tYf6WJDZE6LMnhBl2UbfCetJySvbeljdkiRoaCxr7uKiRVN598JJwzpT//F6JxNiQdJ5v/yFofm5DKmcg+tJ0jmHnOM3LBoo3FbP21z2zG+55OnfkQzFuObjV3H7pFPGVX2pwRAFZ77jga5pRAyB5UhkwSQ3oSLIpJpwqcz5SPpXKAf2ocdwpqofFj7PBSbit4sFOB/YPEpjUiiGpLjKnVgRJJHxzSnLN/dw1MQKZtRHaawM8kJzF2t3JqmLBaiP+nWmrvv7OgDevXDSbpNW0cn+WkucVdt6CJl+v43J1SEsxyNne2Rth+l1UXb0+jWWBrLRH9eyjuv//hOO7NzKX485ne+efgmdoYNngvSKfcIphOcKgWn4AiEaNDjnmKbdGlKNpH/FYA5s1ab14GQ4U9W/AIQQ10gp3162669CiCdHdWQKxSCUr3JjIYNV2+O09Pily8+e38gFi6Zx9YNrOKI6XLYS9nNd71q2lXcv7BvBXRREruuxvTuLEJDJOehCYNleoVeEjeW45BxJRVAnZfUtJRLOW3zlqbv55PIH2VlRxyc+dDXPH3UqEpD9i1WNQ4q6UFEUmrogUXjGoKExsTJEb1n71m/+8RVWbuulI5mjLhrgxGk1Jb/SSBPwVJvWg5eR5nE0CCFmSimbAYQQM4CG0RuWQjE4/Ve5rieZWhsm53qYhs5tT25ia1eaKTV9J6+KoE5bwtrtesWkwOVbe/xeG6ZB0rHJ2i4VIYOt3Rkcz6MiaFATMXitNYOuCUwBWUeyaMsqrltyM9N6d3L38e/i+ndcTCoYQdien99QuM/4cH8PjK4Vug8K30HuerJkWZMSsrZLTcTk7mVbaI1bbO3KEAvq1IRNdvRk6c3Y1McCWI6HqWtcfvqsYe+p2rQevIxUcFwBPCGEKGaLTwc+MyojUhx0HGhzQzFKJ++4PLG+g2zeJaBrNFQES5OPpgmSObekaQAkcy6NlbuX8igmBWbzLiHDDzsNBXRs169VZbuSU2fUkrX9767nO79jVorvPPpLzn/lUTbVNHHe+T/g+anHlK5bjLYa7wR035+ha4KAriHw8100DSi0sm1L5Fg0M8rKbb0EdI1YyCBk6oQCfqDAzoRF3vGY0xhjYmWIx9Z2MLMhNuTvgWrTevAy0gTAJUKIOcBRhU2vSynHsDiCYrwwFuaGxQsauWHJOrZ2ZcjmXQzN70KXzjl0pixqo0Gm1kbY0eNPQBVBnWTOJZ1zBlwJt8YtdiaypHIucSkJ6n5fibBpckR1mI0dKda1JXFcSSrnYDseb3/jOa599OfUp3u59dQPcuNbPkbODI7K844mAogFTUxdY3p9lImVQV7c0kOokH9i6oJI0CAWMNiZyCEQ5B2PitCuqcN2JWFTp6k6zKJZftRYPGsPqzkM1KZ1S2eanckcVz6wSvk8xjEjbR0bAb4MTJNSfloIMUcIcaSU8m+jOzzFeGcszA3zmqo4oipEdzpP3LLRBEypDaEJwYb2NPOadE6dUcfcN0W5a9lW2hIWjZUhLj991oD+jbaERTrnIPAn0pwrsRzHb0yU0tGFIGU55ByPhkwvX19yK+95/SnWNkzn0+dexStNc0blOfcH/TO0Q4Yg78pSGRRdE9iupDKkMbEyiKZpzG+qpKEiwMqtcYKGRtDQsGy/g+HbZtexuiVBzvFK2fdZ2yVg+FpIkaLmMJQ22j9Md0tnmpXbejlharXyeYxzRmqquhN4CVhU+L4deABQguMwZ6zMDblCa9KuVI4VW3vRhCCgCzpTuVKOwLymqt0ERX+WrG5DExAslB7xyhLxDE1QHTEJBzS2d2d43+rH+eajtxG1s9z49gv4+SkfwtFH+ic0NP2LIe6tX0RQiPqV/jXL+5GHDIEQgrApcDyJLiBg6EypiTCxOlTquFfsAX7itGo2tKdJWDYBXeNts+u4YNG0kraH9NMAZSG3o7y0edLys+GH0kb7h+nuTOY4YWp1KRlT+TzGLyP9rZ8lpTxPCHE+gJQyq/pgKGBgc8P+Lms90Kq1eN+GihAnTK1mQ0ea7lSeuliQS98+A4Abl64f1u+yozeLIXzzy/9v78zD5KrKxP1+99baVb2kl3Q6+0ZCQhCRAEbQQQQJOLKIDjDCiKM4uIsTFRw3nGHRHzOIrIIiizooaIQRCAKCgARkCYEsJISErJ2kl3R1d+333vP7494qqju9VHV3dXfIeZ+nn1Tdusupk+7znW83AJ/h5jAoIOQzsJVibmof3/3dVXzgjRd4efJ8vvePX+P12mk4FIStkqsoW9p3Cxju4h70GdiOwrIVhgGOM3hJ9r4w5W2/igPUVriL9t6uFLGkhXIcAj6DyTVhLFtx6anz+xSuuQX/mFm1+YS9870yIt9cCExR2gAAIABJREFUOp+7V25l1fYOBOF9c1z/T8Dntr7NnV/hN4rK8ci9Xnbvau3zOEAoVnBkRCSMtwESkTmA9nFoyp4V3J8P5aQFDTy2vgWAumiQgM8klszmhUaxfpcpNWGSWRvHUT125wLE01lO+evDXPTQbYhjc+WHP8ddR34EZfpQztvmnuE4wDOOu9iHfAbJrA1ehdkcJq4AKOYRAq5D23ETFrO2wjSEjBdtNqMuQDxtE8/YNFSGmFQZZOOeOB/pdZ/BEvYWNFVz5cd6NpnqLdzPOXoqv3jmLWqjPZeYgQTBaGxCNCNDsYLj+8AKYJqI/Bo4DriwXIPSHDiUOyu4Px/Kxj3xfp977aMbi/K7rG+OsXF3J7Gktd/ufmb7Tq5ecT3Hbl/D32a+m++d9mXerPTKkgwiKUqt/KoUdGdslNfPIncsp4UEDPEitBTZAZ6tgIztmaschd9w606lLYfKoMmEigAVAYepfpMls+twlGKd1w2xt2ZWasXZvs4vVRDo0iQHDoMKDs8k9Tpu9vh7cX8vv6qUai3z2DQjTLnCZotZZIb67IF8KP09d6BrCjPEd+xLopTC74O0V97KdGw+98JyvvrMb0ibfi79yFf5w7tOIusUb5ktRWjktISAIVheL++MragIGESCfipDPuqiQWLJDBnLoTmWQqHIWIpcT07B1UpMz2TmNwXlRURlHaiLBkhn3y4df9jkKgC2tcXZtKebvZ1p0pbNG3u6WLMzxrJT5o3I70WpgkCXJjlwGFRwKKWUiPxRKXUU8OAojElTBsYyS3c4zx6K+aK/awqdtZ1JNyt6d2cKv2FgGw7zd2/mRw9dx6I9b7Ji3hK+e/LnaYnWEjYMHJwh+x0Gw2cYZB2HioCPSNBHbSSAIW7r1q60xdyJEdbstGntzqAUhP0mfkORytpYBWPKOcWrQj4MMbAch7AInUlX6wj6DA6fUu0Joiyv7oiR8exiVSE/acthS2ucX63cyhUfG36/86EIAt1b48CgWFPVcyJytFLqhcFP1YxHxjJLdzjPHor5ovc1r27fxys7YiQzNoYIk2tcbaQ+GmRf3CSVTvPNZ+7h03/7HR2hSj5/xqU8PP+4fFHCtJfTYI5k96ECKgIm8bRbyiRpOXSnLDpTFlUhHxUBk5Vvtrv5FgETx1F0pbL4DVebyCHgJSZCPGMT8rlJfLPrI4T9BohQGwlQGfLRHEvlBW9dNNCjqZVSilXbYyP23bQgeGdSrOD4IHCxiLwFxPHMuEqp4W9LNKPCWGbpDufZpe5ac6ao7nSWnR1JEqksW9oSuCu+2xt1+74kAUPwGQbHt2zkC7++mjmt21l++Ilc/sHP0hGuyt/P/UV3X5fiBC9Gxghum1W/aRD0m+zuzFAd9rulO7IW2/cl8s8OmAYOCstSOOzfvyPoMwiYQtZ2yFoKR1lUBk3CATfMeHZ9mJ0dKXZ3pvjwwkksXdTIn9ft7qcz39Cloy5aeHBQrOA4tayj0JSdsYxYGe6zi921FprEDp1URVfK4v9W78JnCCG/j0TGcvtqGw7BdIIL772Nc5//I601E/mPi37EPXWH4Xj5D7mw3OH0VxpMeEQCJoa8nVNhGoIIrGvuJGs7+TwMnwFJq28jmQEEfULQbxAwDUzToD4aYE9nmppIgKDfZEpNiM2tCQKeEyRnKpxZW8HGPd3gmcXSlkN32ubYIfYC10ULDx4G68cRAi4G5gKvAb9QSlkDXaMZn4xlxMpwn13o0O5MWVSHfSxsqt5vN9uXSSxjOWRtm0jQRzhg0pWyeN+bq7j8oeuZGtvDA8edxZ8v+ApORZTo5nbiqSy26ik0CnflxcqRYs5LWTZZRxH2G2Rt1wcxuSZMS1ea1u4UGds9L2sPciPPpOZ4xQkPaYgS9JmkPFvWtvYkQZ9bs6sy7MvPT4XfYHpdBe3xjOsD8rnvc702SkUXLTx4GEzjuBPIAk/jah0Lga+We1CakWcsI1aKfXZfAmJiNMiuWIrKoI8d7UkQiCUyVPhNbn0q0WM329sk1tqdQuGQthSxZJZGO8F3VtzK6S8/wubaKXz2M/9N42kncXhdmJue3Ex3ysqbo1TByi+AylW5VSPn5rAc8BkOEyIhdu5LUhVy+5vHM1aPXI6BnqcAx3HoTjlMq63g3GOm8vruOHMaIry+u4vOZJZdHQkaK4OYppmPqHJ9HRbfXDq/ZNNSf+YoXbTw4GEwwbFQKXU4gIj8Avh7+YekKRdj6agc7Nm9e2LkBMS21gSWowgHTIJ+d2FNZW12d6ZZ0FTVYzdbaBJ7Y08nz2/Zh207CPCBNU9z+Z9vpjYR46Yln+DXH/4U75ozkRfe2sf/vrB9QJOUA/hFsFXx1v9i/ehpC/bEUhgC7QmLjN1Fd0FeyWClR3wGKISqsI8lc+pIZFSPXiWb9sbZ02nQmbL50IK6/XpmlPo7MZA5SifwHTwYg3ye71YzFBOViCwVkQ0isklELu3j8xki8riIvCoiT4rIVO/4B0XklYKflIic6X12h4hsKfjs3aWOS1M6671EsWX3rubaRzeyvnnkIm8A7l65lc0t3Ty7uY32RNrzS5i0JTJEgya7O1N5c0vQZ9CZyu63m126qJFYMstbrd08t7mdjGVTH+/gxvuv4ublV9IancDZF/4Pvz3zYo6Y28jru7t5qy1elB8jW5ApPhj5elED4DPAbwhBn+t0NwzBFOhO90xG9Fz6/RIOmFQEfEyuCZGxFTs7kvnKtfXREO+dXcfSRY0E/Wa+JEgsmd2vz3qxFJqjDJH86xVr9uTnP5bMDvs5mvHNYBrHESLS6b0WIOy9z0VVVfV3oYiYwI3AybhFEV8QkQeUUusKTrsGuEspdaeInAhcBVyglHoCeLd3n1pgE/Dnguu+oZS6r+hvqRkW5XZ6rm+O8cymNiaEvV9H5ZY6n1QdzL0F3NLptqNo6UpjK8VTG1s4bHJVD9NJ2G/w2o4uUlmbj65+nG8/+jNCVpobTvo0d73v4yi/n1lVIZ59s41Exu6z/etIkHOyF/pJCp/kOOD3udqCZTvYjtdIyZB+tZXe2ofgljSf0xAilVX5nX3vXX/Q5+P9c+uoDvuHbaYcLCFTJ/AdHAzWOtYcxr2PATYVdA28BzgDKBQcC3GbRAE8Afyxj/t8HHhYKZUYxlg0w6CcTs/1zTG+/8A6ulJZ0lkbQSEi+ERo7cowqSpEd9rtPhdLZGhPuGXUJ1YG6UpZbNrbzY9XbGBGXQS/CS+9tY/U5i3c/ND1/MOWl1k9fSFXn72MNyZMIZa0sDOZfEc7exAVImAWH1kluOq7zdsLe2Fmd+9zXd+Eyudi+MU1idmOymscAVOoCvmwlSs0/QYks07+fuIJpnjaJugz8zv7vgIRRkrID2aO0nkbBweDmaqGwxRge8H7Hd6xQlYDZ3uvzwIqRaSu1znnAv/b69gVnnnrWhHps3uOiHxORF4UkRdbWlqG9g00AD3MHzlGwumZ02TautNMrg4ST1vEEhbtiSydqSydyQxzGiLMqo9w9MxaxBBCfoPaSIAJkSBL5tSypzPF37e0cd9L21n+0naO//Nveei2L7B4xzq+f9K/8U+fvJr11U3EkhZZxy3T0ZXOkrHUgHkZAlSHA24WdsExsw8TVNgnTKjwEwoYPT5XuH9gAc8sFfYbVAYNQj7XLFWYwGd7CYYBn3uPsN/AbwrdaZtwwGRWfQWGYTCzLkI05PovDENQStGZsvjyh+b0KFVeHfbTHEtRHfaPaDisNkdpoPg8jqHQl5W395/qMuAGEbkQeArYCeR9KSLSBBwOPFJwzWXAbiAA3Ap8C/jhfg9S6lbvcxYvXnwANPAcv5TL6ZnTZOqjQdrjaXymga0cTxMQTNMgEvRx8Qnuopgru214DoQ39nSyY18SnynMbd/B5f/3ExbvWMczc97DpR/+ErtrJgLQmbRR4C3IJgFTyFgDu+zCPrfqroEino7jOAqfASKC40kcQ9xdv890266ahkHY7woBn+GaoJKWIu2AiaI67GNCJMjWtvjbiX0GIG4lWxtQlpMv6e43DdK2wwnzGuhOWezYlyLsdxsutXSn8Zsms+pCNNWEe5RG773rX99PIcP+GCiJT5ujNFBewbEDmFbwfiqwq/AEpdQu3OKJiEgUOFspVeh1/SdguVKq0Enf7L1Mi8gvcYWPpowMNQ9jsCzinL187sQIK9Z04zOEiooAyaxDY1WI+Y1RGipDfUZNAbyyI0YIm88++we+8NRvSPqDfOMjl/DHw08kEvLhtxS2o1DilvPI2CofmRXP2KT7UTmiQYOgz0fGsrEVTKwM0NKVRom7w6/wu4l2Sin8poHPELKOYnJNmKxls6cz7ZU0N/AbDlnHFQpZRzEhEqCl2y0qCFAZ9OMoRUcyg2WD3294fUDAVDB1QpiAz8Q0HY6bXcvOWAoFzKyPMrchQsBn9hDoff0flOKfKuZ8bY7SlNNU9QJwiIjMEpEArsnpgcITRKReRHJjuAy4vdc9zqOXmcrTQnJVe88E1pRh7JoChmL+yC1AsWS2xwJUGI01pSZMV8qiPhqiOuwj4BOSWYdwwOSoGTVUBE0eXbcnH8k1rzHSw0wyZfN67r3j61zyxJ08fsgxnPyZm7lv0YewEaZOiDB1QpgJFQF8XkkPV0MSuhKZPoWGz3D9GiG/j4bKIPWVIW45/z0c2lRNU00YlMJywHJc7aEuEmByTZi6aJBJ1SGuPecILOWW7BB5u8FT7hc8YzuusDGEjKVoqgphOQpDhMqgH7/plkE5ZdEkPji/gfrKIE3V4fx8f/mkQ5jdEOWYWbUcO6s234NkIDNRTqvL2jZ/39LO81va2NzSza9Wbh3w/L6ipjSaHGXTOJRSloh8CdfMZAK3K6XWisgPgReVUg8AJwBXiYjCNVV9MXe9iMzE1Vj+2uvWvxaRBlxT2Cu4me2aMlPqLrMYh3qhJlMfDbqF/cJw1Iwa9sUzPPtmOyIQDZpksjbb2hOctKCBN7e3c9jNP+arD9xBR7iaZed8lwfnvpeU5eZZ+MQ1SzkOLJxcRdhnsGFPN5Ggj1Qmy77E286FnKPab7htVMHtiRFLZNi+L8E37n2VjoTbWbAy5GNza5ys5ZDKOpy8sJGGylBeC1rQVE1jVYjWrjSgcJTK22sNcetNdaUtaioC+eZKTdUmLV0ZLEfRVB2iMuTHclyNImeiK6RUM9HOjiR+k3z/8Mqgj1TW5ulNbaxvju13rU7i0xRDOU1VKKUeAh7qdex7Ba/vA/oMq1VKvcX+znSUUieO7Cg15aCYBajQXl4V9tOZspjfGMV2FM9ubsdxFNNqw6Qth417u5k3MUr3X57iK7dcDhs28OwHPsolR59PZ7gSpRQB0637FPIbZBzFsbNr8+UzfrxiA+3xDG0JC1c89HS4ZR2wMjZ+U0ikLXymSXXIDV91PBNTJOhnTkOUbW1uUmKuPHksmeXomTVc++hGRCmytoPP85IrL2mwPuJnVn2UhZOriSWzHF9Ry/2v7CZj2URDPt4zvYYJkeCgmlypAnxKTZgnN+wl6DPyVXBFXGd+X1Fxo53Ep4siHpiUVXBo3pkU88de7ALU+7rmzjQtXSkEmFYXJhp0rw+nEpx620855a9/gOnT4ZFH+OlblXTv6CDjhSf5TaGhMsDhU2u47V+O7jHepuoQuztTrhNaQcAnmCKkss7beRFeKdy0pVAIR0yt5YWt+9ys7niGioCPioCPxqog7Yks65s76UxZGALXP/4m8xqjHDO7js5kmg17E4hAOGAwrSaMwqC6wtXAjp5Zw2PrWzhuTh27O1O0x7PsjqX5xOKRdzIvXdTI8lU7mRD2oZQibTmkLYd3T6vuU4sYzZpmuijigYsWHJqS6P3HvqWlm0vu2c20unCPwoPFLkCF91vQ5Fa0/UssxeTqED7D9Q4csfY5Lvr1j6jbt5fVZ5zPkXffxPoumzee/zt+06Q6HMB2lFuaxO8jlrT6vP9JCxrZ1hYnnlGYIpiG9EjSM8SNiAp6kUvzJlWxbV+SnfuSJDI28XSWlq408YzNpMogtqNY2FTFul0xENi4t5us7aDEZPqEsNfJz0cya7NwcpQKTwj+bVNb3ow3qyFKS1eKtbs6ufGJzWzcEx/RXfeCpmqOn1vH2l2ddKdd7eawyVV9OtV7l6SvCvk4bHJ12aKmdFHEA5dyOsc170AK/9jbutNs3Nvt1ZXK9nCAF+tQ78sZO6HCj2Ur/LEO/u32H/Lt679OwhfkM5/5H24668tcu3IXv1q5lbDfDYO1HQfTcP0VuQZI/d4/5McQN5EunraxCpL0aiMBaioCTKkOEfDMOkdMrSYaMBHcKrOWA/WRAGII29oSZCybrrT7zKDPYPVO15cwsSpEdYWfQyZG6EhZrNzczra2OG+1dvPMpjZSWVe4tXSleHlbByiFo5w+gwiGy3Fz60hnHbfbn1J0e0K80KleGMxw6KQqFjZVURnyl9V0VK78IE350RqHpiQKfRebWuIEfUa+xWnvHWPuJ7eT/cUzb+1n2urLF7KgqRL/8uV88/+uJ9wd484P/jPXv+9cFs1q4NBJVcSSWZ72du3hgEMiY5O2HAI+A0OEwybvH/KbY2pdhFQ2S3vS6eHjEHHbpxoi7O1KM6PODZMN+EwOmVRJa1eaZNahNhpgbkOEVds7CPiFTS1xqkJ+UlmboM8gnraZWuP2tjBF+Ptb+/B50VVpy2HDnm5CPoP1zV00VoXzcwhuwuFI77rXN8d4bH0L8ydFaY6laItn3ITBE3s63sdi96+LIh64aMGhKYnCP/bulEU06HaYqwq5f/y9d4yD2bF7Lx6Rtr38y08vZ+HKx9gz9zBu+85NPFc1nfdVBpnVEAXcRS3sN9jeniBlOW44q7fjLyy9UTjerG2zaW+cvZ1J4lmI+g1sBMt2czUq/CaJjE19NEDIb7BoclW+xeo3l87nF8+81SP5sCrkJ5mx6E5ZHDm9mpe2dpC2HCIBk86Uq034TcHxwm/9Bc5p23bY52loXcksflPI2IpFU6r6nMPelOJQLhQIM+vd+Ysls2zcE+cjBeeNRTTVWPaI0QwPbarSlERhyYlo0F0k05bD3IkRYP8d42B5Afn7JTIsWHEfF3z2Ixzy4lPs/fYPaFz/Cl//xjlMr61gRn0kf8/W7hSxZIa41+koYzm0dmfY3ZnirPc09VhEly5qZHt7gpVvtpPMWFSH/JgiGKYQ9Ll9uGfXR7yoriy7O1OYhvBGS5yORCZ/n1zOSY65EyN0p90orNpIkPmN7qI8bYL73ec3Rl1zmycUaiMBwK3sm7ScfNFBxI1yOmpGDfXRniXP+2Kg/Ji+KhgXaw7q/f0GG8dIUO7yKJryoTUOTUn0F0LrKMWTG/ayL5Hl+Ll1eT/HYDvZBU3VfHGm4P/Cxcx4+W/sWLSY9M23MOf4o/Ln99ZKNu2Nk8w4iLi7etMwANfhva2t54Lo5lYEae12s7mjIR+zGyJYjnLLgjiKoM/AEEhkbAKmiWlAZzJLZyJL1rK5/5UdWLZbE6qxKsjimRMI+nzMqo/QWBWkOZbqkXeR0wg27u2mOuz6awx5u66U3zQ4f8mMHkEEftMteT7Yrrs/k9KvVm4lkXX20+wq/AZdKWtQc9BY7f51FvqBiRYcmpIp/GNf3xzjVyu38vSmNiZU+HnfHDejuajmPrYNN97I3MsuA8OAm26i6/RzWbGuhZ33rs6bYXovaq3daVKWQ2XQR0XQ/RVWChIZi1XbezqV1zfHWLW9AxRUhv3MbXA1l5e3dpDM2hw9cwLrm7vY05WmsTLkFhn0eoG0x9O8sqMT5eWGNFYFaenO8JfXWzjp0IksO2Ven4tebn5y47ZtJx926zMkX5Awd24pSX39CeLH17dzzKza/QRKxrKJJbP58/oTCLoGlaYUtODQDIsFTdXUV4Y48dCJ+4V35pr7XPPIRlZ59ZmCPpP6aJALZit4//th5Uo49VS45RbW+6u59aktOI5DcyzFqm37eGTtbr584pwei1pdNEhLVxq/721Lq+0oAj6Dwh59ObNOwKsrlc7avLytg/dMr2FeY5TdXWksB06YP5G1u2IsaKrisfV7qPC5voh42iaVtagJ+7EV1EaCVAS8JkkFNbQGmpvcuAN+kyVz+vZHlLLr7k8QK1SfJqnmmFW0QNC7f02xaMGhGRKFDtq1u2IcMbW6x2JWaI5yvFKwgmDaFqc98GtmPXg7RKNw993wyU+CCCse3YjjuJFHQZ9BXSRAZ8ri+sff5Npzj+CSk+fln/35u1+iPZEl6yW0uSYnk5m1Ffkx5Mw6i6ZU8dLWDoI+IWgKa3d1MrshyuWnL8wvlNc+upFYMpuPkAr5TZJZG8dRdKUtQNixL0FN2I+jKNppPNKLcX8mpSOn1fRrktICQTPSaOe4pmR6O2gDpsELW/bR0pXKn7O1Nc629gRf/91q2uMZFk2p4nx/Kz/76cWcvfwW3ljyIVi/Hs4/P99ndWdHkuZYKl8eQ8RtZGQ5qkeRvQVN1Xxj6XwiAYPutOucV0qBUrTFM/kciJxjuD4a4qgZNYT8JmnbIWur/ZywOSf9pKogqaxr3smVd7dsRSRgYNkOO2MpLMcp2mk80i13+3MoX7Bkhu6ToRk1tMahKZneDtpFU6pY+WY7a3d18oF5Qba2xlm13TUJdSWz+NMp3nXDzXz8yd+SrKnl/u/fwNOHHc81Eyf2uO+UmjCrtu2jzotAAjf3oTbi3y/Ed+OeOIYYGIYQ9plEgz6iQR+t3Rl+tXIrV3zsXT3MOvXREPXRngUJCyk0KyWyNrGkRVcy6wok3MgnAMd2a1EVsyAXhiL7DHhyw16Wr9rJ++fW5Z3jQ6E/DUL7KDSjhRYcmpIpdNC2dqfYtDeOUg5vtcXpfDVLV8qiLhIgEvRxzI41XHTnVUxt3cHyo07l/k9+jarGemb1sWNfuqiRR9buzmd/5+oqzaityO/wCxfjtO0QCfgQEeqiAbe8R4GDvNRIod4L8rJ7V+M3Yc2OTpo7XW1qWm2YyRPCRS3IOQGbsWxe2e5mlNeEfazZ1VmWmkzaJKUZLbTgOAgptSJp7/MDptCVssjaNi9t7UApRUciSyJtI2TxG5Bp38cRv7mGc178E9trJvH5T13N87PfTbXlp21DC3u7Ulz7KPt1l/vyiXO4/vE3aY9nqY34mVFbgWka+R3+ijV7sG2Hdc2ddKUsTANCPjNfhNAtk67y9xvOLjynsZy44G3tIpZ0Q3SL6aiXE7DPb+nMm9+UUvkse12TSXOgogXHQcZIdITb05nGUYr2eAbHcdjblSGZdagM+XCU4vBXV3LFn2+isauVO489kxtP+BR2pALHcfMY6ioDOIo+n/2Rd01hdkO0X8G2dleMHe1Jgn6DaNCgO2UTdywsxyGVtelO2xw7uzY//uHswvvSWLa3J9xscJ856PzlBE8uwx7IZ9n3lYSnS4xrDhS04BjnDGcx6evaUmsS9Xl+bQVZy3ZzExLZfK2lxkwXX3jgJj762l94o346l3zqv/n7pHkETCFoK6bXVmAYQtBn0J22+332QIt9Z8oCgZDfpKm6gm12grTlkHWTyJleV5HvwTFc+tJYMlXBHpVlB5q/t5P7hHTWBhHSlsOiKVX7JeHpEuOaAwktOMYxw1lM+ru2K5VlQVNVj3MHqknUX8JZc8ziwwsn8adXd1FX4Wf+0yu45IHrqUx2c+P7z+OmJedgB/wYtgJx263GMxb10SBpyyHq5RyUWg+pOuwjlsiQytpUBEwmVYVo7U4T8pucMH9iv4J1qAK4L79HXbS4iq45wXP3yq084yVIHjm9Gr9p7udr0SXGNQcSWnCMMQMtaMNZTPq7dmdHcr94/62tcd5sjXPqdU8hCEdOq85H/fROOGvtTrFmZycZ2yFoCjX79vK1O67nfWueYe3keVx43hW83jATwxQiAT81YR9tiSw+EZJZO18A8LDJrvAqtR7SwqZqKvwmuzvTdKayTIgEWNBUycz6aD7Po685HqndfKkVXRc0VXPlx97V4/95YqV/P1/LuuYYsUTWK9HuZ+7ECLWRoC4xrhmXlFVwiMhS4DrcnuM/V0pd3evzGcDtQAPQDpyvlNrhfWYDr3mnblNKne4dnwXcA9QCLwMXKKUyHIAMtqANp2Jpf9dWh309SlBsbY3z/JY2/KZJXcSPAlZubmd3Z5plp8zrYedPWxbPb96HAEfPrOHYvyzn27dfgy+b4Wf/eDF/+uAn6LTA7kgR9fmYVBXEZxrYjlsjqi2eBQXzGqM92q6WUg/JHU+CBU1VPSKlBgqPHcnd/FBrOg1kflvfHGN7WxIEqkJuT/CXtnYwvzGar2ir0YwnyiY4RMQEbgROBnYAL4jIA0qpdQWnXQPcpZS6U0ROBK4CLvA+Syql3t3HrX8EXKuUukdEbgE+A9xcru9RTgZb0IbTr6C/a3Nd+nK7391drpknlbXZGXN7Srj5EGlWrNnDJSfPy9v5/2/1XuJpm9ldu/nkz5dx+IaXeGvRYn5/8fcx5s1jvqc1vbGnk50dKTK2Q8hvsuiQeq9Tn7/Hs4uNcuqtlZ20oIGNe+JF32MkS4aXo6bTijV7mNcYZePebtKWQ9Bn5Ht3XHzCnCHfV6MpF+XUOI4BNimlNgOIyD3AGUCh4FgIXOK9fgL440A3FDcL60Tgn71DdwI/4AAVHIMtaMOpWDrQtYW734vueoE1O9IAKIRkxqIrlaUuEuhRwRbg/pe2cdFL93PBn27DNk2u+8S/s+efPklGGVxTYCYq1KQGenYx9KWVPba+pSQz00g3DBrpfImdHUlm1EeIhnxsaonT7eWxVPWRqKjRjAfKWXJkCrC94P0O71ghq4GzvddnAZUiUue9D4nIiyLynIic6R2rAzqUUrnGAX3dEwAR+Zx3/YstLS3D/S5lYbAeCMPpV1Dstbs6UliOQqEwxc1/SGUdOhLZHgtcOKG9AAASF0lEQVTrC396hp/97Kt8dvkNrDn0KP79+7/hLx84k7W74/stwCPZZ2Gwfh7FUNhDZDyW48j9HjRUhlgyu46TFzaycHJ1j06GGs14opwah/RxTPV6vwy4QUQuBJ4CdgK5lXS6UmqXiMwG/iIirwGdRdzTPajUrcCtAIsXL+7znLGmGI1iOLvbYq5NZCyCpkHGUW7JKG+mMrbDvMYI1z20hkPvvInzfn8bqYooV1/wXVYefTJBvwlZm31Jq88FeKR25cMxMxWauCr8BhnLpjlmjbtyHLoTnuZAo5yCYwcwreD9VGBX4QlKqV3AxwBEJAqcrZSKFXyGUmqziDwJHAn8HqgREZ+ndex3zwOJ8dADIez3Mala6Ehm6U67MjsSMIkEfWz405N88tbLadz6Bn896iSu+8cv0jhnKsGEm9TmN4Xj59aVdbxDNTP1NnHlFuPBNJ+xSMIbD78HGk0plFNwvAAc4kVB7QTO5W3fBAAiUg+0K6Uc4DLcCCtEZAKQUEqlvXOOA36slFIi8gTwcdzIqk8B95fxO+zHSC8s5awvVMxYj5xWzcrN7UyuCeedspnObi76822c+dd7idc2cP/lN/P3w99Hx5vt2LEUH5jXkF+IRyrZrj+GuhsfSiTVWCbh6TpTmgOJsvk4PI3gS8AjwHrgd0qptSLyQxE53TvtBGCDiGwEGoErvOMLgBdFZDWu0/zqgmisbwFfF5FNuD6PX5TrO/RmoH7Ppd5nJEttD2es5y+ZwSyvn3dnKsuiN1Zxx3UX8bEnfstrSz/BXbc9yOYlJ1IfDXHs7AlkbTWi/aEHm4uh+kuK7bVdSKGwaY+nWd/cyas7Ovj+A+vK8n+k0RyolDWPQyn1EPBQr2PfK3h9H3BfH9c9Cxzezz0340ZsjTojkQ8wWrvaYse6oKmaZafM4y/Pb+I9N17Fksd+T2bmLO798Z2snX9UDxNR0Ofj5IWN/SbalUo552IoJq6cP6W1O+U1fjKorfDT3p3R5T80mgJ0I6cSGMoutjcjESXUF7137uuaY0WPdcFLT/PFL53Bkr8sh2XLCKxdw6Lzzyh7JFIxczFULW8okVS56KZXt8fYF8+wsyPJtvYE4YAxIv9HGs07BS04SmCw8NliGAnh05sHX93JJb9dzZ9e3cW2tjhvtXazvS3Jtrb4wGNtaYF//mf46EdhwgS3//f/+39QUTGiIbX9UcxcDFXQDmX8Sxc1sr09wZbWOI5yMATStqIrmSVtWbr8h0bjoWtVlcBIhE2OdDLa+uYY1z/+JgjURQL5jONJVQE27OlmQiS4/1iVgnvuga98BWIxuPxyuPRSCAR63LvcDtti5mI44biljn9BUzWNVUEiQR8Z26EiYDCxMoBpCOt2dXHC/ImD30SjOQjQGkcJjMQufKST0Vas2YPlKKpCbie8kN8k6DNIZB2mTgjvP1a7C04/3dU05syBVavge9/bT2iMBoPNxfrmGNvaEzz82m5Wbm7L9zQfjqAdjIyt+OChDTRWhaiPBqkImKAU+xLjJ2FQoxlrtMZRIsPdhQ8Usz+UUN+dHUlqI37SllsXCiDoM2iLZ1gyp/5tR7bjwG23wTe+AZYF//M/rsZhmkP+LsNlsLm49aktNFUFiSUydCazvLy1g3mNUUzTKFtyXE4Les/0mnz5j9HIV9FoDiS04BgD+hI+Q40wmlITJpO12bi3G3CFRmfKwl/QbpVNm+Cii+DJJ+HEE10BMnt2ub5eSfQniAt9G9GQj01747R2p9ndleby0xeWbRHPmSOrw36OnVU7avkqGs2BhDZVjROG6gReuqgR0zSYNzFK0GfQ7pUu//KJc1jQEIFrroHDD4eXX3YFxmOPjRuhMRCFjvP6aIj3zq7jtMObmF5bUdad/2gEBWg0Bzpa4xgnDNUJXGjuCfhNlszxTFyt2+B9Z8ELL7g+jZtugil91oMcl4x0EEEp6CxujWZgtOAYJwxnoeyx0KXTcOWV7s+ECfDb38InPoFbwfDAQRf+02jGL1pwjCGFzvCAKezpTENtRZ8LZVGO8+eeg898Btatg/PPh5/8BOrq+njy+EcX/tNoxi9acIwRfVVvdZQi20fp70Ed5/E4fPe7rqCYMgUefBBOO22sv+Kw0SYjjWZ8ogXHGNFXLakZdRGqw/79akENWHdq3YtuxNSWLfD5z8PVV0NV1eh+GY1Gc1Cho6rGiFJKj/R1br2VYMnVl8JJJ4HPB3/9q+sA10JDo9GUGa1xDJOh9ucoxRne+9zZzz7Gidf9gEisDb75TfjBDyBcvmijsWhudCCNR6M52NAaxzAYTn+OUkqP5M61mndz6hWXcMYPvkhnZQ1bH3wcfvSjsguNkehB8k4dj0ZzMKI1jmEwnP4cpUQNLZhUxbKWv1P77W/hT8b524Vfo/by/2DB9Pr8OeXahY9ED5KRZLyNR6M5GNGCYxgMp3IrFBk1tG0bXHwxUx5+GJYsgV/8guMWLOhxSjkbIg33O4404208Gs3BiDZVDYOR6M/RL44DN98Mhx3mOr6vuw6efhp6CQ0oX3MoKPN3fAeMR6M5GCmr4BCRpSKyQUQ2icilfXw+Q0QeF5FXReRJEZnqHX+3iKwUkbXeZ+cUXHOHiGwRkVe8n3eX8zsMxEiXSM+zcSOccAJ84Qvw3vfCmjUDVrItR3OoHGX7ju+Q8Wg0ByNlExwiYgI3AqcCC4HzRGRhr9OuAe5SSr0L+CFwlXc8AfyLUuowYCnwExGpKbjuG0qpd3s/r5TrOwzGiBfEsyz48Y/hiCPgtdfg9tvhz3+GWbMGvKycu/DxVvRvvI1HozkYKaeP4xhgk1JqM4CI3AOcAawrOGchcIn3+gngjwBKqY25E5RSu0RkL9AAdJRxvENixLKbV6+Gf/1Xt4rtWWfBjTdCU1NRl5a7rtN4y+Aeb+PRaA42ymmqmgJsL3i/wztWyGrgbO/1WUCliPQoriQixwAB4M2Cw1d4JqxrRSQ4ssMeZVIp+M53YPFi2LkT7rsP/vCHAYXG+uYY1z66kWX3rubaR10Zq3fhGo1mtCinxtFXOVbV6/0y4AYRuRB4CtgJ5G0uItIE3A18SinleIcvA3bjCpNbgW/hmrl6Plzkc8DnAKZPnz6c71E+nn3WLUr4+uvwqU+5Xflqawe8ZKAIqt6lSjQajaYclFPj2AFMK3g/FdhVeIJSapdS6mNKqSOB//COxQBEpAp4EPiOUuq5gmualUsa+CWuSWw/lFK3KqUWK6UWNzQ0jOT3Gj7d3fDVr8Lxx0MiAStWwB13DCo0oLwRVBqNRlMM5RQcLwCHiMgsEQkA5wIPFJ4gIvUikhvDZcDt3vEAsBzXcX5vr2uavH8FOBNYU8bvMPI8+qjbke+nP4UvftGNmDrllKIvL2cElUaj0RRD2QSHUsoCvgQ8AqwHfqeUWisiPxSR073TTgA2iMhGoBG4wjv+T8AHgAv7CLv9tYi8BrwG1AP/Va7vMKLs2+c6vz/8YQgG3ZyM66+HysqSbqPzGDQazVgjSvV2O7zzWLx4sXrxxRfHbgB/+IOrXbS0uEUJv/c9CIUGv64PCn0chRFU2hmu0WhGGhF5SSm1uPdxnTleTnbvho9/HM4+GyZNcvt/X3nlkIUG6DwGjUYz9uhaVeVAKbjrLrjkEtf5feWVsGwZ+P2DX1sEOo9Bo9GMJVpwjDRbt8K//Rs88ggcdxz8/Odw6KFjPSqNRqMZMbSpaqRwHLjhBrco4TPPuI7vp57SQkOj0bzj0BrHSLBhg5vI97e/uaG1P/sZzJgx1qPSaDSasqA1juGQzcJVV7lFCdetgzvvhIcf1kJDo9G8o9Eax1BZtcrNy3jlFTdy6oYboFGX9tZoNO98tMZRKqkUXHYZHH20G277+9/DvfdqoaHRaA4atMZRCs884/oyNm6ET38a/vu/YcKEsR6VRqPRjCpa4yiGri740pfg/e+HTMZtrnT77VpoaDSagxItOAbjkUdg0SK46Sa3ou1rr8HJJ4/1qDQajWbM0IKjP9ra3B4ZS5dCJOKG2v7kJxCNjvXINBqNZkzRgqM3Srld+BYuhN/8xu3Ot2oVLFky1iPTaDSacYF2jhfS3OxWsV2+HI46yvVlHHHEWI9Ko9FoxhVa4wBXy/jlL10t4+GH4Uc/guee00JDo9Fo+kBrHFu2wOc+B4895kZN/fznME/37tZoNJr+OHg1Dtt227cuWgTPP+9GTT35pBYaGo1GMwgHp8axbh189rOwciWceqpblHDatLEelUaj0RwQHFwaRzYL//VfcOSRbvb3r34FDz6ohYZGo9GUwMGjcbz0kluU8NVX4ZxzXDPVxIljPSqNRqM54Dg4BMeOHXDMMW4hwj/+Ec44Y6xHpNFoNAcsB4epas8eV9tYt04LDY1GoxkmopQa6zGUHRFpAbaO9TgGoR5oHetBjEP0vPSPnpv+0XPTP6XMzQylVEPvgweF4DgQEJEXlVKLx3oc4w09L/2j56Z/9Nz0z0jMzcFhqtJoNBrNiKEFh0aj0WhKQguO8cOtYz2AcYqel/7Rc9M/em76Z9hzo30cGo1GoykJrXFoNBqNpiS04NBoNBpNSWjBMYqIyFIR2SAim0Tk0j4+ny4iT4jIKhF5VUROG4txjgVFzM0MEXncm5cnRWTqWIxzLBCR20Vkr4is6edzEZGfenP3qoi8Z7THOFYUMTeHishKEUmLyLLRHt9YUcS8fNL7XXlVRJ4VkZKaD2nBMUqIiAncCJwKLATOE5GFvU77DvA7pdSRwLnATaM7yrGhyLm5BrhLKfUu4IfAVaM7yjHlDmDpAJ+fChzi/XwOuHkUxjReuIOB56Yd+Aru78/BxB0MPC9bgH/w/p7+kxId5lpwjB7HAJuUUpuVUhngHqB3/RMFVHmvq4Fdozi+saSYuVkIPO69fqKPz9+xKKWewl0A++MMXKGqlFLPATUi0jQ6oxtbBpsbpdRepdQLQHb0RjX2FDEvzyql9nlvnwNK0uC14Bg9pgDbC97v8I4V8gPgfBHZATwEfHl0hjbmFDM3q4GzvddnAZUiUjcKYzsQKGb+NJr++AzwcCkXaMExekgfx3rHQp8H3KGUmgqcBtwtIgfD/1Exc7MM+AcRWQX8A7ATsMo9sAOEYuZPo9kPEfkgruD4VinXHRxl1ccHO4DCjlFT2d8U9Rk8u6RSaqWIhHALku0dlRGOHYPOjVJqF/AxABGJAmcrpWKjNsLxTTG/WxpND0TkXcDPgVOVUm2lXHsw7GbHCy8Ah4jILBEJ4Dq/H+h1zjbgQwAisgAIAS2jOsqxYdC5EZH6Au3rMuD2UR7jeOYB4F+86Kr3AjGlVPNYD0ozfhGR6cAfgAuUUhtLvV5rHKOEUsoSkS8BjwAmcLtSaq2I/BB4USn1APDvwG0icgmuqeFCdRCk9hc5NycAV4mIAp4CvjhmAx5lROR/cb9/vef/+j7gB1BK3YLrDzsN2AQkgE+PzUhHn8HmRkQmAS/iBp04IvI1YKFSqnOMhjwqFPE78z2gDrhJRACsUirm6pIjGo1GoykJbarSaDQaTUlowaHRaDSaktCCQ6PRaDQloQWHRqPRaEpCCw6NRqPRlIQWHBrNCCAiZ4mIEpFDBznvQhGZPIznnCAifxrq9RrNSKAFh0YzMpwHPIObvDgQFwJDFhwazXhACw6NZph4JVCOwy0Zc27B8W+KyGsislpErhaRjwOLgV+LyCsiEhaRt0Sk3jt/sYg86b0+xuuTsMr7d/7ofzONpm905rhGM3zOBFYopTaKSLvXSKnRO36sUiohIrVKqXYvQ36ZUupFAC9rty9eBz7gZdWfBFzJ29WBNZoxRQsOjWb4nAf8xHt9j/feAH6plEoAKKUG6qfRF9XAnSJyCG75Gf8IjVWjGTZacGg0w8DrCXIisMiro2XiLvS/p7jS5hZvm4xDBcf/E3hCKXWWiMwEnhyhIWs0w0b7ODSa4fFx3O57M5RSM5VS03DbcrYD/yoiFQAiUuud3wVUFlz/FnCU97rQFFWN23MEXIe6RjNu0IJDoxke5wHLex37PW7k1APAiyLyCm4jKnB7Qd+Sc44DlwPXicjTgF1wjx/jVgP+G64Wo9GMG3R1XI1Go9GUhNY4NBqNRlMSWnBoNBqNpiS04NBoNBpNSWjBodFoNJqS0IJDo9FoNCWhBYdGo9FoSkILDo1Go9GUxP8Hyad8OlgTMgsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.6763059133129764\n",
      "y=0.6689963614762676 + [0.3302135] * x\n"
     ]
    }
   ],
   "source": [
    "model.fit()\n",
    "pred = model.predict(test_)\n",
    "# plot some results\n",
    "model.plot_predicted_actual(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7c849f20053e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-7c849f20053e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'mean'"
     ]
    }
   ],
   "source": [
    "sorted(model.best_param.items(), key=lambda x: x[1]['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(model.best_param[(76, 0.01, 0.01, 0.1)]['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(48, 0.01, 1e-05, 0.1): {'mean': 0.002640368925394424,\n",
       "  'sd': 8.654284126819858e-05,\n",
       "  'iter': 19}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.global_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(y):\n",
    "    return (y - 0.23596519750208478) / 0.7634398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>TESTER</th>\n",
       "      <th>Tester_1345</th>\n",
       "      <th>Tester_1349</th>\n",
       "      <th>Tester_1397</th>\n",
       "      <th>Tester_2636</th>\n",
       "      <th>Tester_2652</th>\n",
       "      <th>Tester_2683</th>\n",
       "      <th>Tester_2689</th>\n",
       "      <th>Tester_2690</th>\n",
       "      <th>Tester_2721</th>\n",
       "      <th>Tester_2724</th>\n",
       "      <th>...</th>\n",
       "      <th>Tester_821</th>\n",
       "      <th>Tester_8218</th>\n",
       "      <th>Tester_8246</th>\n",
       "      <th>Tester_8248</th>\n",
       "      <th>Tester_8249</th>\n",
       "      <th>Tester_8250</th>\n",
       "      <th>Tester_8253</th>\n",
       "      <th>Tester_8254</th>\n",
       "      <th>Tester_828</th>\n",
       "      <th>Tester_829</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INBRED</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Inbred_1071</th>\n",
       "      <td>0.980409</td>\n",
       "      <td>0.994684</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.976543</td>\n",
       "      <td>0.974165</td>\n",
       "      <td>0.995130</td>\n",
       "      <td>0.983780</td>\n",
       "      <td>0.988432</td>\n",
       "      <td>0.992565</td>\n",
       "      <td>0.978904</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005097</td>\n",
       "      <td>0.977598</td>\n",
       "      <td>0.968560</td>\n",
       "      <td>0.993774</td>\n",
       "      <td>0.959135</td>\n",
       "      <td>1.008035</td>\n",
       "      <td>0.990623</td>\n",
       "      <td>0.959323</td>\n",
       "      <td>0.982385</td>\n",
       "      <td>0.962652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_122</th>\n",
       "      <td>0.993112</td>\n",
       "      <td>1.005223</td>\n",
       "      <td>1.011692</td>\n",
       "      <td>0.984792</td>\n",
       "      <td>0.990794</td>\n",
       "      <td>1.008257</td>\n",
       "      <td>1.001655</td>\n",
       "      <td>1.002477</td>\n",
       "      <td>1.010452</td>\n",
       "      <td>0.995606</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018659</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.983338</td>\n",
       "      <td>1.006296</td>\n",
       "      <td>0.972745</td>\n",
       "      <td>1.021928</td>\n",
       "      <td>1.008961</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>0.995285</td>\n",
       "      <td>0.976474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1337</th>\n",
       "      <td>0.983181</td>\n",
       "      <td>1.003391</td>\n",
       "      <td>1.001426</td>\n",
       "      <td>0.977210</td>\n",
       "      <td>0.983971</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.992549</td>\n",
       "      <td>1.002466</td>\n",
       "      <td>1.001547</td>\n",
       "      <td>0.989862</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008577</td>\n",
       "      <td>0.990513</td>\n",
       "      <td>0.977442</td>\n",
       "      <td>1.003136</td>\n",
       "      <td>0.967909</td>\n",
       "      <td>1.016893</td>\n",
       "      <td>1.002535</td>\n",
       "      <td>0.962382</td>\n",
       "      <td>0.991178</td>\n",
       "      <td>0.968920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1339</th>\n",
       "      <td>0.986603</td>\n",
       "      <td>1.002087</td>\n",
       "      <td>1.000080</td>\n",
       "      <td>0.978259</td>\n",
       "      <td>0.981661</td>\n",
       "      <td>0.993162</td>\n",
       "      <td>0.993589</td>\n",
       "      <td>1.001733</td>\n",
       "      <td>1.001172</td>\n",
       "      <td>0.987293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007692</td>\n",
       "      <td>0.990252</td>\n",
       "      <td>0.982025</td>\n",
       "      <td>1.006938</td>\n",
       "      <td>0.965393</td>\n",
       "      <td>1.011832</td>\n",
       "      <td>1.001622</td>\n",
       "      <td>0.965333</td>\n",
       "      <td>0.990852</td>\n",
       "      <td>0.974823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1340</th>\n",
       "      <td>0.979284</td>\n",
       "      <td>1.003437</td>\n",
       "      <td>0.993136</td>\n",
       "      <td>0.985227</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.996891</td>\n",
       "      <td>0.988880</td>\n",
       "      <td>1.001007</td>\n",
       "      <td>0.997370</td>\n",
       "      <td>0.982940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005008</td>\n",
       "      <td>0.986178</td>\n",
       "      <td>0.974426</td>\n",
       "      <td>0.998075</td>\n",
       "      <td>0.962446</td>\n",
       "      <td>1.003060</td>\n",
       "      <td>0.997263</td>\n",
       "      <td>0.963366</td>\n",
       "      <td>0.984245</td>\n",
       "      <td>0.972932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1341</th>\n",
       "      <td>0.990734</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>1.003887</td>\n",
       "      <td>0.976886</td>\n",
       "      <td>0.983818</td>\n",
       "      <td>0.996142</td>\n",
       "      <td>0.996278</td>\n",
       "      <td>0.998232</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>0.993054</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012624</td>\n",
       "      <td>0.990135</td>\n",
       "      <td>0.982485</td>\n",
       "      <td>1.001268</td>\n",
       "      <td>0.967268</td>\n",
       "      <td>1.013093</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>0.965196</td>\n",
       "      <td>0.985462</td>\n",
       "      <td>0.974415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1342</th>\n",
       "      <td>0.996584</td>\n",
       "      <td>1.001778</td>\n",
       "      <td>1.009628</td>\n",
       "      <td>0.990728</td>\n",
       "      <td>0.994449</td>\n",
       "      <td>1.013513</td>\n",
       "      <td>1.009237</td>\n",
       "      <td>1.013236</td>\n",
       "      <td>1.011852</td>\n",
       "      <td>1.002786</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022258</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.987266</td>\n",
       "      <td>1.008959</td>\n",
       "      <td>0.978068</td>\n",
       "      <td>1.023008</td>\n",
       "      <td>1.017347</td>\n",
       "      <td>0.976844</td>\n",
       "      <td>1.000676</td>\n",
       "      <td>0.983296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1344</th>\n",
       "      <td>0.982995</td>\n",
       "      <td>0.996668</td>\n",
       "      <td>1.002748</td>\n",
       "      <td>0.978651</td>\n",
       "      <td>0.987840</td>\n",
       "      <td>0.995289</td>\n",
       "      <td>0.988381</td>\n",
       "      <td>0.996842</td>\n",
       "      <td>1.001897</td>\n",
       "      <td>0.989058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010684</td>\n",
       "      <td>0.993665</td>\n",
       "      <td>0.980866</td>\n",
       "      <td>1.000672</td>\n",
       "      <td>0.969134</td>\n",
       "      <td>1.008994</td>\n",
       "      <td>1.003156</td>\n",
       "      <td>0.969949</td>\n",
       "      <td>0.988291</td>\n",
       "      <td>0.974854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1345</th>\n",
       "      <td>0.985641</td>\n",
       "      <td>1.001043</td>\n",
       "      <td>1.001369</td>\n",
       "      <td>0.984036</td>\n",
       "      <td>0.982353</td>\n",
       "      <td>1.002619</td>\n",
       "      <td>0.993185</td>\n",
       "      <td>1.001421</td>\n",
       "      <td>1.002370</td>\n",
       "      <td>0.988553</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014250</td>\n",
       "      <td>0.990240</td>\n",
       "      <td>0.977142</td>\n",
       "      <td>1.001610</td>\n",
       "      <td>0.966857</td>\n",
       "      <td>1.008551</td>\n",
       "      <td>1.005593</td>\n",
       "      <td>0.966727</td>\n",
       "      <td>0.991769</td>\n",
       "      <td>0.971657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1346</th>\n",
       "      <td>0.987535</td>\n",
       "      <td>1.004796</td>\n",
       "      <td>1.008348</td>\n",
       "      <td>0.985109</td>\n",
       "      <td>0.990876</td>\n",
       "      <td>1.009594</td>\n",
       "      <td>0.996730</td>\n",
       "      <td>1.007754</td>\n",
       "      <td>1.007920</td>\n",
       "      <td>1.002045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013621</td>\n",
       "      <td>0.997709</td>\n",
       "      <td>0.987703</td>\n",
       "      <td>1.014958</td>\n",
       "      <td>0.976277</td>\n",
       "      <td>1.017296</td>\n",
       "      <td>1.008281</td>\n",
       "      <td>0.975834</td>\n",
       "      <td>0.997755</td>\n",
       "      <td>0.979706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1347</th>\n",
       "      <td>0.986006</td>\n",
       "      <td>1.005276</td>\n",
       "      <td>1.000564</td>\n",
       "      <td>0.988387</td>\n",
       "      <td>0.984386</td>\n",
       "      <td>1.004401</td>\n",
       "      <td>0.993111</td>\n",
       "      <td>1.001932</td>\n",
       "      <td>1.005727</td>\n",
       "      <td>0.993253</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007407</td>\n",
       "      <td>0.987348</td>\n",
       "      <td>0.975984</td>\n",
       "      <td>1.007981</td>\n",
       "      <td>0.969719</td>\n",
       "      <td>1.012010</td>\n",
       "      <td>1.003508</td>\n",
       "      <td>0.965522</td>\n",
       "      <td>0.991686</td>\n",
       "      <td>0.980041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1348</th>\n",
       "      <td>0.972204</td>\n",
       "      <td>0.978912</td>\n",
       "      <td>0.987823</td>\n",
       "      <td>0.958712</td>\n",
       "      <td>0.964943</td>\n",
       "      <td>0.981692</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>0.985437</td>\n",
       "      <td>0.987041</td>\n",
       "      <td>0.975665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989018</td>\n",
       "      <td>0.976662</td>\n",
       "      <td>0.962686</td>\n",
       "      <td>0.985210</td>\n",
       "      <td>0.953325</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.986122</td>\n",
       "      <td>0.948583</td>\n",
       "      <td>0.975190</td>\n",
       "      <td>0.954469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1349</th>\n",
       "      <td>0.994366</td>\n",
       "      <td>1.010742</td>\n",
       "      <td>1.016306</td>\n",
       "      <td>0.984300</td>\n",
       "      <td>0.992187</td>\n",
       "      <td>1.004665</td>\n",
       "      <td>1.001915</td>\n",
       "      <td>1.008058</td>\n",
       "      <td>1.013331</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017643</td>\n",
       "      <td>1.003429</td>\n",
       "      <td>0.991511</td>\n",
       "      <td>1.008267</td>\n",
       "      <td>0.972806</td>\n",
       "      <td>1.024252</td>\n",
       "      <td>1.010654</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.979645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1350</th>\n",
       "      <td>1.012278</td>\n",
       "      <td>1.019688</td>\n",
       "      <td>1.024257</td>\n",
       "      <td>0.996634</td>\n",
       "      <td>1.002873</td>\n",
       "      <td>1.022793</td>\n",
       "      <td>1.008106</td>\n",
       "      <td>1.017114</td>\n",
       "      <td>1.025664</td>\n",
       "      <td>1.008994</td>\n",
       "      <td>...</td>\n",
       "      <td>1.029129</td>\n",
       "      <td>1.013654</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>1.022883</td>\n",
       "      <td>0.984060</td>\n",
       "      <td>1.034517</td>\n",
       "      <td>1.019979</td>\n",
       "      <td>0.988956</td>\n",
       "      <td>1.009684</td>\n",
       "      <td>0.990897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1351</th>\n",
       "      <td>0.974277</td>\n",
       "      <td>0.984699</td>\n",
       "      <td>0.989962</td>\n",
       "      <td>0.965861</td>\n",
       "      <td>0.969184</td>\n",
       "      <td>0.980603</td>\n",
       "      <td>0.983247</td>\n",
       "      <td>0.982487</td>\n",
       "      <td>0.987904</td>\n",
       "      <td>0.978780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.966268</td>\n",
       "      <td>0.990615</td>\n",
       "      <td>0.952116</td>\n",
       "      <td>0.994493</td>\n",
       "      <td>0.988509</td>\n",
       "      <td>0.958644</td>\n",
       "      <td>0.973255</td>\n",
       "      <td>0.956597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1352</th>\n",
       "      <td>0.986065</td>\n",
       "      <td>1.005773</td>\n",
       "      <td>1.004770</td>\n",
       "      <td>0.980408</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>0.995913</td>\n",
       "      <td>0.988084</td>\n",
       "      <td>1.001525</td>\n",
       "      <td>1.004441</td>\n",
       "      <td>0.993944</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006142</td>\n",
       "      <td>0.988646</td>\n",
       "      <td>0.980461</td>\n",
       "      <td>1.003645</td>\n",
       "      <td>0.968073</td>\n",
       "      <td>1.008934</td>\n",
       "      <td>1.005205</td>\n",
       "      <td>0.967378</td>\n",
       "      <td>0.994481</td>\n",
       "      <td>0.971988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1353</th>\n",
       "      <td>0.983748</td>\n",
       "      <td>0.994380</td>\n",
       "      <td>0.993504</td>\n",
       "      <td>0.978412</td>\n",
       "      <td>0.983752</td>\n",
       "      <td>0.994274</td>\n",
       "      <td>0.993389</td>\n",
       "      <td>0.994038</td>\n",
       "      <td>0.995081</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005780</td>\n",
       "      <td>0.987794</td>\n",
       "      <td>0.970432</td>\n",
       "      <td>1.002756</td>\n",
       "      <td>0.961482</td>\n",
       "      <td>1.003809</td>\n",
       "      <td>0.992230</td>\n",
       "      <td>0.968965</td>\n",
       "      <td>0.983521</td>\n",
       "      <td>0.964866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1354</th>\n",
       "      <td>0.986369</td>\n",
       "      <td>1.002751</td>\n",
       "      <td>1.001406</td>\n",
       "      <td>0.985224</td>\n",
       "      <td>0.980569</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>0.991953</td>\n",
       "      <td>1.001037</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.994704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006592</td>\n",
       "      <td>0.992791</td>\n",
       "      <td>0.977675</td>\n",
       "      <td>1.000611</td>\n",
       "      <td>0.968075</td>\n",
       "      <td>1.011364</td>\n",
       "      <td>1.000578</td>\n",
       "      <td>0.970716</td>\n",
       "      <td>0.988078</td>\n",
       "      <td>0.981160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1355</th>\n",
       "      <td>0.996667</td>\n",
       "      <td>1.010028</td>\n",
       "      <td>1.020887</td>\n",
       "      <td>0.989858</td>\n",
       "      <td>0.997209</td>\n",
       "      <td>1.013511</td>\n",
       "      <td>1.007564</td>\n",
       "      <td>1.010426</td>\n",
       "      <td>1.011904</td>\n",
       "      <td>1.009500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024835</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>0.992730</td>\n",
       "      <td>1.013407</td>\n",
       "      <td>0.980667</td>\n",
       "      <td>1.019842</td>\n",
       "      <td>1.015768</td>\n",
       "      <td>0.980801</td>\n",
       "      <td>1.003978</td>\n",
       "      <td>0.982103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1357</th>\n",
       "      <td>0.992364</td>\n",
       "      <td>1.005652</td>\n",
       "      <td>1.007424</td>\n",
       "      <td>0.988756</td>\n",
       "      <td>0.991412</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>0.997503</td>\n",
       "      <td>1.004847</td>\n",
       "      <td>1.005478</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017513</td>\n",
       "      <td>0.990578</td>\n",
       "      <td>0.981726</td>\n",
       "      <td>1.006284</td>\n",
       "      <td>0.974327</td>\n",
       "      <td>1.016765</td>\n",
       "      <td>1.007948</td>\n",
       "      <td>0.968808</td>\n",
       "      <td>0.995628</td>\n",
       "      <td>0.976094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1358</th>\n",
       "      <td>1.012085</td>\n",
       "      <td>1.020553</td>\n",
       "      <td>1.026835</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>1.002579</td>\n",
       "      <td>1.020724</td>\n",
       "      <td>1.019114</td>\n",
       "      <td>1.023302</td>\n",
       "      <td>1.029273</td>\n",
       "      <td>1.009291</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028168</td>\n",
       "      <td>1.014105</td>\n",
       "      <td>1.001954</td>\n",
       "      <td>1.023378</td>\n",
       "      <td>0.986769</td>\n",
       "      <td>1.037895</td>\n",
       "      <td>1.024220</td>\n",
       "      <td>0.984748</td>\n",
       "      <td>1.011999</td>\n",
       "      <td>0.990053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1359</th>\n",
       "      <td>0.986741</td>\n",
       "      <td>1.007385</td>\n",
       "      <td>1.005824</td>\n",
       "      <td>0.983435</td>\n",
       "      <td>0.982688</td>\n",
       "      <td>0.991474</td>\n",
       "      <td>1.004925</td>\n",
       "      <td>1.004377</td>\n",
       "      <td>1.003687</td>\n",
       "      <td>0.990656</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014806</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>0.986400</td>\n",
       "      <td>1.002047</td>\n",
       "      <td>0.969860</td>\n",
       "      <td>1.014170</td>\n",
       "      <td>1.003132</td>\n",
       "      <td>0.964876</td>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.972910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1360</th>\n",
       "      <td>0.983477</td>\n",
       "      <td>1.001658</td>\n",
       "      <td>0.998824</td>\n",
       "      <td>0.985474</td>\n",
       "      <td>0.989191</td>\n",
       "      <td>1.004546</td>\n",
       "      <td>0.997026</td>\n",
       "      <td>1.001003</td>\n",
       "      <td>1.006109</td>\n",
       "      <td>0.990403</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012055</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>0.978270</td>\n",
       "      <td>1.001573</td>\n",
       "      <td>0.969121</td>\n",
       "      <td>1.010258</td>\n",
       "      <td>1.001624</td>\n",
       "      <td>0.971591</td>\n",
       "      <td>0.988260</td>\n",
       "      <td>0.975650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1361</th>\n",
       "      <td>0.984845</td>\n",
       "      <td>1.001273</td>\n",
       "      <td>1.002712</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.983640</td>\n",
       "      <td>1.002897</td>\n",
       "      <td>0.990575</td>\n",
       "      <td>0.997387</td>\n",
       "      <td>1.000427</td>\n",
       "      <td>0.990912</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013385</td>\n",
       "      <td>0.989952</td>\n",
       "      <td>0.976230</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.968570</td>\n",
       "      <td>1.012425</td>\n",
       "      <td>1.003068</td>\n",
       "      <td>0.969595</td>\n",
       "      <td>0.989143</td>\n",
       "      <td>0.972274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1364</th>\n",
       "      <td>0.978663</td>\n",
       "      <td>0.992002</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.970413</td>\n",
       "      <td>0.973644</td>\n",
       "      <td>0.990635</td>\n",
       "      <td>0.987925</td>\n",
       "      <td>0.993813</td>\n",
       "      <td>0.992720</td>\n",
       "      <td>0.987265</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002614</td>\n",
       "      <td>0.981710</td>\n",
       "      <td>0.970325</td>\n",
       "      <td>0.998042</td>\n",
       "      <td>0.963643</td>\n",
       "      <td>1.006317</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.960111</td>\n",
       "      <td>0.980591</td>\n",
       "      <td>0.963462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1365</th>\n",
       "      <td>0.991965</td>\n",
       "      <td>1.006516</td>\n",
       "      <td>1.001920</td>\n",
       "      <td>0.989448</td>\n",
       "      <td>0.988054</td>\n",
       "      <td>1.010584</td>\n",
       "      <td>1.004298</td>\n",
       "      <td>1.009459</td>\n",
       "      <td>1.008347</td>\n",
       "      <td>0.996858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019539</td>\n",
       "      <td>0.995855</td>\n",
       "      <td>0.986018</td>\n",
       "      <td>1.010585</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>1.020309</td>\n",
       "      <td>1.010756</td>\n",
       "      <td>0.973392</td>\n",
       "      <td>0.994375</td>\n",
       "      <td>0.981952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1366</th>\n",
       "      <td>1.013238</td>\n",
       "      <td>1.030162</td>\n",
       "      <td>1.033369</td>\n",
       "      <td>1.009662</td>\n",
       "      <td>1.009809</td>\n",
       "      <td>1.030763</td>\n",
       "      <td>1.019270</td>\n",
       "      <td>1.030586</td>\n",
       "      <td>1.028017</td>\n",
       "      <td>1.022557</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038241</td>\n",
       "      <td>1.022452</td>\n",
       "      <td>1.011798</td>\n",
       "      <td>1.029189</td>\n",
       "      <td>0.997612</td>\n",
       "      <td>1.039928</td>\n",
       "      <td>1.032357</td>\n",
       "      <td>0.996674</td>\n",
       "      <td>1.018761</td>\n",
       "      <td>1.006881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1367</th>\n",
       "      <td>0.990982</td>\n",
       "      <td>1.007072</td>\n",
       "      <td>1.004958</td>\n",
       "      <td>0.985465</td>\n",
       "      <td>0.985601</td>\n",
       "      <td>1.003689</td>\n",
       "      <td>0.994609</td>\n",
       "      <td>1.002368</td>\n",
       "      <td>1.005966</td>\n",
       "      <td>0.992494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014494</td>\n",
       "      <td>0.992579</td>\n",
       "      <td>0.988507</td>\n",
       "      <td>1.003532</td>\n",
       "      <td>0.971075</td>\n",
       "      <td>1.020302</td>\n",
       "      <td>1.006124</td>\n",
       "      <td>0.971198</td>\n",
       "      <td>0.991817</td>\n",
       "      <td>0.979357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1368</th>\n",
       "      <td>0.988792</td>\n",
       "      <td>1.009082</td>\n",
       "      <td>1.003181</td>\n",
       "      <td>0.984914</td>\n",
       "      <td>0.979432</td>\n",
       "      <td>0.991044</td>\n",
       "      <td>0.996928</td>\n",
       "      <td>0.997187</td>\n",
       "      <td>1.001085</td>\n",
       "      <td>0.987306</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>0.987324</td>\n",
       "      <td>0.982477</td>\n",
       "      <td>1.000902</td>\n",
       "      <td>0.967305</td>\n",
       "      <td>1.014822</td>\n",
       "      <td>1.001548</td>\n",
       "      <td>0.964227</td>\n",
       "      <td>0.989249</td>\n",
       "      <td>0.969967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1369</th>\n",
       "      <td>0.986989</td>\n",
       "      <td>0.999387</td>\n",
       "      <td>1.000753</td>\n",
       "      <td>0.979675</td>\n",
       "      <td>0.983415</td>\n",
       "      <td>1.006858</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>1.001528</td>\n",
       "      <td>1.004523</td>\n",
       "      <td>0.990677</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014054</td>\n",
       "      <td>0.992554</td>\n",
       "      <td>0.982174</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.969612</td>\n",
       "      <td>1.015096</td>\n",
       "      <td>1.002364</td>\n",
       "      <td>0.963640</td>\n",
       "      <td>0.985089</td>\n",
       "      <td>0.975277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_771</th>\n",
       "      <td>1.012683</td>\n",
       "      <td>1.021208</td>\n",
       "      <td>1.025507</td>\n",
       "      <td>1.002805</td>\n",
       "      <td>1.005684</td>\n",
       "      <td>1.015289</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>1.021554</td>\n",
       "      <td>1.024093</td>\n",
       "      <td>1.011293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033476</td>\n",
       "      <td>1.015389</td>\n",
       "      <td>0.997322</td>\n",
       "      <td>1.020983</td>\n",
       "      <td>0.987704</td>\n",
       "      <td>1.038217</td>\n",
       "      <td>1.021249</td>\n",
       "      <td>0.993606</td>\n",
       "      <td>1.016552</td>\n",
       "      <td>0.991856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_773</th>\n",
       "      <td>1.014538</td>\n",
       "      <td>1.031584</td>\n",
       "      <td>1.029797</td>\n",
       "      <td>1.016666</td>\n",
       "      <td>1.013919</td>\n",
       "      <td>1.029277</td>\n",
       "      <td>1.024694</td>\n",
       "      <td>1.035250</td>\n",
       "      <td>1.032276</td>\n",
       "      <td>1.023089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047824</td>\n",
       "      <td>1.023791</td>\n",
       "      <td>1.011978</td>\n",
       "      <td>1.033999</td>\n",
       "      <td>1.001845</td>\n",
       "      <td>1.043226</td>\n",
       "      <td>1.038862</td>\n",
       "      <td>1.001949</td>\n",
       "      <td>1.023036</td>\n",
       "      <td>1.010320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_774</th>\n",
       "      <td>0.981185</td>\n",
       "      <td>1.005784</td>\n",
       "      <td>1.001465</td>\n",
       "      <td>0.984754</td>\n",
       "      <td>0.990232</td>\n",
       "      <td>0.998727</td>\n",
       "      <td>0.999064</td>\n",
       "      <td>1.008128</td>\n",
       "      <td>1.007016</td>\n",
       "      <td>0.995871</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011769</td>\n",
       "      <td>0.995981</td>\n",
       "      <td>0.984225</td>\n",
       "      <td>1.001723</td>\n",
       "      <td>0.975522</td>\n",
       "      <td>1.010516</td>\n",
       "      <td>1.010560</td>\n",
       "      <td>0.968175</td>\n",
       "      <td>0.992774</td>\n",
       "      <td>0.980532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_776</th>\n",
       "      <td>0.976955</td>\n",
       "      <td>0.988704</td>\n",
       "      <td>0.993313</td>\n",
       "      <td>0.972290</td>\n",
       "      <td>0.977887</td>\n",
       "      <td>0.993534</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.990019</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>0.977832</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002983</td>\n",
       "      <td>0.981401</td>\n",
       "      <td>0.966759</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.960431</td>\n",
       "      <td>1.004098</td>\n",
       "      <td>0.991671</td>\n",
       "      <td>0.956797</td>\n",
       "      <td>0.978182</td>\n",
       "      <td>0.962198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_777</th>\n",
       "      <td>0.985314</td>\n",
       "      <td>1.002532</td>\n",
       "      <td>1.005324</td>\n",
       "      <td>0.982679</td>\n",
       "      <td>0.981374</td>\n",
       "      <td>0.999718</td>\n",
       "      <td>0.990896</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>1.002924</td>\n",
       "      <td>0.990716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006604</td>\n",
       "      <td>0.991383</td>\n",
       "      <td>0.984601</td>\n",
       "      <td>1.004775</td>\n",
       "      <td>0.967783</td>\n",
       "      <td>1.009057</td>\n",
       "      <td>1.006205</td>\n",
       "      <td>0.973151</td>\n",
       "      <td>0.994895</td>\n",
       "      <td>0.969039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_779</th>\n",
       "      <td>0.972204</td>\n",
       "      <td>0.986956</td>\n",
       "      <td>0.988778</td>\n",
       "      <td>0.969720</td>\n",
       "      <td>0.972913</td>\n",
       "      <td>0.988988</td>\n",
       "      <td>0.978934</td>\n",
       "      <td>0.984807</td>\n",
       "      <td>0.988293</td>\n",
       "      <td>0.976411</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001717</td>\n",
       "      <td>0.975894</td>\n",
       "      <td>0.967530</td>\n",
       "      <td>0.987591</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>0.988080</td>\n",
       "      <td>0.959897</td>\n",
       "      <td>0.974727</td>\n",
       "      <td>0.958087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_781</th>\n",
       "      <td>0.985014</td>\n",
       "      <td>1.001517</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.982624</td>\n",
       "      <td>0.986434</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>0.991623</td>\n",
       "      <td>1.000893</td>\n",
       "      <td>1.001930</td>\n",
       "      <td>0.991824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010578</td>\n",
       "      <td>0.990727</td>\n",
       "      <td>0.979858</td>\n",
       "      <td>1.003880</td>\n",
       "      <td>0.965469</td>\n",
       "      <td>1.009047</td>\n",
       "      <td>0.998748</td>\n",
       "      <td>0.968356</td>\n",
       "      <td>0.990160</td>\n",
       "      <td>0.973687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_782</th>\n",
       "      <td>0.974887</td>\n",
       "      <td>0.992984</td>\n",
       "      <td>0.992017</td>\n",
       "      <td>0.973078</td>\n",
       "      <td>0.972776</td>\n",
       "      <td>0.985758</td>\n",
       "      <td>0.989375</td>\n",
       "      <td>0.985497</td>\n",
       "      <td>0.992061</td>\n",
       "      <td>0.977740</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004502</td>\n",
       "      <td>0.984265</td>\n",
       "      <td>0.975649</td>\n",
       "      <td>0.986729</td>\n",
       "      <td>0.959397</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.990913</td>\n",
       "      <td>0.957482</td>\n",
       "      <td>0.973863</td>\n",
       "      <td>0.967156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_783</th>\n",
       "      <td>0.983040</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>1.000339</td>\n",
       "      <td>0.980290</td>\n",
       "      <td>0.977896</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.991895</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>1.003916</td>\n",
       "      <td>0.993089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012059</td>\n",
       "      <td>0.988054</td>\n",
       "      <td>0.981020</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.966226</td>\n",
       "      <td>1.007341</td>\n",
       "      <td>1.000194</td>\n",
       "      <td>0.959326</td>\n",
       "      <td>0.983848</td>\n",
       "      <td>0.971872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_785</th>\n",
       "      <td>0.986271</td>\n",
       "      <td>1.003313</td>\n",
       "      <td>1.006096</td>\n",
       "      <td>0.979509</td>\n",
       "      <td>0.987176</td>\n",
       "      <td>0.989649</td>\n",
       "      <td>0.992020</td>\n",
       "      <td>1.000504</td>\n",
       "      <td>1.000273</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012902</td>\n",
       "      <td>0.994937</td>\n",
       "      <td>0.982571</td>\n",
       "      <td>1.000390</td>\n",
       "      <td>0.962921</td>\n",
       "      <td>1.020726</td>\n",
       "      <td>1.000442</td>\n",
       "      <td>0.973378</td>\n",
       "      <td>0.993122</td>\n",
       "      <td>0.970993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_787</th>\n",
       "      <td>0.980598</td>\n",
       "      <td>0.996307</td>\n",
       "      <td>1.001361</td>\n",
       "      <td>0.978398</td>\n",
       "      <td>0.981875</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.002470</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013038</td>\n",
       "      <td>0.990101</td>\n",
       "      <td>0.974586</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.963165</td>\n",
       "      <td>1.007297</td>\n",
       "      <td>1.005341</td>\n",
       "      <td>0.961933</td>\n",
       "      <td>0.985694</td>\n",
       "      <td>0.969027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_789</th>\n",
       "      <td>1.001304</td>\n",
       "      <td>1.013434</td>\n",
       "      <td>1.012871</td>\n",
       "      <td>0.993024</td>\n",
       "      <td>0.997267</td>\n",
       "      <td>1.012849</td>\n",
       "      <td>1.002341</td>\n",
       "      <td>1.010481</td>\n",
       "      <td>1.010981</td>\n",
       "      <td>1.002888</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025869</td>\n",
       "      <td>1.002512</td>\n",
       "      <td>0.994281</td>\n",
       "      <td>1.018937</td>\n",
       "      <td>0.978196</td>\n",
       "      <td>1.025276</td>\n",
       "      <td>1.011783</td>\n",
       "      <td>0.986253</td>\n",
       "      <td>1.001839</td>\n",
       "      <td>0.984112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_790</th>\n",
       "      <td>0.989185</td>\n",
       "      <td>1.008799</td>\n",
       "      <td>1.009835</td>\n",
       "      <td>0.989670</td>\n",
       "      <td>0.988487</td>\n",
       "      <td>1.006699</td>\n",
       "      <td>1.001244</td>\n",
       "      <td>1.006330</td>\n",
       "      <td>1.008517</td>\n",
       "      <td>0.997464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019496</td>\n",
       "      <td>1.002880</td>\n",
       "      <td>0.989913</td>\n",
       "      <td>1.009043</td>\n",
       "      <td>0.975837</td>\n",
       "      <td>1.019026</td>\n",
       "      <td>1.013528</td>\n",
       "      <td>0.976333</td>\n",
       "      <td>0.995402</td>\n",
       "      <td>0.985142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_791</th>\n",
       "      <td>0.984970</td>\n",
       "      <td>1.003915</td>\n",
       "      <td>1.006809</td>\n",
       "      <td>0.979011</td>\n",
       "      <td>0.985676</td>\n",
       "      <td>1.002229</td>\n",
       "      <td>0.991980</td>\n",
       "      <td>1.002990</td>\n",
       "      <td>1.006935</td>\n",
       "      <td>0.991596</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008316</td>\n",
       "      <td>0.995242</td>\n",
       "      <td>0.981466</td>\n",
       "      <td>1.005249</td>\n",
       "      <td>0.970154</td>\n",
       "      <td>1.012876</td>\n",
       "      <td>1.007302</td>\n",
       "      <td>0.968349</td>\n",
       "      <td>0.993694</td>\n",
       "      <td>0.967011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_792</th>\n",
       "      <td>0.976414</td>\n",
       "      <td>0.993066</td>\n",
       "      <td>0.992953</td>\n",
       "      <td>0.979177</td>\n",
       "      <td>0.974876</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.982881</td>\n",
       "      <td>0.994151</td>\n",
       "      <td>0.996565</td>\n",
       "      <td>0.977813</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000522</td>\n",
       "      <td>0.984396</td>\n",
       "      <td>0.974773</td>\n",
       "      <td>0.997245</td>\n",
       "      <td>0.962381</td>\n",
       "      <td>1.000898</td>\n",
       "      <td>0.992627</td>\n",
       "      <td>0.956488</td>\n",
       "      <td>0.980759</td>\n",
       "      <td>0.972310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_793</th>\n",
       "      <td>0.974064</td>\n",
       "      <td>0.994485</td>\n",
       "      <td>0.993102</td>\n",
       "      <td>0.975495</td>\n",
       "      <td>0.978957</td>\n",
       "      <td>0.990106</td>\n",
       "      <td>0.986482</td>\n",
       "      <td>0.991480</td>\n",
       "      <td>1.000052</td>\n",
       "      <td>0.977662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005381</td>\n",
       "      <td>0.989225</td>\n",
       "      <td>0.973099</td>\n",
       "      <td>0.997227</td>\n",
       "      <td>0.961555</td>\n",
       "      <td>1.003377</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>0.960341</td>\n",
       "      <td>0.979547</td>\n",
       "      <td>0.973189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_794</th>\n",
       "      <td>0.984498</td>\n",
       "      <td>1.001433</td>\n",
       "      <td>1.003981</td>\n",
       "      <td>0.981674</td>\n",
       "      <td>0.985587</td>\n",
       "      <td>0.996665</td>\n",
       "      <td>0.987401</td>\n",
       "      <td>0.996251</td>\n",
       "      <td>1.006394</td>\n",
       "      <td>0.987822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013820</td>\n",
       "      <td>0.992904</td>\n",
       "      <td>0.974938</td>\n",
       "      <td>1.002088</td>\n",
       "      <td>0.965385</td>\n",
       "      <td>1.013525</td>\n",
       "      <td>1.001314</td>\n",
       "      <td>0.972838</td>\n",
       "      <td>0.995897</td>\n",
       "      <td>0.965372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_796</th>\n",
       "      <td>0.986359</td>\n",
       "      <td>1.003029</td>\n",
       "      <td>0.996023</td>\n",
       "      <td>0.981691</td>\n",
       "      <td>0.983211</td>\n",
       "      <td>0.995648</td>\n",
       "      <td>0.993223</td>\n",
       "      <td>1.000417</td>\n",
       "      <td>0.998090</td>\n",
       "      <td>0.986504</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008162</td>\n",
       "      <td>0.990057</td>\n",
       "      <td>0.979069</td>\n",
       "      <td>1.006440</td>\n",
       "      <td>0.963179</td>\n",
       "      <td>1.010277</td>\n",
       "      <td>0.996957</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>0.989601</td>\n",
       "      <td>0.975658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_799</th>\n",
       "      <td>0.983363</td>\n",
       "      <td>1.001059</td>\n",
       "      <td>1.004279</td>\n",
       "      <td>0.979173</td>\n",
       "      <td>0.983130</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.993124</td>\n",
       "      <td>1.000661</td>\n",
       "      <td>1.000223</td>\n",
       "      <td>0.994388</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010196</td>\n",
       "      <td>0.990049</td>\n",
       "      <td>0.977690</td>\n",
       "      <td>1.003117</td>\n",
       "      <td>0.966184</td>\n",
       "      <td>1.011255</td>\n",
       "      <td>1.003523</td>\n",
       "      <td>0.969115</td>\n",
       "      <td>0.992107</td>\n",
       "      <td>0.972310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_800</th>\n",
       "      <td>0.983710</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>1.002690</td>\n",
       "      <td>0.979385</td>\n",
       "      <td>0.984645</td>\n",
       "      <td>1.000373</td>\n",
       "      <td>0.992091</td>\n",
       "      <td>0.996964</td>\n",
       "      <td>1.000619</td>\n",
       "      <td>0.991692</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013603</td>\n",
       "      <td>0.992905</td>\n",
       "      <td>0.980076</td>\n",
       "      <td>1.003917</td>\n",
       "      <td>0.970153</td>\n",
       "      <td>1.013102</td>\n",
       "      <td>0.997914</td>\n",
       "      <td>0.970003</td>\n",
       "      <td>0.992121</td>\n",
       "      <td>0.972649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_801</th>\n",
       "      <td>0.979287</td>\n",
       "      <td>0.991852</td>\n",
       "      <td>1.000744</td>\n",
       "      <td>0.973468</td>\n",
       "      <td>0.977531</td>\n",
       "      <td>0.990461</td>\n",
       "      <td>0.987515</td>\n",
       "      <td>0.994060</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.989208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005076</td>\n",
       "      <td>0.986361</td>\n",
       "      <td>0.971625</td>\n",
       "      <td>0.995284</td>\n",
       "      <td>0.967153</td>\n",
       "      <td>1.006653</td>\n",
       "      <td>0.997958</td>\n",
       "      <td>0.957530</td>\n",
       "      <td>0.984419</td>\n",
       "      <td>0.968476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_803</th>\n",
       "      <td>0.991930</td>\n",
       "      <td>1.004238</td>\n",
       "      <td>1.007813</td>\n",
       "      <td>0.988171</td>\n",
       "      <td>0.989758</td>\n",
       "      <td>1.002732</td>\n",
       "      <td>0.994845</td>\n",
       "      <td>1.004625</td>\n",
       "      <td>1.006822</td>\n",
       "      <td>0.998209</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015252</td>\n",
       "      <td>0.995180</td>\n",
       "      <td>0.985764</td>\n",
       "      <td>1.006869</td>\n",
       "      <td>0.970097</td>\n",
       "      <td>1.014704</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>0.973981</td>\n",
       "      <td>0.993028</td>\n",
       "      <td>0.975052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_804</th>\n",
       "      <td>0.985343</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>1.003852</td>\n",
       "      <td>0.977143</td>\n",
       "      <td>0.983671</td>\n",
       "      <td>1.004409</td>\n",
       "      <td>0.985711</td>\n",
       "      <td>1.000567</td>\n",
       "      <td>1.003469</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011928</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.978227</td>\n",
       "      <td>1.002004</td>\n",
       "      <td>0.963595</td>\n",
       "      <td>1.012462</td>\n",
       "      <td>1.005526</td>\n",
       "      <td>0.967324</td>\n",
       "      <td>0.990364</td>\n",
       "      <td>0.964424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_805</th>\n",
       "      <td>1.003995</td>\n",
       "      <td>1.016539</td>\n",
       "      <td>1.019129</td>\n",
       "      <td>0.996998</td>\n",
       "      <td>1.002927</td>\n",
       "      <td>1.012533</td>\n",
       "      <td>1.009426</td>\n",
       "      <td>1.016522</td>\n",
       "      <td>1.016813</td>\n",
       "      <td>1.008650</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028385</td>\n",
       "      <td>1.006763</td>\n",
       "      <td>0.996114</td>\n",
       "      <td>1.019109</td>\n",
       "      <td>0.986626</td>\n",
       "      <td>1.030247</td>\n",
       "      <td>1.016463</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>1.011850</td>\n",
       "      <td>0.987929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_810</th>\n",
       "      <td>0.970069</td>\n",
       "      <td>0.987933</td>\n",
       "      <td>0.988071</td>\n",
       "      <td>0.965165</td>\n",
       "      <td>0.966407</td>\n",
       "      <td>0.983297</td>\n",
       "      <td>0.981676</td>\n",
       "      <td>0.987789</td>\n",
       "      <td>0.987117</td>\n",
       "      <td>0.972874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990700</td>\n",
       "      <td>0.978224</td>\n",
       "      <td>0.966092</td>\n",
       "      <td>0.988950</td>\n",
       "      <td>0.951701</td>\n",
       "      <td>0.997253</td>\n",
       "      <td>0.988998</td>\n",
       "      <td>0.952409</td>\n",
       "      <td>0.976948</td>\n",
       "      <td>0.956669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_818</th>\n",
       "      <td>0.960623</td>\n",
       "      <td>0.973917</td>\n",
       "      <td>0.977398</td>\n",
       "      <td>0.957202</td>\n",
       "      <td>0.957659</td>\n",
       "      <td>0.973140</td>\n",
       "      <td>0.964996</td>\n",
       "      <td>0.980554</td>\n",
       "      <td>0.976111</td>\n",
       "      <td>0.970071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984301</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>0.951403</td>\n",
       "      <td>0.976937</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.985912</td>\n",
       "      <td>0.978551</td>\n",
       "      <td>0.940589</td>\n",
       "      <td>0.964013</td>\n",
       "      <td>0.951296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_819</th>\n",
       "      <td>0.987173</td>\n",
       "      <td>0.999843</td>\n",
       "      <td>1.002623</td>\n",
       "      <td>0.984998</td>\n",
       "      <td>0.988027</td>\n",
       "      <td>1.004846</td>\n",
       "      <td>0.998810</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>1.008788</td>\n",
       "      <td>0.994137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012747</td>\n",
       "      <td>0.993502</td>\n",
       "      <td>0.982811</td>\n",
       "      <td>1.002573</td>\n",
       "      <td>0.970140</td>\n",
       "      <td>1.019393</td>\n",
       "      <td>1.010899</td>\n",
       "      <td>0.959790</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.973737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_821</th>\n",
       "      <td>0.996637</td>\n",
       "      <td>1.010187</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.989644</td>\n",
       "      <td>0.994489</td>\n",
       "      <td>1.010343</td>\n",
       "      <td>0.995183</td>\n",
       "      <td>1.012663</td>\n",
       "      <td>1.013075</td>\n",
       "      <td>1.000434</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014305</td>\n",
       "      <td>1.003472</td>\n",
       "      <td>0.995632</td>\n",
       "      <td>1.011939</td>\n",
       "      <td>0.973786</td>\n",
       "      <td>1.021362</td>\n",
       "      <td>1.008337</td>\n",
       "      <td>0.979931</td>\n",
       "      <td>1.001972</td>\n",
       "      <td>0.984514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_828</th>\n",
       "      <td>1.006311</td>\n",
       "      <td>1.016128</td>\n",
       "      <td>1.016300</td>\n",
       "      <td>0.995067</td>\n",
       "      <td>0.996732</td>\n",
       "      <td>1.012995</td>\n",
       "      <td>1.014969</td>\n",
       "      <td>1.019466</td>\n",
       "      <td>1.018950</td>\n",
       "      <td>1.004171</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.006098</td>\n",
       "      <td>0.995987</td>\n",
       "      <td>1.018762</td>\n",
       "      <td>0.978540</td>\n",
       "      <td>1.031831</td>\n",
       "      <td>1.020410</td>\n",
       "      <td>0.979587</td>\n",
       "      <td>1.000721</td>\n",
       "      <td>0.987451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_836</th>\n",
       "      <td>0.975653</td>\n",
       "      <td>0.989030</td>\n",
       "      <td>0.995654</td>\n",
       "      <td>0.970493</td>\n",
       "      <td>0.966354</td>\n",
       "      <td>0.988670</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>0.984908</td>\n",
       "      <td>0.995144</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.980482</td>\n",
       "      <td>0.964862</td>\n",
       "      <td>0.988631</td>\n",
       "      <td>0.957032</td>\n",
       "      <td>1.000761</td>\n",
       "      <td>0.994189</td>\n",
       "      <td>0.950851</td>\n",
       "      <td>0.975176</td>\n",
       "      <td>0.967962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>593 rows  496 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "TESTER       Tester_1345  Tester_1349  Tester_1397  Tester_2636  Tester_2652  \\\n",
       "INBRED                                                                         \n",
       "Inbred_1071     0.980409     0.994684     0.995833     0.976543     0.974165   \n",
       "Inbred_122      0.993112     1.005223     1.011692     0.984792     0.990794   \n",
       "Inbred_1337     0.983181     1.003391     1.001426     0.977210     0.983971   \n",
       "Inbred_1339     0.986603     1.002087     1.000080     0.978259     0.981661   \n",
       "Inbred_1340     0.979284     1.003437     0.993136     0.985227     0.982593   \n",
       "Inbred_1341     0.990734     0.999071     1.003887     0.976886     0.983818   \n",
       "Inbred_1342     0.996584     1.001778     1.009628     0.990728     0.994449   \n",
       "Inbred_1344     0.982995     0.996668     1.002748     0.978651     0.987840   \n",
       "Inbred_1345     0.985641     1.001043     1.001369     0.984036     0.982353   \n",
       "Inbred_1346     0.987535     1.004796     1.008348     0.985109     0.990876   \n",
       "Inbred_1347     0.986006     1.005276     1.000564     0.988387     0.984386   \n",
       "Inbred_1348     0.972204     0.978912     0.987823     0.958712     0.964943   \n",
       "Inbred_1349     0.994366     1.010742     1.016306     0.984300     0.992187   \n",
       "Inbred_1350     1.012278     1.019688     1.024257     0.996634     1.002873   \n",
       "Inbred_1351     0.974277     0.984699     0.989962     0.965861     0.969184   \n",
       "Inbred_1352     0.986065     1.005773     1.004770     0.980408     0.978125   \n",
       "Inbred_1353     0.983748     0.994380     0.993504     0.978412     0.983752   \n",
       "Inbred_1354     0.986369     1.002751     1.001406     0.985224     0.980569   \n",
       "Inbred_1355     0.996667     1.010028     1.020887     0.989858     0.997209   \n",
       "Inbred_1357     0.992364     1.005652     1.007424     0.988756     0.991412   \n",
       "Inbred_1358     1.012085     1.020553     1.026835     0.999711     1.002579   \n",
       "Inbred_1359     0.986741     1.007385     1.005824     0.983435     0.982688   \n",
       "Inbred_1360     0.983477     1.001658     0.998824     0.985474     0.989191   \n",
       "Inbred_1361     0.984845     1.001273     1.002712     0.981104     0.983640   \n",
       "Inbred_1364     0.978663     0.992002     0.994444     0.970413     0.973644   \n",
       "Inbred_1365     0.991965     1.006516     1.001920     0.989448     0.988054   \n",
       "Inbred_1366     1.013238     1.030162     1.033369     1.009662     1.009809   \n",
       "Inbred_1367     0.990982     1.007072     1.004958     0.985465     0.985601   \n",
       "Inbred_1368     0.988792     1.009082     1.003181     0.984914     0.979432   \n",
       "Inbred_1369     0.986989     0.999387     1.000753     0.979675     0.983415   \n",
       "...                  ...          ...          ...          ...          ...   \n",
       "Inbred_771      1.012683     1.021208     1.025507     1.002805     1.005684   \n",
       "Inbred_773      1.014538     1.031584     1.029797     1.016666     1.013919   \n",
       "Inbred_774      0.981185     1.005784     1.001465     0.984754     0.990232   \n",
       "Inbred_776      0.976955     0.988704     0.993313     0.972290     0.977887   \n",
       "Inbred_777      0.985314     1.002532     1.005324     0.982679     0.981374   \n",
       "Inbred_779      0.972204     0.986956     0.988778     0.969720     0.972913   \n",
       "Inbred_781      0.985014     1.001517     0.999996     0.982624     0.986434   \n",
       "Inbred_782      0.974887     0.992984     0.992017     0.973078     0.972776   \n",
       "Inbred_783      0.983040     0.999716     1.000339     0.980290     0.977896   \n",
       "Inbred_785      0.986271     1.003313     1.006096     0.979509     0.987176   \n",
       "Inbred_787      0.980598     0.996307     1.001361     0.978398     0.981875   \n",
       "Inbred_789      1.001304     1.013434     1.012871     0.993024     0.997267   \n",
       "Inbred_790      0.989185     1.008799     1.009835     0.989670     0.988487   \n",
       "Inbred_791      0.984970     1.003915     1.006809     0.979011     0.985676   \n",
       "Inbred_792      0.976414     0.993066     0.992953     0.979177     0.974876   \n",
       "Inbred_793      0.974064     0.994485     0.993102     0.975495     0.978957   \n",
       "Inbred_794      0.984498     1.001433     1.003981     0.981674     0.985587   \n",
       "Inbred_796      0.986359     1.003029     0.996023     0.981691     0.983211   \n",
       "Inbred_799      0.983363     1.001059     1.004279     0.979173     0.983130   \n",
       "Inbred_800      0.983710     0.999581     1.002690     0.979385     0.984645   \n",
       "Inbred_801      0.979287     0.991852     1.000744     0.973468     0.977531   \n",
       "Inbred_803      0.991930     1.004238     1.007813     0.988171     0.989758   \n",
       "Inbred_804      0.985343     0.999222     1.003852     0.977143     0.983671   \n",
       "Inbred_805      1.003995     1.016539     1.019129     0.996998     1.002927   \n",
       "Inbred_810      0.970069     0.987933     0.988071     0.965165     0.966407   \n",
       "Inbred_818      0.960623     0.973917     0.977398     0.957202     0.957659   \n",
       "Inbred_819      0.987173     0.999843     1.002623     0.984998     0.988027   \n",
       "Inbred_821      0.996637     1.010187     1.009766     0.989644     0.994489   \n",
       "Inbred_828      1.006311     1.016128     1.016300     0.995067     0.996732   \n",
       "Inbred_836      0.975653     0.989030     0.995654     0.970493     0.966354   \n",
       "\n",
       "TESTER       Tester_2683  Tester_2689  Tester_2690  Tester_2721  Tester_2724  \\\n",
       "INBRED                                                                         \n",
       "Inbred_1071     0.995130     0.983780     0.988432     0.992565     0.978904   \n",
       "Inbred_122      1.008257     1.001655     1.002477     1.010452     0.995606   \n",
       "Inbred_1337     0.999385     0.992549     1.002466     1.001547     0.989862   \n",
       "Inbred_1339     0.993162     0.993589     1.001733     1.001172     0.987293   \n",
       "Inbred_1340     0.996891     0.988880     1.001007     0.997370     0.982940   \n",
       "Inbred_1341     0.996142     0.996278     0.998232     0.999010     0.993054   \n",
       "Inbred_1342     1.013513     1.009237     1.013236     1.011852     1.002786   \n",
       "Inbred_1344     0.995289     0.988381     0.996842     1.001897     0.989058   \n",
       "Inbred_1345     1.002619     0.993185     1.001421     1.002370     0.988553   \n",
       "Inbred_1346     1.009594     0.996730     1.007754     1.007920     1.002045   \n",
       "Inbred_1347     1.004401     0.993111     1.001932     1.005727     0.993253   \n",
       "Inbred_1348     0.981692     0.980335     0.985437     0.987041     0.975665   \n",
       "Inbred_1349     1.004665     1.001915     1.008058     1.013331     0.997229   \n",
       "Inbred_1350     1.022793     1.008106     1.017114     1.025664     1.008994   \n",
       "Inbred_1351     0.980603     0.983247     0.982487     0.987904     0.978780   \n",
       "Inbred_1352     0.995913     0.988084     1.001525     1.004441     0.993944   \n",
       "Inbred_1353     0.994274     0.993389     0.994038     0.995081     0.983345   \n",
       "Inbred_1354     0.996822     0.991953     1.001037     0.998869     0.994704   \n",
       "Inbred_1355     1.013511     1.007564     1.010426     1.011904     1.009500   \n",
       "Inbred_1357     1.000769     0.997503     1.004847     1.005478     0.999168   \n",
       "Inbred_1358     1.020724     1.019114     1.023302     1.029273     1.009291   \n",
       "Inbred_1359     0.991474     1.004925     1.004377     1.003687     0.990656   \n",
       "Inbred_1360     1.004546     0.997026     1.001003     1.006109     0.990403   \n",
       "Inbred_1361     1.002897     0.990575     0.997387     1.000427     0.990912   \n",
       "Inbred_1364     0.990635     0.987925     0.993813     0.992720     0.987265   \n",
       "Inbred_1365     1.010584     1.004298     1.009459     1.008347     0.996858   \n",
       "Inbred_1366     1.030763     1.019270     1.030586     1.028017     1.022557   \n",
       "Inbred_1367     1.003689     0.994609     1.002368     1.005966     0.992494   \n",
       "Inbred_1368     0.991044     0.996928     0.997187     1.001085     0.987306   \n",
       "Inbred_1369     1.006858     0.990557     1.001528     1.004523     0.990677   \n",
       "...                  ...          ...          ...          ...          ...   \n",
       "Inbred_771      1.015289     1.012019     1.021554     1.024093     1.011293   \n",
       "Inbred_773      1.029277     1.024694     1.035250     1.032276     1.023089   \n",
       "Inbred_774      0.998727     0.999064     1.008128     1.007016     0.995871   \n",
       "Inbred_776      0.993534     0.986486     0.990019     0.994321     0.977832   \n",
       "Inbred_777      0.999718     0.990896     0.999521     1.002924     0.990716   \n",
       "Inbred_779      0.988988     0.978934     0.984807     0.988293     0.976411   \n",
       "Inbred_781      0.996747     0.991623     1.000893     1.001930     0.991824   \n",
       "Inbred_782      0.985758     0.989375     0.985497     0.992061     0.977740   \n",
       "Inbred_783      0.998959     0.991895     0.995238     1.003916     0.993089   \n",
       "Inbred_785      0.989649     0.992020     1.000504     1.000273     0.992259   \n",
       "Inbred_787      0.999077     0.996550     0.992899     1.002470     0.986014   \n",
       "Inbred_789      1.012849     1.002341     1.010481     1.010981     1.002888   \n",
       "Inbred_790      1.006699     1.001244     1.006330     1.008517     0.997464   \n",
       "Inbred_791      1.002229     0.991980     1.002990     1.006935     0.991596   \n",
       "Inbred_792      0.991807     0.982881     0.994151     0.996565     0.977813   \n",
       "Inbred_793      0.990106     0.986482     0.991480     1.000052     0.977662   \n",
       "Inbred_794      0.996665     0.987401     0.996251     1.006394     0.987822   \n",
       "Inbred_796      0.995648     0.993223     1.000417     0.998090     0.986504   \n",
       "Inbred_799      0.999395     0.993124     1.000661     1.000223     0.994388   \n",
       "Inbred_800      1.000373     0.992091     0.996964     1.000619     0.991692   \n",
       "Inbred_801      0.990461     0.987515     0.994060     0.996622     0.989208   \n",
       "Inbred_803      1.002732     0.994845     1.004625     1.006822     0.998209   \n",
       "Inbred_804      1.004409     0.985711     1.000567     1.003469     0.994321   \n",
       "Inbred_805      1.012533     1.009426     1.016522     1.016813     1.008650   \n",
       "Inbred_810      0.983297     0.981676     0.987789     0.987117     0.972874   \n",
       "Inbred_818      0.973140     0.964996     0.980554     0.976111     0.970071   \n",
       "Inbred_819      1.004846     0.998810     0.999735     1.008788     0.994137   \n",
       "Inbred_821      1.010343     0.995183     1.012663     1.013075     1.000434   \n",
       "Inbred_828      1.012995     1.014969     1.019466     1.018950     1.004171   \n",
       "Inbred_836      0.988670     0.979492     0.984908     0.995144     0.981132   \n",
       "\n",
       "TESTER       ...  Tester_821  Tester_8218  Tester_8246  Tester_8248  \\\n",
       "INBRED       ...                                                      \n",
       "Inbred_1071  ...    1.005097     0.977598     0.968560     0.993774   \n",
       "Inbred_122   ...    1.018659     0.999634     0.983338     1.006296   \n",
       "Inbred_1337  ...    1.008577     0.990513     0.977442     1.003136   \n",
       "Inbred_1339  ...    1.007692     0.990252     0.982025     1.006938   \n",
       "Inbred_1340  ...    1.005008     0.986178     0.974426     0.998075   \n",
       "Inbred_1341  ...    1.012624     0.990135     0.982485     1.001268   \n",
       "Inbred_1342  ...    1.022258     1.001805     0.987266     1.008959   \n",
       "Inbred_1344  ...    1.010684     0.993665     0.980866     1.000672   \n",
       "Inbred_1345  ...    1.014250     0.990240     0.977142     1.001610   \n",
       "Inbred_1346  ...    1.013621     0.997709     0.987703     1.014958   \n",
       "Inbred_1347  ...    1.007407     0.987348     0.975984     1.007981   \n",
       "Inbred_1348  ...    0.989018     0.976662     0.962686     0.985210   \n",
       "Inbred_1349  ...    1.017643     1.003429     0.991511     1.008267   \n",
       "Inbred_1350  ...    1.029129     1.013654     0.999308     1.022883   \n",
       "Inbred_1351  ...    0.998320     0.977521     0.966268     0.990615   \n",
       "Inbred_1352  ...    1.006142     0.988646     0.980461     1.003645   \n",
       "Inbred_1353  ...    1.005780     0.987794     0.970432     1.002756   \n",
       "Inbred_1354  ...    1.006592     0.992791     0.977675     1.000611   \n",
       "Inbred_1355  ...    1.024835     1.002719     0.992730     1.013407   \n",
       "Inbred_1357  ...    1.017513     0.990578     0.981726     1.006284   \n",
       "Inbred_1358  ...    1.028168     1.014105     1.001954     1.023378   \n",
       "Inbred_1359  ...    1.014806     0.993381     0.986400     1.002047   \n",
       "Inbred_1360  ...    1.012055     0.992444     0.978270     1.001573   \n",
       "Inbred_1361  ...    1.013385     0.989952     0.976230     0.999006   \n",
       "Inbred_1364  ...    1.002614     0.981710     0.970325     0.998042   \n",
       "Inbred_1365  ...    1.019539     0.995855     0.986018     1.010585   \n",
       "Inbred_1366  ...    1.038241     1.022452     1.011798     1.029189   \n",
       "Inbred_1367  ...    1.014494     0.992579     0.988507     1.003532   \n",
       "Inbred_1368  ...    1.011500     0.987324     0.982477     1.000902   \n",
       "Inbred_1369  ...    1.014054     0.992554     0.982174     0.998292   \n",
       "...          ...         ...          ...          ...          ...   \n",
       "Inbred_771   ...    1.033476     1.015389     0.997322     1.020983   \n",
       "Inbred_773   ...    1.047824     1.023791     1.011978     1.033999   \n",
       "Inbred_774   ...    1.011769     0.995981     0.984225     1.001723   \n",
       "Inbred_776   ...    1.002983     0.981401     0.966759     0.988162   \n",
       "Inbred_777   ...    1.006604     0.991383     0.984601     1.004775   \n",
       "Inbred_779   ...    1.001717     0.975894     0.967530     0.987591   \n",
       "Inbred_781   ...    1.010578     0.990727     0.979858     1.003880   \n",
       "Inbred_782   ...    1.004502     0.984265     0.975649     0.986729   \n",
       "Inbred_783   ...    1.012059     0.988054     0.981020     0.998519   \n",
       "Inbred_785   ...    1.012902     0.994937     0.982571     1.000390   \n",
       "Inbred_787   ...    1.013038     0.990101     0.974586     0.996986   \n",
       "Inbred_789   ...    1.025869     1.002512     0.994281     1.018937   \n",
       "Inbred_790   ...    1.019496     1.002880     0.989913     1.009043   \n",
       "Inbred_791   ...    1.008316     0.995242     0.981466     1.005249   \n",
       "Inbred_792   ...    1.000522     0.984396     0.974773     0.997245   \n",
       "Inbred_793   ...    1.005381     0.989225     0.973099     0.997227   \n",
       "Inbred_794   ...    1.013820     0.992904     0.974938     1.002088   \n",
       "Inbred_796   ...    1.008162     0.990057     0.979069     1.006440   \n",
       "Inbred_799   ...    1.010196     0.990049     0.977690     1.003117   \n",
       "Inbred_800   ...    1.013603     0.992905     0.980076     1.003917   \n",
       "Inbred_801   ...    1.005076     0.986361     0.971625     0.995284   \n",
       "Inbred_803   ...    1.015252     0.995180     0.985764     1.006869   \n",
       "Inbred_804   ...    1.011928     0.992288     0.978227     1.002004   \n",
       "Inbred_805   ...    1.028385     1.006763     0.996114     1.019109   \n",
       "Inbred_810   ...    0.990700     0.978224     0.966092     0.988950   \n",
       "Inbred_818   ...    0.984301     0.968297     0.951403     0.976937   \n",
       "Inbred_819   ...    1.012747     0.993502     0.982811     1.002573   \n",
       "Inbred_821   ...    1.014305     1.003472     0.995632     1.011939   \n",
       "Inbred_828   ...    1.024400     1.006098     0.995987     1.018762   \n",
       "Inbred_836   ...    0.996000     0.980482     0.964862     0.988631   \n",
       "\n",
       "TESTER       Tester_8249  Tester_8250  Tester_8253  Tester_8254  Tester_828  \\\n",
       "INBRED                                                                        \n",
       "Inbred_1071     0.959135     1.008035     0.990623     0.959323    0.982385   \n",
       "Inbred_122      0.972745     1.021928     1.008961     0.974354    0.995285   \n",
       "Inbred_1337     0.967909     1.016893     1.002535     0.962382    0.991178   \n",
       "Inbred_1339     0.965393     1.011832     1.001622     0.965333    0.990852   \n",
       "Inbred_1340     0.962446     1.003060     0.997263     0.963366    0.984245   \n",
       "Inbred_1341     0.967268     1.013093     0.998827     0.965196    0.985462   \n",
       "Inbred_1342     0.978068     1.023008     1.017347     0.976844    1.000676   \n",
       "Inbred_1344     0.969134     1.008994     1.003156     0.969949    0.988291   \n",
       "Inbred_1345     0.966857     1.008551     1.005593     0.966727    0.991769   \n",
       "Inbred_1346     0.976277     1.017296     1.008281     0.975834    0.997755   \n",
       "Inbred_1347     0.969719     1.012010     1.003508     0.965522    0.991686   \n",
       "Inbred_1348     0.953325     0.997930     0.986122     0.948583    0.975190   \n",
       "Inbred_1349     0.972806     1.024252     1.010654     0.969828    0.998250   \n",
       "Inbred_1350     0.984060     1.034517     1.019979     0.988956    1.009684   \n",
       "Inbred_1351     0.952116     0.994493     0.988509     0.958644    0.973255   \n",
       "Inbred_1352     0.968073     1.008934     1.005205     0.967378    0.994481   \n",
       "Inbred_1353     0.961482     1.003809     0.992230     0.968965    0.983521   \n",
       "Inbred_1354     0.968075     1.011364     1.000578     0.970716    0.988078   \n",
       "Inbred_1355     0.980667     1.019842     1.015768     0.980801    1.003978   \n",
       "Inbred_1357     0.974327     1.016765     1.007948     0.968808    0.995628   \n",
       "Inbred_1358     0.986769     1.037895     1.024220     0.984748    1.011999   \n",
       "Inbred_1359     0.969860     1.014170     1.003132     0.964876    0.993405   \n",
       "Inbred_1360     0.969121     1.010258     1.001624     0.971591    0.988260   \n",
       "Inbred_1361     0.968570     1.012425     1.003068     0.969595    0.989143   \n",
       "Inbred_1364     0.963643     1.006317     0.998650     0.960111    0.980591   \n",
       "Inbred_1365     0.974891     1.020309     1.010756     0.973392    0.994375   \n",
       "Inbred_1366     0.997612     1.039928     1.032357     0.996674    1.018761   \n",
       "Inbred_1367     0.971075     1.020302     1.006124     0.971198    0.991817   \n",
       "Inbred_1368     0.967305     1.014822     1.001548     0.964227    0.989249   \n",
       "Inbred_1369     0.969612     1.015096     1.002364     0.963640    0.985089   \n",
       "...                  ...          ...          ...          ...         ...   \n",
       "Inbred_771      0.987704     1.038217     1.021249     0.993606    1.016552   \n",
       "Inbred_773      1.001845     1.043226     1.038862     1.001949    1.023036   \n",
       "Inbred_774      0.975522     1.010516     1.010560     0.968175    0.992774   \n",
       "Inbred_776      0.960431     1.004098     0.991671     0.956797    0.978182   \n",
       "Inbred_777      0.967783     1.009057     1.006205     0.973151    0.994895   \n",
       "Inbred_779      0.950990     0.993590     0.988080     0.959897    0.974727   \n",
       "Inbred_781      0.965469     1.009047     0.998748     0.968356    0.990160   \n",
       "Inbred_782      0.959397     0.999329     0.990913     0.957482    0.973863   \n",
       "Inbred_783      0.966226     1.007341     1.000194     0.959326    0.983848   \n",
       "Inbred_785      0.962921     1.020726     1.000442     0.973378    0.993122   \n",
       "Inbred_787      0.963165     1.007297     1.005341     0.961933    0.985694   \n",
       "Inbred_789      0.978196     1.025276     1.011783     0.986253    1.001839   \n",
       "Inbred_790      0.975837     1.019026     1.013528     0.976333    0.995402   \n",
       "Inbred_791      0.970154     1.012876     1.007302     0.968349    0.993694   \n",
       "Inbred_792      0.962381     1.000898     0.992627     0.956488    0.980759   \n",
       "Inbred_793      0.961555     1.003377     0.995311     0.960341    0.979547   \n",
       "Inbred_794      0.965385     1.013525     1.001314     0.972838    0.995897   \n",
       "Inbred_796      0.963179     1.010277     0.996957     0.972335    0.989601   \n",
       "Inbred_799      0.966184     1.011255     1.003523     0.969115    0.992107   \n",
       "Inbred_800      0.970153     1.013102     0.997914     0.970003    0.992121   \n",
       "Inbred_801      0.967153     1.006653     0.997958     0.957530    0.984419   \n",
       "Inbred_803      0.970097     1.014704     1.004944     0.973981    0.993028   \n",
       "Inbred_804      0.963595     1.012462     1.005526     0.967324    0.990364   \n",
       "Inbred_805      0.986626     1.030247     1.016463     0.983165    1.011850   \n",
       "Inbred_810      0.951701     0.997253     0.988998     0.952409    0.976948   \n",
       "Inbred_818      0.941300     0.985912     0.978551     0.940589    0.964013   \n",
       "Inbred_819      0.970140     1.019393     1.010899     0.959790    0.991402   \n",
       "Inbred_821      0.973786     1.021362     1.008337     0.979931    1.001972   \n",
       "Inbred_828      0.978540     1.031831     1.020410     0.979587    1.000721   \n",
       "Inbred_836      0.957032     1.000761     0.994189     0.950851    0.975176   \n",
       "\n",
       "TESTER       Tester_829  \n",
       "INBRED                   \n",
       "Inbred_1071    0.962652  \n",
       "Inbred_122     0.976474  \n",
       "Inbred_1337    0.968920  \n",
       "Inbred_1339    0.974823  \n",
       "Inbred_1340    0.972932  \n",
       "Inbred_1341    0.974415  \n",
       "Inbred_1342    0.983296  \n",
       "Inbred_1344    0.974854  \n",
       "Inbred_1345    0.971657  \n",
       "Inbred_1346    0.979706  \n",
       "Inbred_1347    0.980041  \n",
       "Inbred_1348    0.954469  \n",
       "Inbred_1349    0.979645  \n",
       "Inbred_1350    0.990897  \n",
       "Inbred_1351    0.956597  \n",
       "Inbred_1352    0.971988  \n",
       "Inbred_1353    0.964866  \n",
       "Inbred_1354    0.981160  \n",
       "Inbred_1355    0.982103  \n",
       "Inbred_1357    0.976094  \n",
       "Inbred_1358    0.990053  \n",
       "Inbred_1359    0.972910  \n",
       "Inbred_1360    0.975650  \n",
       "Inbred_1361    0.972274  \n",
       "Inbred_1364    0.963462  \n",
       "Inbred_1365    0.981952  \n",
       "Inbred_1366    1.006881  \n",
       "Inbred_1367    0.979357  \n",
       "Inbred_1368    0.969967  \n",
       "Inbred_1369    0.975277  \n",
       "...                 ...  \n",
       "Inbred_771     0.991856  \n",
       "Inbred_773     1.010320  \n",
       "Inbred_774     0.980532  \n",
       "Inbred_776     0.962198  \n",
       "Inbred_777     0.969039  \n",
       "Inbred_779     0.958087  \n",
       "Inbred_781     0.973687  \n",
       "Inbred_782     0.967156  \n",
       "Inbred_783     0.971872  \n",
       "Inbred_785     0.970993  \n",
       "Inbred_787     0.969027  \n",
       "Inbred_789     0.984112  \n",
       "Inbred_790     0.985142  \n",
       "Inbred_791     0.967011  \n",
       "Inbred_792     0.972310  \n",
       "Inbred_793     0.973189  \n",
       "Inbred_794     0.965372  \n",
       "Inbred_796     0.975658  \n",
       "Inbred_799     0.972310  \n",
       "Inbred_800     0.972649  \n",
       "Inbred_801     0.968476  \n",
       "Inbred_803     0.975052  \n",
       "Inbred_804     0.964424  \n",
       "Inbred_805     0.987929  \n",
       "Inbred_810     0.956669  \n",
       "Inbred_818     0.951296  \n",
       "Inbred_819     0.973737  \n",
       "Inbred_821     0.984514  \n",
       "Inbred_828     0.987451  \n",
       "Inbred_836     0.967962  \n",
       "\n",
       "[593 rows x 496 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>TESTER</th>\n",
       "      <th>Tester_1345</th>\n",
       "      <th>Tester_1349</th>\n",
       "      <th>Tester_1397</th>\n",
       "      <th>Tester_2636</th>\n",
       "      <th>Tester_2652</th>\n",
       "      <th>Tester_2683</th>\n",
       "      <th>Tester_2689</th>\n",
       "      <th>Tester_2690</th>\n",
       "      <th>Tester_2721</th>\n",
       "      <th>Tester_2724</th>\n",
       "      <th>...</th>\n",
       "      <th>Tester_821</th>\n",
       "      <th>Tester_8218</th>\n",
       "      <th>Tester_8246</th>\n",
       "      <th>Tester_8248</th>\n",
       "      <th>Tester_8249</th>\n",
       "      <th>Tester_8250</th>\n",
       "      <th>Tester_8253</th>\n",
       "      <th>Tester_8254</th>\n",
       "      <th>Tester_828</th>\n",
       "      <th>Tester_829</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INBRED</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Inbred_1071</th>\n",
       "      <td>0.960248</td>\n",
       "      <td>1.072859</td>\n",
       "      <td>0.853497</td>\n",
       "      <td>0.954354</td>\n",
       "      <td>0.981084</td>\n",
       "      <td>0.951334</td>\n",
       "      <td>0.905989</td>\n",
       "      <td>1.002726</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.906590</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069394</td>\n",
       "      <td>1.130785</td>\n",
       "      <td>0.876549</td>\n",
       "      <td>1.040942</td>\n",
       "      <td>1.131783</td>\n",
       "      <td>1.305137</td>\n",
       "      <td>0.982661</td>\n",
       "      <td>0.846477</td>\n",
       "      <td>1.020654</td>\n",
       "      <td>1.040759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_122</th>\n",
       "      <td>0.991007</td>\n",
       "      <td>1.277923</td>\n",
       "      <td>0.968752</td>\n",
       "      <td>1.172546</td>\n",
       "      <td>1.148055</td>\n",
       "      <td>1.013730</td>\n",
       "      <td>0.858975</td>\n",
       "      <td>0.943870</td>\n",
       "      <td>1.202416</td>\n",
       "      <td>0.513578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.181079</td>\n",
       "      <td>1.077675</td>\n",
       "      <td>0.867327</td>\n",
       "      <td>1.184029</td>\n",
       "      <td>0.888835</td>\n",
       "      <td>1.253450</td>\n",
       "      <td>1.181471</td>\n",
       "      <td>0.570893</td>\n",
       "      <td>1.093515</td>\n",
       "      <td>0.967627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1337</th>\n",
       "      <td>1.116238</td>\n",
       "      <td>1.076004</td>\n",
       "      <td>1.070392</td>\n",
       "      <td>0.816487</td>\n",
       "      <td>1.198018</td>\n",
       "      <td>0.500668</td>\n",
       "      <td>0.986212</td>\n",
       "      <td>1.163561</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>1.150713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131569</td>\n",
       "      <td>0.763190</td>\n",
       "      <td>0.833753</td>\n",
       "      <td>0.964613</td>\n",
       "      <td>1.133970</td>\n",
       "      <td>1.275041</td>\n",
       "      <td>0.823543</td>\n",
       "      <td>1.115956</td>\n",
       "      <td>0.934752</td>\n",
       "      <td>0.778390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1339</th>\n",
       "      <td>0.998943</td>\n",
       "      <td>1.083697</td>\n",
       "      <td>1.059308</td>\n",
       "      <td>0.789831</td>\n",
       "      <td>1.239354</td>\n",
       "      <td>0.210427</td>\n",
       "      <td>0.764500</td>\n",
       "      <td>0.957091</td>\n",
       "      <td>0.834656</td>\n",
       "      <td>0.633761</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064582</td>\n",
       "      <td>0.383970</td>\n",
       "      <td>0.608767</td>\n",
       "      <td>0.882642</td>\n",
       "      <td>0.687332</td>\n",
       "      <td>1.011313</td>\n",
       "      <td>0.770064</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.778381</td>\n",
       "      <td>0.414438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inbred_1340</th>\n",
       "      <td>0.994563</td>\n",
       "      <td>1.034544</td>\n",
       "      <td>0.676873</td>\n",
       "      <td>1.079141</td>\n",
       "      <td>0.723833</td>\n",
       "      <td>1.726039</td>\n",
       "      <td>1.160108</td>\n",
       "      <td>1.170362</td>\n",
       "      <td>0.454562</td>\n",
       "      <td>1.486461</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099733</td>\n",
       "      <td>1.977327</td>\n",
       "      <td>1.231918</td>\n",
       "      <td>1.202647</td>\n",
       "      <td>1.792767</td>\n",
       "      <td>1.710140</td>\n",
       "      <td>1.165308</td>\n",
       "      <td>1.210375</td>\n",
       "      <td>1.304240</td>\n",
       "      <td>1.781526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  496 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "TESTER       Tester_1345  Tester_1349  Tester_1397  Tester_2636  Tester_2652  \\\n",
       "INBRED                                                                         \n",
       "Inbred_1071     0.960248     1.072859     0.853497     0.954354     0.981084   \n",
       "Inbred_122      0.991007     1.277923     0.968752     1.172546     1.148055   \n",
       "Inbred_1337     1.116238     1.076004     1.070392     0.816487     1.198018   \n",
       "Inbred_1339     0.998943     1.083697     1.059308     0.789831     1.239354   \n",
       "Inbred_1340     0.994563     1.034544     0.676873     1.079141     0.723833   \n",
       "\n",
       "TESTER       Tester_2683  Tester_2689  Tester_2690  Tester_2721  Tester_2724  \\\n",
       "INBRED                                                                         \n",
       "Inbred_1071     0.951334     0.905989     1.002726     0.732800     0.906590   \n",
       "Inbred_122      1.013730     0.858975     0.943870     1.202416     0.513578   \n",
       "Inbred_1337     0.500668     0.986212     1.163561     0.603596     1.150713   \n",
       "Inbred_1339     0.210427     0.764500     0.957091     0.834656     0.633761   \n",
       "Inbred_1340     1.726039     1.160108     1.170362     0.454562     1.486461   \n",
       "\n",
       "TESTER          ...      Tester_821  Tester_8218  Tester_8246  Tester_8248  \\\n",
       "INBRED          ...                                                          \n",
       "Inbred_1071     ...        1.069394     1.130785     0.876549     1.040942   \n",
       "Inbred_122      ...        1.181079     1.077675     0.867327     1.184029   \n",
       "Inbred_1337     ...        1.131569     0.763190     0.833753     0.964613   \n",
       "Inbred_1339     ...        1.064582     0.383970     0.608767     0.882642   \n",
       "Inbred_1340     ...        1.099733     1.977327     1.231918     1.202647   \n",
       "\n",
       "TESTER       Tester_8249  Tester_8250  Tester_8253  Tester_8254  Tester_828  \\\n",
       "INBRED                                                                        \n",
       "Inbred_1071     1.131783     1.305137     0.982661     0.846477    1.020654   \n",
       "Inbred_122      0.888835     1.253450     1.181471     0.570893    1.093515   \n",
       "Inbred_1337     1.133970     1.275041     0.823543     1.115956    0.934752   \n",
       "Inbred_1339     0.687332     1.011313     0.770064     0.726572    0.778381   \n",
       "Inbred_1340     1.792767     1.710140     1.165308     1.210375    1.304240   \n",
       "\n",
       "TESTER       Tester_829  \n",
       "INBRED                   \n",
       "Inbred_1071    1.040759  \n",
       "Inbred_122     0.967627  \n",
       "Inbred_1337    0.778390  \n",
       "Inbred_1339    0.414438  \n",
       "Inbred_1340    1.781526  \n",
       "\n",
       "[5 rows x 496 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.apply(correction).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Inbred_1071', 'Inbred_122', 'Inbred_1337', 'Inbred_1339',\n",
       "       'Inbred_1340', 'Inbred_1341', 'Inbred_1342', 'Inbred_1344',\n",
       "       'Inbred_1345', 'Inbred_1346',\n",
       "       ...\n",
       "       'Inbred_801', 'Inbred_803', 'Inbred_804', 'Inbred_805', 'Inbred_810',\n",
       "       'Inbred_818', 'Inbred_819', 'Inbred_821', 'Inbred_828', 'Inbred_836'],\n",
       "      dtype='object', name='INBRED', length=593)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwWYwdPjbmKi",
    "outputId": "b95e979a-f0ef-4f36-d863-a49df07b6bb8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmax = 30\n",
    "ks  = range(2, kmax+1, 2)\n",
    "alphas = [10**i for i in range(-5,0)]\n",
    "betas = [10**i for i in range(-5,0)]\n",
    "lambdas = [10**i for i in range(-5,0)]\n",
    "best_test_error = {}\n",
    "for k in ks:\n",
    "    print(f'running k = {k}')\n",
    "    if k > 5:\n",
    "        for a in alphas:\n",
    "            for beta in betas:\n",
    "                for lambda_bias in lambdas:\n",
    "                    \n",
    "                    mf = MF(train_df.values, K=k, alpha=a, beta=beta, iterations=300, lambda_bias=lambda_bias, test=test_df.values)\n",
    "                    training_process, test_process = mf.train()\n",
    "                    best_test_error[(k, a, beta, lambda_bias)] = min(test_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM0cM0u_bmKu"
   },
   "outputs": [],
   "source": [
    "mf = MF(train_.values, 26, 0.001, 0.001, 150, 1e-05, test=test_.values)\n",
    "training_process, test_process = mf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.all((train_df.values * test_df.values) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKrlWcX_bmKy",
    "outputId": "09bcf481-05eb-4830-8a57-a8bb3518927a"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(test_process)\n",
    "axs[0].set_title('Test error')\n",
    "axs[1].plot(list(map(lambda x: x[1], training_process)))\n",
    "axs[1].set_title('Training  error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "818rO1YKbmK1",
    "outputId": "0f8d5fad-5075-46db-bf8d-f499165b40e8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-85dd1d13e4bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_' is not defined"
     ]
    }
   ],
   "source": [
    "def abline():\n",
    "    gca = plt.gca()\n",
    "    gca.set_autoscale_on(False)\n",
    "    gca.plot(gca.get_xlim(),gca.get_ylim(), 'red')\n",
    "    \n",
    "test = test_.values\n",
    "x = test[test.nonzero()].flatten()\n",
    "y = mf.full_matrix()[test.nonzero()].flatten()\n",
    "plt.scatter(x, y, alpha = 0.5)\n",
    "plt.xlabel(\"actual test\")\n",
    "plt.ylabel(\"predicted test\")\n",
    "abline()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_ddx86ubmK3"
   },
   "outputs": [],
   "source": [
    "to_write = open('split1.txt', 'a')\n",
    "to_write.write(\"a\\\\n\")\n",
    "to_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMfNPCcJbmK4"
   },
   "outputs": [],
   "source": [
    "# mf = MF(train_df.values, K=10, alpha=0.01, beta=0.1, iterations=300, lambda_bias=0.15, test=test_df.values)\n",
    "# training_process, test_process = mf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hehFoojxbmK7",
    "outputId": "836ae917-1fba-45de-f93c-c4cd01297b3a"
   },
   "outputs": [],
   "source": [
    "plt.hist(mf.full_matrix().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFs9kj1cbmK8",
    "outputId": "d9866935-94c4-4680-d87b-45da67e7181d"
   },
   "outputs": [],
   "source": [
    "best_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1ncx3fRbmK-"
   },
   "outputs": [],
   "source": [
    "ordered_results = sorted(best_test_error.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8vMPhd2bmLA",
    "outputId": "0d9234c9-227c-45c1-9ef5-da67259207a2"
   },
   "outputs": [],
   "source": [
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqYb3-M6bmLC",
    "outputId": "823baa4d-3d24-4a43-a1ca-d2ac3a868fd0"
   },
   "outputs": [],
   "source": [
    "best_test_error.itertuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MphTv4w6bmLH",
    "outputId": "bb28465e-66d8-4b62-a356-7ae4bc796b6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(best_test_error.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmJcB6JXbmLI"
   },
   "outputs": [],
   "source": [
    "# elements = [val for i, val in enumerate(ordered_results) if i < 20]\n",
    "count = 0\n",
    "with open('results.txt', 'w') as f:\n",
    "    while count < 20:\n",
    "        val = ordered_results[count]\n",
    "        f.write(str(val)[1:-1] + '\\n')\n",
    "        count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsrdF0ExcXbU"
   },
   "outputs": [],
   "source": [
    "k, a, beta, lambda_bias = 26, 0.001, 0.001, 1e-05\n",
    "mf = MF(train_df.values, K=k, alpha=a, beta=beta, iterations=300, lambda_bias=lambda_bias, test=test_df.values)\n",
    "training_process, test_process = mf.train()\n",
    "plt.plot(test_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "4Mi2cGmId1Qt",
    "outputId": "7b006eb6-5262-4b27-d953-b6fec1feca03"
   },
   "outputs": [],
   "source": [
    "plt.plot(test_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0pFoobYPd5os",
    "outputId": "bfec9eed-e398-46d0-9f18-73f6258178bb"
   },
   "outputs": [],
   "source": [
    "print(min(test_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2XRR06LbmLL"
   },
   "outputs": [],
   "source": [
    "# another MF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR_5FK70dP6Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFtQ1x6O2Gf7"
   },
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.1, \n",
    "                 user_fact_reg=0.1,\n",
    "                 item_bias_reg=0.1,\n",
    "                 user_bias_reg=0.1,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.01):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        self.user_vecs_velocity = np.zeros((self.n_users, self.n_factors))\n",
    "        self.item_vecs_velocity = np.zeros((self.n_items, self.n_factors))\n",
    "\n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.momentum = 0.9\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.user_bias_velocity = np.zeros(self.n_users)\n",
    "            self.item_bias_velocity = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias_velocity[u] = self.momentum * self.user_bias_velocity[u] + \\\n",
    "                                        self.learning_rate * (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.user_bias[u] += self.user_bias_velocity[u]\n",
    "\n",
    "            self.item_bias_velocity[i] = self.momentum * self.item_bias_velocity[i] + \\\n",
    "                                        self.learning_rate * (e - self.item_bias_reg * self.item_bias[i])\n",
    "            self.item_bias[i] += self.item_bias_velocity[i]\n",
    "\n",
    "            # self.user_bias[u] += self.learning_rate * \\\n",
    "            #                     (e - self.user_bias_reg * self.user_bias[u])\n",
    "            # self.item_bias[i] += self.learning_rate * \\\n",
    "            #                     (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs_velocity[u, :] = self.momentum * self.user_vecs_velocity[u, :] + \\\n",
    "                                        self.learning_rate * \\\n",
    "                                        (e * self.item_vecs[i, :] - \\\n",
    "                                        self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.user_vecs[u, :] += self.user_vecs_velocity[u, :]\n",
    "\n",
    "            self.item_vecs_velocity[i, :] = self.momentum * self.item_vecs_velocity[i, :] + \\\n",
    "                                        self.learning_rate * \\\n",
    "                                        (e * self.user_vecs[u, :] - \\\n",
    "                                        self.item_fact_reg * self.item_vecs[i,:])\n",
    "            self.item_vecs[i :] += self.item_vecs_velocity[i, :]\n",
    "\n",
    "            # self.user_vecs[u, :] += self.learning_rate * \\\n",
    "            #                         (e * self.item_vecs[i, :] - \\\n",
    "            #                          self.user_fact_reg * self.user_vecs[u,:])\n",
    "            # self.item_vecs[i, :] += self.learning_rate * \\\n",
    "            #                         (e * self.user_vecs[u, :] - \\\n",
    "            #                          self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5ua3Jkaehe1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DFzGgi4dGcz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sc6kIHKtdJJm"
   },
   "outputs": [],
   "source": [
    "#MF_SGD = ExplicitMF(train, 40, learning='sgd', verbose=True)\n",
    "MF_SGD = ExplicitMF(train_df.values,\n",
    "                    n_factors=40,\n",
    "                    learning='sgd',\n",
    "                    \n",
    "                    item_fact_reg=0.1, \n",
    "                    user_fact_reg=0.1,\n",
    "                    item_bias_reg=0.1,\n",
    "                    user_bias_reg=0.1,\n",
    "                    verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200,300]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_df.values, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "LYDIYcRDdy3p",
    "outputId": "7102f6ae-d42a-4486-e904-81c80c24219e"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(iter_array, MF_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oXOSQ1PSeIJK",
    "outputId": "5d8cfcbc-8290-4d0d-9f55-7f902755187a"
   },
   "outputs": [],
   "source": [
    "iter_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E719bdBmeuFQ",
    "outputId": "3f7a08b4-a27c-42a6-bacd-fcd687cfa468"
   },
   "outputs": [],
   "source": [
    "get_mse(MF_SGD.predict_all(),test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YJlOKVqXe6GX",
    "outputId": "8950986e-170c-4961-e603-9a581108defa"
   },
   "outputs": [],
   "source": [
    "get_mse(mf.full_matrix(),test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSLwTla4fDeN"
   },
   "outputs": [],
   "source": [
    "# try to take log, train, then exponentiate back\n",
    "train_df_log = np.ma.log(train_df.values)\n",
    "train_df_log = train_df_log.filled(0)\n",
    "test_df_log = np.ma.log(test_df.values)\n",
    "test_df_log = test_df_log.filled(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "8G_cVAVeh9Qv",
    "outputId": "19fbe1de-e85c-4ad8-c0bf-11ff4aa0157a"
   },
   "outputs": [],
   "source": [
    "k, a, beta, lambda_bias = 26, 0.001, 0.001, 1e-05\n",
    "mf = MF(train_df_log, K=k, alpha=a, beta=beta, iterations=300, lambda_bias=lambda_bias, test=test_df_log)\n",
    "training_process, test_process = mf.train()\n",
    "plt.plot(test_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_yCf-izqh9E0",
    "outputId": "455735f2-5b55-4e0a-9e2b-0f93c8e99d65"
   },
   "outputs": [],
   "source": [
    "get_mse(np.exp(mf.full_matrix()),test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "oH6dSMUVgbow",
    "outputId": "d5016d1b-63cf-4582-f87a-d90d18a3ed91"
   },
   "outputs": [],
   "source": [
    "MF_SGD = ExplicitMF(train_df.values,\n",
    "                    n_factors=40,\n",
    "                    learning='sgd',\n",
    "                    item_fact_reg=0.1, \n",
    "                    user_fact_reg=0.1,\n",
    "                    item_bias_reg=0.1,\n",
    "                    user_bias_reg=0.1,\n",
    "                    verbose=False)\n",
    "iter_array = [1, 2, 5, 10, 25, 50]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_df.values, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "zMLXDeOvV_e-",
    "outputId": "ca1dde91-c814-4a8d-f2eb-72d7890943f2"
   },
   "outputs": [],
   "source": [
    "MF_SGD = ExplicitMF(train,\n",
    "                    n_factors=40,\n",
    "                    learning='sgd',\n",
    "                    item_fact_reg=0.1, \n",
    "                    user_fact_reg=0.1,\n",
    "                    item_bias_reg=0.1,\n",
    "                    user_bias_reg=0.1,\n",
    "                    verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ohNJaeuAWMrZ",
    "outputId": "b0247317-3e6b-4e5f-c25e-1f92620f8f39"
   },
   "outputs": [],
   "source": [
    "get_mse(np.exp(MF_SGD.predict_all()),np.exp(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "tM4-qwKdhgWC",
    "outputId": "dfdcdba6-d447-415b-9787-b45a0da8de3f"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(iter_array, MF_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "BEokm4zw7taf",
    "outputId": "d45fe805-c359-4729-d524-43c330d42183"
   },
   "outputs": [],
   "source": [
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cm_da4hY8Bcd"
   },
   "outputs": [],
   "source": [
    "X_incomplete = train_df.replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdUHcuTT-Znl"
   },
   "outputs": [],
   "source": [
    "X_incomplete_log = np.log(X_incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "kN5FX2Ib8YVd",
    "outputId": "8c4dedde-5a1d-4eb9-81a6-7993c9b414ae"
   },
   "outputs": [],
   "source": [
    "X_filled_nnm = NuclearNormMinimization().fit_transform(X_incomplete.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5AGclE_BNzj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_incomplete_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-139e4b5ac700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnull_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_incomplete_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_incomplete_log' is not defined"
     ]
    }
   ],
   "source": [
    "null_mat = X_incomplete_log.isnull().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BrOwsBfNCgXi",
    "outputId": "078cd9a2-4f7e-4645-e49a-6fc941b4cd1b"
   },
   "outputs": [],
   "source": [
    "null_mat[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z6tAQ-AQClOG",
    "outputId": "a007d505-515d-4b21-ff8e-81a88e79a033"
   },
   "outputs": [],
   "source": [
    "null_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhMB0R7dC19J"
   },
   "outputs": [],
   "source": [
    "X_incomplete = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "t0iddVhx_KnJ",
    "outputId": "878a47d6-1826-4902-fb46-214a05ed5f44"
   },
   "outputs": [],
   "source": [
    "for i in range(len(null_mat)):\n",
    "    #print(train_df_log[i,0])\n",
    "    if null_mat[i,:].sum() == 496:\n",
    "        print(\"zero row\")\n",
    "        idx = np.random.choice(495)\n",
    "        X_incomplete.iloc[i,idx] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnYT0epoAU7d"
   },
   "outputs": [],
   "source": [
    "X_incomplete.replace(0, np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "vyj2oWg3Dof_",
    "outputId": "19cf4c30-7f09-4d73-aa36-5d81db3da498"
   },
   "outputs": [],
   "source": [
    "X_incomplete_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukyZ6bZV-uVF"
   },
   "outputs": [],
   "source": [
    "si = SoftImpute(max_iters = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBNvkREi8ynp"
   },
   "outputs": [],
   "source": [
    "# X_incomplete_normalized = BiScaler().fit_transform(train_na)\n",
    "X_filled_softimpute = si.fit_transform(train_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2vl0iQWT9hN9",
    "outputId": "dc13f44b-e4e8-4452-9023-1ac08d887467"
   },
   "outputs": [],
   "source": [
    "get_mse(np.exp(X_filled_softimpute), np.exp(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "SB-xMg7vaYkX",
    "outputId": "5f3ed116-1bc6-409c-ed3c-82e052000e69"
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.exp(test[test.nonzero()]).flatten(), np.exp(X_filled_softimpute[test.nonzero()].flatten()), alpha = 0.5)\n",
    "#plt.plot(np.exp(test[test.nonzero()]).flatten(), alpha = 0.5)\n",
    "plt.xlabel(\"test\")\n",
    "plt.ylabel(\"pred test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iU82IrHVaGCc",
    "outputId": "f0ea9603-9dd8-4342-de07-88a16475cd4f"
   },
   "outputs": [],
   "source": [
    "np.sqrt(0.00017838840904557725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_1xVgy0XJvP2",
    "outputId": "de96d182-3094-4661-b0d5-fc9a9463149a"
   },
   "outputs": [],
   "source": [
    "np.exp(train).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UttwhX-9FrD9",
    "outputId": "df393c27-4112-4ad7-903e-fbabce21668f"
   },
   "outputs": [],
   "source": [
    "np.exp(X_filled_softimpute).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "32DSrBid-Owg",
    "outputId": "f8a9c134-c70a-4853-cde6-369c38a6c8d2"
   },
   "outputs": [],
   "source": [
    "np.exp(X_filled_softimpute).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcdFmdKoGlgz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aVxSKP3B6W61",
    "outputId": "d5340421-021a-408b-b2f8-b70c859331d9"
   },
   "outputs": [],
   "source": [
    "get_mse(MF_SGD.predict_all(),test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QQ3SqmcWh3tA",
    "outputId": "57c756bd-193f-4fee-957f-8586d33b0ec2"
   },
   "outputs": [],
   "source": [
    "get_mse(np.exp(MF_SGD.predict_all()),test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyFJtiTNiNhl"
   },
   "outputs": [],
   "source": [
    "res = np.exp(MF_SGD.predict_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AbswyQGhiZo-",
    "outputId": "fade015e-7ca6-467d-a202-67a17de1e571"
   },
   "outputs": [],
   "source": [
    "res.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wKsYKgBvivEV",
    "outputId": "5f249f6c-79aa-4777-b4c4-dbbd8d9c6e83"
   },
   "outputs": [],
   "source": [
    "res.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GKzuq53Diwyq",
    "outputId": "88691997-1a54-437c-f083-c55d7bd6e8cc"
   },
   "outputs": [],
   "source": [
    "train_df.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9eoGo4e8i0K5",
    "outputId": "e4e6384e-593f-4f78-fb57-2ee9fd75beb7"
   },
   "outputs": [],
   "source": [
    "res.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "E2t7XYqInpNS",
    "outputId": "5e5b4790-acd9-4791-9c08-2c2088a02268"
   },
   "outputs": [],
   "source": [
    "np.exp(MF_SGD.predict_all())\n",
    "plt.hist(np.exp(MF_SGD.predict_all())[test.nonzero()].flatten(), alpha = 0.5, label = \"test prediction\",bins = 20)\n",
    "plt.title(\"Histogram of Test Prediction vs Test Data\")\n",
    "plt.hist(np.exp(test)[test.nonzero()].flatten(), alpha = 0.5, label = \"test data\",bins = 20)\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "m3nGrW7Vn50m",
    "outputId": "e98fb5ef-33d0-4e28-b256-a64837b640c8"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(MF_SGD.predict_all())[train.nonzero()].flatten(), alpha = 0.5, label = \"train prediction\")\n",
    "plt.title(\"Histogram of Train Prediction vs Train Data\")\n",
    "plt.hist(np.exp(train)[train.nonzero()].flatten(), alpha = 0.5, label = \"train data\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "pZN-6Uh2Gnwj",
    "outputId": "f3f73dfa-7171-4066-bdec-968e3b67ff1a"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(X_filled_softimpute)[test.nonzero()].flatten(), alpha = 0.5, label = \"test prediction\")\n",
    "plt.title(\"Histogram of Test Prediction vs Test Data\")\n",
    "plt.hist(np.exp(test)[test.nonzero()].flatten(), alpha = 0.5, label = \"test data\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "6WlxfeItZbZI",
    "outputId": "59bbfad0-715d-4db1-d618-04c92322b796"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(X_filled_softimpute)[train.nonzero()].flatten(), alpha = 0.5, label = \"train prediction\")\n",
    "plt.title(\"Histogram of Train Prediction vs Train Data\")\n",
    "plt.hist(np.exp(train)[train.nonzero()].flatten(), alpha = 0.5, label = \"train data\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZvF4jHweY-ab",
    "outputId": "0f3176fd-9b88-4044-9a11-17e8f27c6228"
   },
   "outputs": [],
   "source": [
    "np.exp(X_filled_softimpute)[test.nonzero()].flatten().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hvH4hi-nZBEd",
    "outputId": "2c179ada-abc9-4228-e03b-c073c4339c8e"
   },
   "outputs": [],
   "source": [
    "np.exp(test)[test.nonzero()].flatten().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "lV1ce0DFYKnr",
    "outputId": "6fae3fda-8d3b-404d-a730-2d8f8f594017"
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qd6PfA0POiHr",
    "outputId": "449c76f9-08f0-4da3-9107-6133262e477f"
   },
   "outputs": [],
   "source": [
    "np.all(train*test) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "HxOit7lpGwaH",
    "outputId": "d9623e17-0b4c-4f8e-adaa-487e9adc8b99"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(X_filled_softimpute)[test.nonzero()].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "A08n0V8aG1WF",
    "outputId": "321be8d7-ae81-4ccf-b3f0-8e542237cb13"
   },
   "outputs": [],
   "source": [
    "plt.hist(test_df.values[test_df.values.nonzero()].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "TeOj1S3Fi7WX",
    "outputId": "df7c31d4-f0ad-4f55-e825-8991511d9965"
   },
   "outputs": [],
   "source": [
    "plt.hist(res[train_df.values.nonzero()].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "4QXcboBIi9w5",
    "outputId": "3936dd19-7abe-443e-f1ac-35aaab4fa601"
   },
   "outputs": [],
   "source": [
    "plt.hist(train_df.values[train_df.values.nonzero()].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RqfFkQ-gjIl4",
    "outputId": "c1824de3-ec0e-48dc-c136-fbc1b99bac9c"
   },
   "outputs": [],
   "source": [
    "train_df.values.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "gIeZ0UaCjUZQ",
    "outputId": "30df9729-18d9-4419-b76a-280571eb6987"
   },
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200,250]\n",
    "learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "best_params = {}\n",
    "best_params['learning_rate'] = None\n",
    "best_params['n_iter'] = 0\n",
    "best_params['train_mse'] = np.inf\n",
    "best_params['test_mse'] = np.inf\n",
    "best_params['model'] = None\n",
    "\n",
    "\n",
    "for rate in learning_rates:\n",
    "    print ('Rate: {}'.format(rate))\n",
    "    MF_SGD = ExplicitMF(train_df_log, n_factors=35, learning='sgd')\n",
    "    MF_SGD.calculate_learning_curve(iter_array, test_df_log, learning_rate=rate)\n",
    "    min_idx = np.argmin(MF_SGD.test_mse)\n",
    "    if MF_SGD.test_mse[min_idx] < best_params['test_mse']:\n",
    "        best_params['n_iter'] = iter_array[min_idx]\n",
    "        best_params['learning_rate'] = rate\n",
    "        best_params['train_mse'] = MF_SGD.train_mse[min_idx]\n",
    "        best_params['test_mse'] = MF_SGD.test_mse[min_idx]\n",
    "        best_params['model'] = MF_SGD\n",
    "        print ('New optimal hyperparameters')\n",
    "        print (pd.Series(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "myenpXyYkM61",
    "outputId": "87253f7e-3f63-44e8-8310-3fb9475bb84f"
   },
   "outputs": [],
   "source": [
    "best_params #for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zPioO-5mmdpO",
    "outputId": "26982db2-c8d2-4c5f-dfc1-79cd85bf4dc1"
   },
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200, 250]\n",
    "latent_factors = [ 5, 10, 20, 25, 30, 35, 40]\n",
    "regularizations = [0.001, 0.01, 0.1, 1.]\n",
    "regularizations.sort()\n",
    "\n",
    "best_params = {}\n",
    "best_params['n_factors'] = latent_factors[0]\n",
    "best_params['reg'] = regularizations[0]\n",
    "best_params['n_iter'] = 0\n",
    "best_params['train_mse'] = np.inf\n",
    "best_params['test_mse'] = np.inf\n",
    "best_params['model'] = None\n",
    "\n",
    "for fact in latent_factors:\n",
    "    print ('Factors: {}'.format(fact))\n",
    "    for reg in regularizations:\n",
    "        print ('Regularization: {}'.format(reg))\n",
    "        MF_SGD = ExplicitMF(train_df_log, n_factors=fact, learning='sgd',\\\n",
    "                            user_fact_reg=reg, item_fact_reg=reg, \\\n",
    "                            user_bias_reg=reg, item_bias_reg=reg)\n",
    "        MF_SGD.calculate_learning_curve(iter_array, test_df_log, learning_rate=0.001)\n",
    "        min_idx = np.argmin(MF_SGD.test_mse)\n",
    "        if MF_SGD.test_mse[min_idx] < best_params['test_mse']:\n",
    "            best_params['n_factors'] = fact\n",
    "            best_params['reg'] = reg\n",
    "            best_params['n_iter'] = iter_array[min_idx]\n",
    "            best_params['train_mse'] = MF_SGD.train_mse[min_idx]\n",
    "            best_params['test_mse'] = MF_SGD.test_mse[min_idx]\n",
    "            best_params['model'] = MF_SGD\n",
    "            print ('New optimal hyperparameters')\n",
    "            print (pd.Series(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "rk1uAd6GyJmd",
    "outputId": "9ca10b38-3638-4f26-e2bb-b67a426109e5"
   },
   "outputs": [],
   "source": [
    "best_params #for factors and reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvBNz4wIMJET"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings, \n",
    "                 n_factors=40, \n",
    "                 item_reg=0.0, \n",
    "                 user_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        \n",
    "        item_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_reg = item_reg\n",
    "        self.user_reg = user_reg\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.random((self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.random((self.n_items, self.n_factors))\n",
    "        \n",
    "        self.partial_train(n_iter)\n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                           self.item_vecs, \n",
    "                                           self.ratings, \n",
    "                                           self.user_reg, \n",
    "                                           type='user')\n",
    "            self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                           self.user_vecs, \n",
    "                                           self.ratings, \n",
    "                                           self.item_reg, \n",
    "                                           type='item')\n",
    "            ctr += 1\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item. \"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction. \"\"\"\n",
    "        return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpzDfHY8l2YJ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0fc2498176ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m MF_ALS = ExplicitMF(train_df_log, n_factors=30, \\\n\u001b[0m\u001b[0;32m      2\u001b[0m                     user_reg=0.1, item_reg=0.1)\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0miter_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMF_ALS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df_log' is not defined"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_df_log, n_factors=30, \\\n",
    "                    user_reg=0.1, item_reg=0.1)\n",
    "\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXW3u7y0W2ow"
   },
   "outputs": [],
   "source": [
    "MF_ALS = ExplicitMF(train, n_factors=30, \\\n",
    "                    user_reg=0.1, item_reg=0.1)\n",
    "\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "I_41PIOgl_SD",
    "outputId": "d2a4de2c-380a-4945-ce4f-e44182f52d11"
   },
   "outputs": [],
   "source": [
    "#mse of ALS method \n",
    "als_pred = MF_ALS.predict_all()\n",
    "als_pred = np.exp(als_pred)\n",
    "print(als_pred.min())\n",
    "print(als_pred.max())\n",
    "print(\"mse of ALS method: \", get_mse(als_pred, test)) # it is better than SGD method :o \n",
    "print((np.exp(train)).max(), (np.exp(train)).min())\n",
    "print((np.exp(test)).max(), (np.exp(test)).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "Y2kJUaZ6mzGr",
    "outputId": "199cb7ee-ecee-46fe-9bfc-3b735d3d184d"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(iter_array, MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-F3wLAvnfwO"
   },
   "outputs": [],
   "source": [
    "# my code to process the training data to have at least 1 entry in each row or column\n",
    "train_df = pd.read_csv(\"../data/input/CC2020_train_final.csv\")\n",
    "# test_df = pd.read_csv(\"CC2020_test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "CRN7Df6XKjFi",
    "outputId": "895b8f65-88f3-42ae-ffda-1a0b9c244342"
   },
   "outputs": [],
   "source": [
    "a = np.array([0,0,0])\n",
    "a.nonzero()[0]\n",
    "np.random.choice(a.nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "7bUAayrsHJZN",
    "outputId": "cb58ad63-d8d6-4af9-bf3f-4414bb439580"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['INBRED', 'TESTER', 'YIELD']]\n",
    "grouped_df = train_df.groupby(['INBRED','TESTER'], as_index=False).mean()\n",
    "grouped_df['YIELD'] = np.log(grouped_df['YIELD']).values\n",
    "train_df = grouped_df\n",
    "def train_test_split(mat):\n",
    "    test = np.zeros(mat.shape)\n",
    "    train = mat.copy()\n",
    "    for inbred in range(mat.shape[0]):\n",
    "        test_yields = np.random.choice(mat[inbred, :].nonzero()[0], \n",
    "                                        size=max(1, len(mat[inbred, :].nonzero()[0])//10), \n",
    "                                        replace=False)\n",
    "        train[inbred, test_yields] = 0.\n",
    "        test[inbred, test_yields] = mat[inbred, test_yields]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test\n",
    "inbred_index = {}\n",
    "tester_index = {}\n",
    "i = 0\n",
    "j = 0\n",
    "for idx, row in train_df.iterrows():\n",
    "    if row['INBRED'] not in inbred_index:\n",
    "        inbred_index[row['INBRED']] = i\n",
    "        i += 1\n",
    "    if row['TESTER'] not in tester_index:\n",
    "        tester_index[row['TESTER']] = j\n",
    "        j += 1\n",
    "print(len(inbred_index))\n",
    "print(len(tester_index))\n",
    "print(i)\n",
    "print(j)\n",
    "n_inbred = 593\n",
    "n_tester = 496\n",
    "mat = np.zeros((n_inbred, n_tester))\n",
    "for row in train_df.itertuples():\n",
    "    mat[inbred_index[row[1]], tester_index[row[2]]] = row[3]\n",
    "mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xd7RWAzpMZqu"
   },
   "outputs": [],
   "source": [
    "def train_test_split(mat):\n",
    "    test = np.zeros(mat.shape)\n",
    "    train = mat.copy()\n",
    "    for inbred in range(mat.shape[0]):\n",
    "        if (mat[inbred, :].sum() == 0): print(\"row is zero\")\n",
    "        # print(mat[inbred, :].sum())\n",
    "        test_yields = np.random.choice(mat[inbred, :].nonzero()[0], \n",
    "                                        size=max(0, len(mat[inbred, :].nonzero()[0])//10), \n",
    "                                        replace=False)\n",
    "        train[inbred, test_yields] = 0.\n",
    "        test[inbred, test_yields] = mat[inbred, test_yields]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uCV6ZqXHppH"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nxk38opVHuei"
   },
   "outputs": [],
   "source": [
    "mat_na = np.zeros(mat.shape)\n",
    "for i in range(len(mat)):\n",
    "    for j in range(len(mat[0])):\n",
    "        if mat[i,j] == 0:\n",
    "            mat_na[i,j] = np.nan\n",
    "        else:\n",
    "            mat_na[i,j] = mat[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "_NuGb9N_H7h-",
    "outputId": "2c19410f-9ec0-4345-f85d-827e9c4fc2b7"
   },
   "outputs": [],
   "source": [
    "mat_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-JYZ6h6I5v2"
   },
   "outputs": [],
   "source": [
    "train_na = np.zeros(train.shape)\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[0])):\n",
    "        if train[i,j] == 0:\n",
    "            train_na[i,j] = np.nan\n",
    "        else:\n",
    "            train_na[i,j] = train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UH9eD5qTJP5F",
    "outputId": "ca31fb58-e59b-48a0-d4ae-5c2dd79bbfb3"
   },
   "outputs": [],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "KTvuJZ5_LDpm",
    "outputId": "ab16a26e-980e-4f76-94ae-ed004fb450fa"
   },
   "outputs": [],
   "source": [
    "np.isnan(train_na[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wCh6pVnNTjL"
   },
   "outputs": [],
   "source": [
    "for i in range(train.shape[0]):\n",
    "    if train[i,:].sum() == 0:\n",
    "        print(\"row is zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJblBq-ATJCp"
   },
   "outputs": [],
   "source": [
    "for i in range(mat.shape[0]):\n",
    "    if mat[i,:].sum() == 0:\n",
    "        print(\"row is zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOsXtBzATbPv"
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/kastnerkyle/9341182\n",
    "\n",
    "# (C) Kyle Kastner, June 2014\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def minibatch_indices(X, minibatch_size):\n",
    "    X = list(X)\n",
    "    minibatch_indices = np.arange(0, len(X), minibatch_size)\n",
    "    minibatch_indices = np.asarray(list(minibatch_indices) + [len(X)])\n",
    "    start_indices = minibatch_indices[:-1]\n",
    "    end_indices = minibatch_indices[1:]\n",
    "    return zip(start_indices, end_indices)\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    \"\"\"\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def PMF(X, rank=10, learning_rate=0.001, momentum=0.8,\n",
    "        regularization=0.25, minibatch_size=1000, max_epoch=1000,\n",
    "        nan_value=0, status_percentage=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Python implementation of Probabilistic Matrix Factorization (PMF).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array or scipy.sparse coo matrix, shape (n_users, n_items)\n",
    "        Input data. If a dense array is passed in, it will be converted to a\n",
    "        sparse matrix by looking for all `nan_value` numbers and treating them\n",
    "        as empty.\n",
    "    rank: int, optional (default=10)\n",
    "       Rank of the low-rank factor matrices. A higher rank should result in a\n",
    "       better approximation, at the cost of more memory and slower computataion.\n",
    "    learning_rate: float, optional (default=0.001)\n",
    "        Learning rate for minibatch gradient descent.\n",
    "    momentum: float, optional (default=0.8)\n",
    "        Momentum for minibatch gradient descent.\n",
    "    regularization: float, optional (default=0.25)\n",
    "        L2 regularization penalty for minibatch gradient descent.\n",
    "    minibatch_size: int, optional (default=1000)\n",
    "       The size of each minibatch. If this is larger than size of the dataset,\n",
    "       will default to running over the whole dataset.\n",
    "    max_epoch: int, optional (default=1000)\n",
    "        The maximum number of epochs.\n",
    "    nan_value: int, optional (default=0)\n",
    "        This value will be masked out of the input for calculations\n",
    "        Should match the value considered the \"not rated\" in the dataset X.\n",
    "    status_percentage: float in (0, 1), optional (default=0.1)\n",
    "        The relative percentage of `max_epochs` when status will be printed.\n",
    "        For example, 0.1 is every 10%, 0.01 is every 1%, and so on. For\n",
    "        the default values of max_epoch=1000, status_percentage=0.1 this\n",
    "        is equivalent to a status print every 100 epochs.\n",
    "    random_state: RandomState, int, or None, optional (default=None)\n",
    "        Random state to pass in. Can be an int, None, or np.random.RandomState\n",
    "        object.\n",
    "    Returns\n",
    "    -------\n",
    "    U: array-like, shape (X.shape[0], rank)\n",
    "        Row basis for reconstruction.\n",
    "        Usage:\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    V: array-like, shape (X.shape[1], rank)\n",
    "        Column basis for reconstruction.\n",
    "        Usage:\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    X_mean: float\n",
    "        Global mean prediction, needed for reconstruction\n",
    "        Usage\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    Notes\n",
    "    -----\n",
    "    Based on code from Ruslan Salakhutdinov\n",
    "    http://www.cs.toronto.edu/~rsalakhu/code_BPMF/pmf.m\n",
    "    Probabilistic Matrix Factorization, R. Salakhutdinov and A. Mnih,\n",
    "    Advances in Neural Information Processing Systems 20, 2008\n",
    "    \"\"\"\n",
    "    if not sparse.isspmatrix_coo(X):\n",
    "        val_index = np.where(X != nan_value)\n",
    "        X = sparse.coo_matrix((X[val_index[0], val_index[1]],\n",
    "                               (val_index[0], val_index[1])))\n",
    "    # Simplest prediction is the global mean\n",
    "    X_mean = X.mean()\n",
    "    lr = learning_rate\n",
    "    reg = regularization\n",
    "    mom = momentum\n",
    "    if random_state is None or type(random_state) is int:\n",
    "        random_state = np.random.RandomState(random_state)\n",
    "    N, M = X.shape\n",
    "    U = 0.1 * random_state.randn(N, rank)\n",
    "    V = 0.1 * random_state.randn(M, rank)\n",
    "    U_inc = np.zeros_like(U)\n",
    "    V_inc = np.zeros_like(V)\n",
    "    dU = np.zeros_like(U)\n",
    "    dV = np.zeros_like(V)\n",
    "    epoch = 0\n",
    "    status_inc = int(np.ceil(max_epoch * status_percentage))\n",
    "    print(\"Printing updates every %i epochs\" % status_inc)\n",
    "    status_points = list(range(0, max_epoch, status_inc)) + [max_epoch - 1]\n",
    "    # Need this in order to index\n",
    "    X_s = X.tolil()\n",
    "    while epoch < max_epoch:\n",
    "        # Get indices for non-NaN values\n",
    "        r, c = X.nonzero()\n",
    "        mb_indices = minibatch_indices(zip(r, c), minibatch_size)\n",
    "        mb_indices = list(mb_indices)\n",
    "        n_batches = len(mb_indices)\n",
    "        shuffle_in_unison(r, c)\n",
    "        mean_abs_err = 0.\n",
    "        for i, j in mb_indices:\n",
    "            # Reset derivative matrices each minibatch\n",
    "            dU[:, :] = 0.\n",
    "            dV[:, :] = 0.\n",
    "            # Slice out row and column indices\n",
    "            r_i = r[i:j]\n",
    "            c_i = c[i:j]\n",
    "            # Get data corresponding to the row and column indices\n",
    "            X_i = X_s[r_i, c_i].toarray().ravel() - X_mean\n",
    "            # Compute predictions\n",
    "            pred = np.sum(U[r_i] * V[c_i], axis=1)\n",
    "            # Compute how algorithm is doing\n",
    "            mean_abs_err += np.sum(np.abs(pred - X_i)) / (n_batches * (j - i))\n",
    "            # Loss has a tendency to be unstable, but is the \"right thing\"\n",
    "            # to monitor instead of sum_abs_err\n",
    "            # pred_loss = (pred - X_i) ** 2\n",
    "            # Compute gradients\n",
    "            grad_loss = 2 * (pred - X_i)\n",
    "            grad_U = grad_loss[:, None] * V[c_i] + reg * U[r_i]\n",
    "            grad_V = grad_loss[:, None] * U[r_i] + reg * V[c_i]\n",
    "            dU[r_i] = grad_U\n",
    "            dV[c_i] = grad_V\n",
    "            # Momentum storage\n",
    "            U_inc = mom * U_inc + lr * dU\n",
    "            V_inc = mom * V_inc + lr * dV\n",
    "            U -= U_inc\n",
    "            V -= V_inc\n",
    "        if epoch in status_points:\n",
    "            print(\"Epoch %i of %i\" % (epoch + 1, max_epoch))\n",
    "            print(\"Mean absolute error %f\" % (mean_abs_err))\n",
    "        epoch += 1\n",
    "    return U, V, X_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "joVcbrawLwnU",
    "outputId": "73882ce4-1f97-4609-85e5-aab7e75686b7"
   },
   "outputs": [],
   "source": [
    "R = np.array([[5, 3, 0, 1],\n",
    "                [4, 0, 0, 1],\n",
    "                [1, 1, 0, 5],\n",
    "                [1, 0, 0, 4],\n",
    "                [0, 1, 5, 4]], dtype=float)\n",
    "U, V, m = PMF(R, learning_rate=0.001, momentum=0.95,\\\n",
    "                minibatch_size=2, rank=5, max_epoch=250, random_state=1999)\n",
    "R2 = np.dot(U, V.T) + m\n",
    "plt.matshow(R * (R > 0))\n",
    "plt.title(\"Ground truth ratings\")\n",
    "plt.matshow(R2 * (R > 0))\n",
    "plt.title(\"Predicted ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "BnLPOYYvMJ50",
    "outputId": "6e9240c0-0cae-4aa7-ff64-b5c9452688f2"
   },
   "outputs": [],
   "source": [
    "U, V, m = PMF(train, learning_rate=0.001, momentum=0.95,\\\n",
    "                minibatch_size=5, rank=30, max_epoch=250, random_state=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hu5J9e9nNPv6"
   },
   "outputs": [],
   "source": [
    "pred = np.dot(U, V.T) + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z3CaSBfJNYM6",
    "outputId": "94733f26-3ab6-4f8e-afa8-29165876947c"
   },
   "outputs": [],
   "source": [
    "get_mse(np.exp(pred), np.exp(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "2l9P4Q0SNfkQ",
    "outputId": "c5ef43e1-cd33-4e74-e078-d39743303e5f"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(pred)[train.nonzero()].flatten(), alpha = 0.5, label = \"test prediction\")\n",
    "plt.title(\"Histogram of Test Prediction vs Test Data\")\n",
    "plt.hist(np.exp(train)[train.nonzero()].flatten(), alpha = 0.5, label = \"test data\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "4Rk5sK32PgNR",
    "outputId": "0a6bc1c4-3141-4aa5-acfa-1ea6a1e48d10"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(pred).flatten(), alpha = 0.5, label = \"test prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "SwqhC1BxPraw",
    "outputId": "e6639a2c-56f9-42a1-b2b0-33833cbf0c61"
   },
   "outputs": [],
   "source": [
    "np.exp(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5sjuGZfIPNzP",
    "outputId": "cf9cd191-19ff-408c-f1ea-f446822ced78"
   },
   "outputs": [],
   "source": [
    "np.exp(pred).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JjtO3xL7PUlE",
    "outputId": "1f2c7691-196d-4095-c2b0-b325b4ce7058"
   },
   "outputs": [],
   "source": [
    "np.exp(pred).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1E4DlpKWNhQp"
   },
   "outputs": [],
   "source": [
    "#explore this for graph auto encoder\n",
    "#https://github.com/tkipf/gae"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "syngenta_data_exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
